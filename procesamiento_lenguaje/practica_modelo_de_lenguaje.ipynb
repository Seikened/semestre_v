{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4261fdc",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO | PRACTICA MODELO DE LENGUAJE PROBABILISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709c1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fdc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
    "    porcentaje = (i + 1) / cantidad_registros * 100\n",
    "    # Con emojis\n",
    "    barra = int(50 * (i + 1) / cantidad_registros) * \"🟩\"\n",
    "    espacio = int(50 - len(barra)) * \"⬛️\"\n",
    "\n",
    "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3464b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================= Limpieza de texto =======================\n",
    "def limpiar_texto(texto):\n",
    "    \n",
    "    texto = BeautifulSoup(texto, \"html.parser\").get_text()\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "    texto = re.sub(r\"@\\w+\", \"\", texto)\n",
    "    texto = re.sub(r\"#+\", \"\", texto)\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    texto_limpio = [word for word in texto.split() if word not in stop_words]\n",
    "    return \" \".join(texto_limpio)\n",
    "\n",
    "\n",
    "# ======================= Carga de datos =======================\n",
    "def get_texts_from_folder(path_folder):\n",
    "    tr_txt = []  # aquí van los documentos\n",
    "    tr_y = []  # aquí van las etiquetas\n",
    "\n",
    "    for file in os.listdir(path_folder):\n",
    "        if file.endswith(\".xml\"):\n",
    "            tree = ET.parse(os.path.join(path_folder, file))\n",
    "            root = tree.getroot()\n",
    "            docs = []\n",
    "            for doc in root.iter(\"document\"):\n",
    "                texto_limpio = limpiar_texto(doc.text)\n",
    "                docs.append(texto_limpio)\n",
    "    \n",
    "            tr_txt.append(\" \".join(docs))\n",
    "\n",
    "    truth_file = os.path.join(path_folder, \"truth.txt\")\n",
    "\n",
    "    file_to_label = {}\n",
    "    with open(truth_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\":::\")\n",
    "            # País\n",
    "            \n",
    "            # pais = parts[2]\n",
    "            # file_to_label[parts[0]] = pais\n",
    "            \n",
    "            # Genero\n",
    "            genero = parts[1]\n",
    "            file_to_label[parts[0]] = genero\n",
    "\n",
    "\n",
    "\n",
    "    for file in os.listdir(path_folder):\n",
    "        if file.endswith(\".xml\"):\n",
    "            file_id = file.split(\".\")[0]\n",
    "            if file_id in file_to_label:\n",
    "                tr_y.append(file_to_label[file_id])\n",
    "\n",
    "        print_bar(len(tr_y), len(file_to_label), contexto=\"CARGA DE ETIQUETAS\")\n",
    "\n",
    "    return tr_txt, tr_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf67095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/21ypxx3x0019wgj7rqnlm2340000gn/T/ipykernel_70542/549244330.py:4: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  texto = BeautifulSoup(texto, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARGA DE ETIQUETAS: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.02%\n",
      "Textos train: 4200, Etiquetas train: 4200\n"
     ]
    }
   ],
   "source": [
    "# ======================= Carga de datos =======================\n",
    "path_test = '/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test'\n",
    "path_train = '/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train'\n",
    "tr_txt_train, tr_y_train = get_texts_from_folder(path_train)\n",
    "#tr_txt_test, tr_y_test = get_texts_from_folder(path_test)\n",
    "\n",
    "print(f\"\\nTextos train: {len(tr_txt_train)}, Etiquetas train: {len(tr_y_train)}\")\n",
    "#print(f\"Textos test: {len(tr_txt_test)}, Etiquetas test: {len(tr_y_test)}\")\n",
    "\n",
    "\n",
    "paises = sorted(list(set(tr_y_train)))\n",
    "paises_numericas = {pais: idx for idx, pais in enumerate(paises)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train = [paises_numericas[pais] for pais in tr_y_train]\n",
    "#y_test = [paises_numericas[pais] for pais in tr_y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ac2a3",
   "metadata": {},
   "source": [
    "# PARTE 1: Procesamiento y tratamiento de los datos para un modelo de lenguaje probabilista\n",
    "- Necesitamos contar las ocurrencias de trigramas o de bigramas en el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf50168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramData:\n",
    "    MODE = \"TRIGRAM\"\n",
    "    def __init__(self, vocab_max, tokenizer):\n",
    "        self.vocab_max = vocab_max\n",
    "        self.tokenizer = tokenizer\n",
    "        self.final_vocab = set()\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.UNK = \"<unk>\"\n",
    "        \n",
    "    def fit(self, raw_text): # En row_text recibiré los tweets\n",
    "        freq_dist = nltk.FreqDist()\n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        for txt in raw_text:\n",
    "            tokens = self.tokenizer.tokenize(txt.lower())\n",
    "            tokenized_corpus.append(tokens) # recordar que esta es una lista de listas de tweets tokenizados\n",
    "            for w in tokens:\n",
    "                freq_dist[w] += 1\n",
    "                \n",
    "        self.final_vocab = { token for token, _ in freq_dist.most_common(self.vocab_max) }\n",
    "        self.final_vocab.update([self.SOS, self.EOS, self.UNK])\n",
    "        \n",
    "        transform_corpus = []\n",
    "        for tokens in tokenized_corpus:\n",
    "            transform_corpus.append(self.transform(tokens)) # tokens es un tweet tokenizado\n",
    "        \n",
    "        return transform_corpus\n",
    "    \n",
    "    \n",
    "    def mask_out_of_vocab(self, word):\n",
    "        if word in self.final_vocab:\n",
    "            return word\n",
    "        else:\n",
    "            return self.UNK\n",
    "\n",
    "    def add_sos_eos(self, tokenized_text):\n",
    "        \n",
    "        if self.MODE == \"BIGRAM\":\n",
    "            return [self.SOS] + tokenized_text + [self.EOS]\n",
    "        elif self.MODE == \"TRIGRAM\":\n",
    "            return [self.SOS, self.SOS] + tokenized_text + [self.EOS]\n",
    "\n",
    "    def transform(self, tokenized_text):\n",
    "        transformed = [] # Tokens transformados\n",
    "        for w in tokenized_text:\n",
    "            transformed.append(self.mask_out_of_vocab(w)) # Mask  Out of Vocabulary Word\n",
    "        transformed = self.add_sos_eos(transformed)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057aaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_PALABRAS = 10_000\n",
    "tokenizador = TweetTokenizer()\n",
    "\n",
    "trigram_data = TrigramData(vocab_max=TOP_PALABRAS, tokenizer=tokenizador)\n",
    "\n",
    "transformed_corpus = trigram_data.fit(tr_txt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d976b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario final: 10,003\n"
     ]
    }
   ],
   "source": [
    "final_vocab = trigram_data.final_vocab\n",
    "print(f\"Tamaño del vocabulario final: {len(final_vocab):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0fae9",
   "metadata": {},
   "source": [
    "# BUILDING A TRIGRAM LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd26131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "    \"\"\" Modelo interpolado  unigramas + bigramas + trigramas \"\"\"\n",
    "    def __init__(self, lambda_1 = 0.4, lambda_2 = 0.3, lambda_3 = 0.3, vocab=None):\n",
    "        # Las lambdas deben sumar 1 y son los pesos de cada modelo\n",
    "        self.lambda_1 = lambda_1 # Trigramas\n",
    "        self.lambda_2 = lambda_2 # Bigramas\n",
    "        self.lambda_3 = lambda_3 # Unigramas\n",
    "        \n",
    "        # Contadores\n",
    "        self.unigram_counts = {} # Los unigramas con las palabras solitas\n",
    "        self.bigram_counts = {}  # Los bigramas subsecuencias de tamaño 2\n",
    "        self.trigram_counts = {} # Los trigramas subsecuencias de tamaño 3\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab) if vocab is not None else 0\n",
    "\n",
    "\n",
    "    def train(self, transformed_corpus):\n",
    "        for tokens in transformed_corpus: # primero recorro tweet por tweet\n",
    "            for i, word in enumerate(tokens): # Luego para cada tweet le reccorro sus palabras\n",
    "                \n",
    "                # Unigramas\n",
    "                self.unigram_counts[word] = self.unigram_counts.get(word, 0) + 1\n",
    "                \n",
    "                # Bigramas\n",
    "                if i > 0: # Solo si ya vi un palabra antes, puedo formar un bigrama\n",
    "                    bigrama = (tokens[i-1], word)\n",
    "                    self.bigram_counts[bigrama] = self.bigram_counts.get(bigrama, 0) + 1\n",
    "                \n",
    "                # Trigramas\n",
    "                if i > 1: # Solo si ya vi dos palabras antes, puedo formar un trigrama\n",
    "                    trigrama = (tokens[i-2], tokens[i-1], word)\n",
    "                    self.trigram_counts[trigrama] = self.trigram_counts.get(trigrama, 0) + 1\n",
    "            \n",
    "            self.total_tokens = sum(self.unigram_counts.values())\n",
    "    \n",
    "    \n",
    "    def unigram_probability(self, word):\n",
    "        numerador = self.unigram_counts[word]\n",
    "        denominador = self.total_tokens\n",
    "        # TODO: Validar palabras fuera de vocabulario\n",
    "        # TODO: Evitar multiplicar por cero para evitar probabilidad cero\n",
    "        return numerador / denominador\n",
    "                    \n",
    "                    \n",
    "    def bigram_probability(self, word_1, word_2):\n",
    "        numerador = 1\n",
    "        denominador = 1\n",
    "        return  numerador / denominador\n",
    "    \n",
    "    \n",
    "    def trigram_probability(self, word_1, word_2, word_3):\n",
    "        numerador = 1\n",
    "        denominador = 1\n",
    "        return  numerador / denominador\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
