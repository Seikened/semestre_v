{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4261fdc",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO | PRACTICA MODELO DE LENGUAJE PROBABILISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "709c1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import permutations\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from time import sleep\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3464b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "\n",
    "    tr_txt = [] # Aqui van los twits\n",
    "    tr_y = [] # Aqui van las etiquetas\n",
    "\n",
    "    with open(path_corpus, 'r', encoding='utf-8') as f_corpus, open(path_truth , 'r', encoding='utf-8') as f_truth:\n",
    "        \n",
    "        for twitt in f_corpus:\n",
    "            tr_txt += [twitt]\n",
    "            \n",
    "        for label in f_truth:\n",
    "            tr_y += [label]\n",
    "    \n",
    "    return tr_txt,tr_y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cf67095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path_global = \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/mex\"\n",
    "path_corpus = path_global + '/mex20_train.txt'\n",
    "path_truth = path_global + '/mex20_train_labels.txt'\n",
    "\n",
    "\n",
    "tr_txt, tr_y = get_texts_from_file(path_corpus,path_truth)\n",
    "\n",
    "\n",
    "\n",
    "# Construir los datos de validaci√≥n\n",
    "path_corpus = path_global + '/mex20_val.txt'\n",
    "path_truth = path_global + '/mex20_val_labels.txt'\n",
    "\n",
    "\n",
    "va_txt, va_y = get_texts_from_file(path_corpus,path_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ac2a3",
   "metadata": {},
   "source": [
    "# PARTE 1: Procesamiento y tratamiento de los datos para un modelo de lenguaje probabilista\n",
    "- Necesitamos contar las ocurrencias de trigramas o de bigramas en el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf50168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramData:\n",
    "    MODE = \"TRIGRAM\"\n",
    "    def __init__(self, vocab_max, tokenizer):\n",
    "        self.vocab_max = vocab_max\n",
    "        self.tokenizer = tokenizer\n",
    "        self.final_vocab = set()\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.UNK = \"<unk>\"\n",
    "        \n",
    "    def fit(self, raw_text): # En row_text recibir√© los tweets\n",
    "        freq_dist = nltk.FreqDist()\n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        for txt in raw_text:\n",
    "            tokens = self.tokenizer.tokenize(txt.lower())\n",
    "            tokenized_corpus.append(tokens) # recordar que esta es una lista de listas de tweets tokenizados\n",
    "            for w in tokens:\n",
    "                freq_dist[w] += 1\n",
    "                \n",
    "        self.final_vocab = { token for token, _ in freq_dist.most_common(self.vocab_max) }\n",
    "        self.final_vocab.update([self.SOS, self.EOS, self.UNK])\n",
    "        \n",
    "        transform_corpus = []\n",
    "        for tokens in tokenized_corpus:\n",
    "            transform_corpus.append(self.transform(tokens)) # tokens es un tweet tokenizado\n",
    "        \n",
    "        \n",
    "        return transform_corpus\n",
    "    \n",
    "    \n",
    "    def mask_out_of_vocab(self, word):\n",
    "        if word in self.final_vocab:\n",
    "            return word\n",
    "        else:\n",
    "            return self.UNK\n",
    "\n",
    "    def add_sos_eos(self, tokenized_text):\n",
    "        \n",
    "        if self.MODE == \"BIGRAM\":\n",
    "            return [self.SOS] + tokenized_text + [self.EOS]\n",
    "        elif self.MODE == \"TRIGRAM\":\n",
    "            return [self.SOS, self.SOS] + tokenized_text + [self.EOS]\n",
    "\n",
    "    def transform(self, tokenized_text):\n",
    "        transformed = [] # Tokens transformados\n",
    "        for w in tokenized_text:\n",
    "            transformed.append(self.mask_out_of_vocab(w)) # Mask  Out of Vocabulary Word\n",
    "        transformed = self.add_sos_eos(transformed)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "057aaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_PALABRAS = 10_000\n",
    "tokenizador = TweetTokenizer()\n",
    "\n",
    "trigram_data = TrigramData(vocab_max=TOP_PALABRAS, tokenizer=tokenizador)\n",
    "\n",
    "transformed_corpus = trigram_data.fit(tr_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d976b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del vocabulario final: 10,003\n"
     ]
    }
   ],
   "source": [
    "final_vocab = trigram_data.final_vocab\n",
    "print(f\"Tama√±o del vocabulario final: {len(final_vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77761c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arbitral\n",
      "desmadrito\n",
      "tremenda\n",
      "we'll\n",
      "trav√©s\n",
      "#ud√≠acomohoy\n",
      "vino\n",
      "recorriendo\n",
      "asquerosas\n",
      "üñï\n",
      "angustia\n",
      "rival\n",
      "porqu√©\n",
      "10,000\n",
      "apagan\n",
      "#dragonballz\n",
      "#escort\n",
      "osorio\n",
      "choro\n",
      "tuvieron\n"
     ]
    }
   ],
   "source": [
    "for palabra in list(final_vocab)[:20]:\n",
    "    print(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "076364d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 20 palabras del vocabulario final:\n",
      " 1. !\n",
      " 2. \"\n",
      " 3. #\n",
      " 4. #140caracteres\n",
      " 5. #19s\n",
      " 6. #280caracteres\n",
      " 7. #aba\n",
      " 8. #aborto\n",
      " 9. #acropolispuebla\n",
      "10. #addi\n",
      "11. #adn\n",
      "12. #agsmx\n",
      "13. #aguascalientes\n",
      "14. #aguilas\n",
      "15. #ahscult\n",
      "16. #alaorden\n",
      "17. #alerta\n",
      "18. #alvlavida\n",
      "19. #amedroauditor\n",
      "20. #amigos\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeras 20 palabras del vocabulario final:\")\n",
    "for idx, palabra in enumerate(sorted(final_vocab)):\n",
    "    if idx < 20:\n",
    "        print(f\"{idx+1:2d}. {palabra}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0fae9",
   "metadata": {},
   "source": [
    "# BUILDING A TRIGRAM LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dd26131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "    \"\"\" Modelo interpolado  unigramas + bigramas + trigramas \"\"\"\n",
    "    def __init__(self, lambda_1 = 0.59, lambda_2 = 0.40, lambda_3 = 0.01, vocab=None):\n",
    "        # Las lambdas deben sumar 1 y son los pesos de cada modelo\n",
    "        self.lambda_1 = lambda_1 # Trigramas\n",
    "        self.lambda_2 = lambda_2 # Bigramas\n",
    "        self.lambda_3 = lambda_3 # Unigramas\n",
    "        \n",
    "        # Contadores\n",
    "        self.unigram_counts = {} # Los unigramas con las palabras solitas\n",
    "        self.bigram_counts = {}  # Los bigramas subsecuencias de tama√±o 2\n",
    "        self.trigram_counts = {} # Los trigramas subsecuencias de tama√±o 3\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab) if vocab is not None else 0\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.UNK = \"<unk>\"\n",
    "\n",
    "\n",
    "    def train(self, transformed_corpus):\n",
    "        for tokens in transformed_corpus: # primero recorro tweet por tweet\n",
    "            for i, word in enumerate(tokens): # Luego para cada tweet le reccorro sus palabras\n",
    "                \n",
    "                # Unigramas\n",
    "                self.unigram_counts[word] = self.unigram_counts.get(word, 0) + 1\n",
    "                \n",
    "                # Bigramas\n",
    "                if i > 0: # Solo si ya vi un palabra antes, puedo formar un bigrama\n",
    "                    bigrama = (tokens[i-1], word)\n",
    "                    self.bigram_counts[bigrama] = self.bigram_counts.get(bigrama, 0) + 1\n",
    "                \n",
    "                # Trigramas\n",
    "                if i > 1: # Solo si ya vi dos palabras antes, puedo formar un trigrama\n",
    "                    trigrama = (tokens[i-2], tokens[i-1], word)\n",
    "                    self.trigram_counts[trigrama] = self.trigram_counts.get(trigrama, 0) + 1\n",
    "            \n",
    "            self.total_tokens = sum(self.unigram_counts.values())\n",
    "    \n",
    "    \n",
    "    def mask_out_of_vocab(self, word):\n",
    "        return \"<unk>\" if word not in self.vocab else word\n",
    "    \n",
    "    \n",
    "    def unigram_probability(self, word):\n",
    "        numerador = self.unigram_counts.get(self.mask_out_of_vocab(word), 0) + 1\n",
    "        denominador = self.total_tokens + self.vocab_size\n",
    "        return numerador / denominador\n",
    "                    \n",
    "                    \n",
    "    def bigram_probability(self, word_prev, word): # Esta funci√≥n calcula P(word | word_prev)\n",
    "        w_prev = self.mask_out_of_vocab(word_prev)\n",
    "        w = self.mask_out_of_vocab(word)\n",
    "        bigrama = (w_prev, w) # P(w | w_prev)\n",
    "        \n",
    "        numerador = self.bigram_counts.get(bigrama, 0) + 1\n",
    "        denominador = self.unigram_counts.get(w_prev, 0) + self.vocab_size\n",
    "        return  numerador / denominador\n",
    "    \n",
    "    \n",
    "    def trigram_probability(self, word_prev2, word_prev1, word): # Esta funci√≥n calcula P(word | word_prev2, word_prev1)\n",
    "        w_prev2 = self.mask_out_of_vocab(word_prev2)\n",
    "        w_prev1 = self.mask_out_of_vocab(word_prev1)\n",
    "        w = self.mask_out_of_vocab(word)\n",
    "        \n",
    "        trigrama = (w_prev2, w_prev1, w)\n",
    "        bigrama = (w_prev2, w_prev1)\n",
    "        numerador = self.trigram_counts.get(trigrama, 0) + 1\n",
    "        denominador = self.bigram_counts.get(bigrama, 0) + self.vocab_size\n",
    "        return  numerador / denominador\n",
    "        \n",
    "    def check_probs(self):\n",
    "        print(sum(self.unigram_probability(w) for w in self.vocab)) # Type: ignore\n",
    "        print(sum(self.bigram_probability(\"gato\", w) for w in self.vocab)) # Type: ignore\n",
    "        print(sum(self.trigram_probability(\"hola\", \"como\", w) for w in self.vocab)) # Type: ignore\n",
    "\n",
    "    def probabilidades_uso(self, word_prev2, word_prev1, word):\n",
    "        p_trigrama = self.trigram_probability(word_prev2, word_prev1, word)\n",
    "        p_bigrama = self.bigram_probability(word_prev1, word)\n",
    "        p_unigrama = self.unigram_probability(word)\n",
    "\n",
    "        return p_trigrama,p_bigrama,p_unigrama\n",
    "\n",
    "    def probability_of_word(self, word_prev2, word_prev1, word):\n",
    "        p_trigrama,p_bigrama,p_unigrama = self.probabilidades_uso(word_prev2, word_prev1, word)\n",
    "\n",
    "        return (self.lambda_1 * p_trigrama) + (self.lambda_2 * p_bigrama) + (self.lambda_3 * p_unigrama)\n",
    "\n",
    "\n",
    "    def top_next_words(self, w_prev2, w_prev, top_k=5):\n",
    "        candidates = []\n",
    "        for cand in self.vocab:\n",
    "            p_w = self.probability_of_word(w_prev2, w_prev, cand)\n",
    "            candidates.append((cand, p_w))\n",
    "        # Ordena de mayor a menor probabilidad\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[:top_k]\n",
    "        \n",
    "    \n",
    "    def secuence_probability(self, seciencia_tokens):\n",
    "        probabilidad_acumulada = 0\n",
    "        for i in range(2, len(seciencia_tokens)):\n",
    "            w_prev2 = seciencia_tokens[i-2]\n",
    "            w_prev1 = seciencia_tokens[i-1]\n",
    "            w = seciencia_tokens[i]\n",
    "            probabilidad = self.probability_of_word(w_prev2, w_prev1, w)\n",
    "            probabilidad_acumulada += math.log(probabilidad)\n",
    "\n",
    "        return math.exp(probabilidad_acumulada)\n",
    "\n",
    "    def rank_permutations(self, tokens, top_k=5, bottom_k=5):\n",
    "        permutaciones = list(permutations(tokens))\n",
    "        \n",
    "        scores = []\n",
    "        for perm in permutaciones:\n",
    "            p_sencuencia = self.secuence_probability(perm)\n",
    "            scores.append((\" \".join(perm), p_sencuencia))\n",
    "        \n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        best = scores[:top_k]\n",
    "        worst = scores[-bottom_k:]\n",
    "        \n",
    "        return best,worst\n",
    "    \n",
    "    \n",
    "    def generate_from_tokens_as_chatgpt(self, first_word, second_word, max_length=20, delay=0.5):\n",
    "        def validar_palabra(word):\n",
    "            return word if word in self.vocab else \"<unk>\"\n",
    "        \n",
    "        first_word = validar_palabra(first_word)\n",
    "        second_word = validar_palabra(second_word)\n",
    "        \n",
    "        generated_tokens = [first_word, second_word]\n",
    "        \n",
    "        print(first_word, second_word, end='', flush=True)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            w_prev2 = generated_tokens[-2]\n",
    "            w_prev1 = generated_tokens[-1]\n",
    "            \n",
    "            distribucion = []\n",
    "            total_p =  0.0\n",
    "            for cand in self.vocab: # type: ignore\n",
    "                p_w = self.probability_of_word(w_prev2, w_prev1, cand)\n",
    "                distribucion.append((cand, p_w))\n",
    "                total_p += p_w\n",
    "            \n",
    "            if total_p == 0:\n",
    "                break\n",
    "            \n",
    "            r = random.random() * total_p\n",
    "            acumulado = 0.0\n",
    "            siguiente_palabra = None\n",
    "            \n",
    "            for w, p in distribucion:\n",
    "                acumulado += p\n",
    "                if acumulado >= r:\n",
    "                    siguiente_palabra = w\n",
    "                    break\n",
    "                \n",
    "            if siguiente_palabra == self.EOS or siguiente_palabra is None:\n",
    "                break\n",
    "            generated_tokens.append(siguiente_palabra)\n",
    "            print(\" \" + siguiente_palabra, end='', flush=True)\n",
    "            sleep(delay)\n",
    "        print()\n",
    "        return generated_tokens\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81da5d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o de las permutaciones: 6\n",
      "('sino', 'gano', 'me')\n",
      "('me', 'sino', 'gano')\n",
      "('sino', 'me', 'gano')\n",
      "('gano', 'me', 'sino')\n",
      "('gano', 'sino', 'me')\n",
      "('me', 'gano', 'sino')\n"
     ]
    }
   ],
   "source": [
    "# EJEMPLO DE LAS PERMUTACIONES\n",
    "test_tokens = [\"sino\",\"gano\",\"me\"]\n",
    "permitaciones = set(permutations(test_tokens))\n",
    "print(f\"Tama√±o de las permutaciones: {len(permitaciones)}\")\n",
    "for p in list(permitaciones)[:10]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e09fff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = trigram_data.final_vocab\n",
    "\n",
    "trigram_lm = TrigramLanguageModel(vocab=final_vocab)\n",
    "trigram_lm.train(transformed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb5a32f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "trigram_lm.check_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b618b",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b642ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(mundo | <s>, hola) = 0.00010335132933804556\n"
     ]
    }
   ],
   "source": [
    "word_prev2, word_prev1, word = \"<s>\", \"hola\", \"mundo\"\n",
    "p_w = trigram_lm.probability_of_word(word_prev2, word_prev1, word)\n",
    "print(f\"P({word} | {word_prev2}, {word_prev1}) = {p_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4ed6414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(a | <s>, saludos) = 0.0002778290833875056\n"
     ]
    }
   ],
   "source": [
    "word_prev2, word_prev1, word = \"<s>\", \"saludos\", \"a\"\n",
    "p_w = trigram_lm.probability_of_word(word_prev2, word_prev1, word)\n",
    "print(f\"P({word} | {word_prev2}, {word_prev1}) = {p_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28f19aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(tu | hijo, de) = 0.0034786898082040775\n"
     ]
    }
   ],
   "source": [
    "word_prev2, word_prev1, word = \"hijo\", \"de\", \"tu\"\n",
    "p_w = trigram_lm.probability_of_word(word_prev2, word_prev1, word)\n",
    "print(f\"P({word} | {word_prev2}, {word_prev1}) = {p_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de5468f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(la | vete, a) = 0.011153217757174417\n"
     ]
    }
   ],
   "source": [
    "word_prev2, word_prev1, word = \"vete\", \"a\", \"la\"\n",
    "p_w = trigram_lm.probability_of_word(word_prev2, word_prev1, word)\n",
    "print(f\"P({word} | {word_prev2}, {word_prev1}) = {p_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da6dc6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(la | hijo, de) = 0.009244873899638673\n",
      "P(<unk> | hijo, de) = 0.005531514118896191\n",
      "P(tu | hijo, de) = 0.0034786898082040775\n",
      "P(su | hijo, de) = 0.0026121131631923304\n",
      "P(mi | hijo, de) = 0.0025834426767438\n"
     ]
    }
   ],
   "source": [
    "top_5 = trigram_lm.top_next_words(\"hijo\", \"de\", top_k=5)\n",
    "for w, p in top_5:\n",
    "    print(f\"P({w} | hijo, de) = {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed5d4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(<s> | hola, mundo) = 0.0009548905419916221\n",
      "P(. | hola, mundo) = 0.0007077447034876222\n",
      "P(</s> | hola, mundo) = 0.0006859567869160297\n",
      "P(, | hola, mundo) = 0.0005422899688591583\n",
      "P(de | hola, mundo) = 0.00042929561320088607\n"
     ]
    }
   ],
   "source": [
    "top_5 = trigram_lm.top_next_words(\"hola\", \"mundo\", top_k=5)\n",
    "for w, p in top_5:\n",
    "    print(f\"P({w} | hola, mundo) = {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fadcfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(<s> | saludos, desde) = 0.0009548787514258432\n",
      "P(que | saludos, desde) = 0.0007886220919455008\n",
      "P(el | saludos, desde) = 0.0006111244424612674\n",
      "P(</s> | saludos, desde) = 0.000526851827413192\n",
      "P(la | saludos, desde) = 0.00044192585759427003\n"
     ]
    }
   ],
   "source": [
    "top_5 = trigram_lm.top_next_words(\"saludos\", \"desde\", top_k=5)\n",
    "for w, p in top_5:\n",
    "    print(f\"P({w} | saludos, desde) = {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11951538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(la | partido, de) = 0.00906949641563996\n",
      "P(<unk> | partido, de) = 0.005473304741341722\n",
      "P(mi | partido, de) = 0.002583817352411454\n",
      "P(los | partido, de) = 0.002471707927558721\n",
      "P(que | partido, de) = 0.0023270595921347985\n"
     ]
    }
   ],
   "source": [
    "top_5 = trigram_lm.top_next_words(\"partido\", \"de\", top_k=5)\n",
    "for w, p in top_5:\n",
    "    print(f\"P({w} | partido, de) = {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50bd01be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(<s> | como, estas) = 0.0009549202710826276\n",
      "P(</s> | como, estas) = 0.0006256786737036542\n",
      "P(<unk> | como, estas) = 0.00035164397556725974\n",
      "P(que | como, estas) = 0.00035115739702537346\n",
      "P(de | como, estas) = 0.00034977875782336225\n"
     ]
    }
   ],
   "source": [
    "top_5 = trigram_lm.top_next_words(\"como\", \"estas\", top_k=5)\n",
    "for w, p in top_5:\n",
    "    print(f\"P({w} | como, estas) = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c55048",
   "metadata": {},
   "source": [
    "# EVALUACION CUALITATUVA CON LOS MODELOS DE LENGUAJE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be715412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de la secuencia \"sino gano me voy a la chingada\": 0.00000000000002799 [17]\n",
      "\n",
      "Top 5 permutaciones:\n",
      "Probabilidad de:\"sino gano me voy a la chingada\") = 0.00000000000002799 [17]\n",
      "Probabilidad de:\"gano sino me voy a la chingada\") = 0.00000000000002799 [17]\n",
      "Probabilidad de:\"gano me voy a la chingada sino\") = 0.00000000000001271 [17]\n",
      "Probabilidad de:\"sino me voy a la chingada gano\") = 0.00000000000001263 [17]\n",
      "Probabilidad de:\"sino gano voy a la chingada me\") = 0.00000000000000241 [17]\n",
      "\n",
      "Bottom 5 permutaciones:\n",
      "Probabilidad de:\"a la sino chingada voy me gano\") = 0.00000000000000000002 [20]\n",
      "Probabilidad de:\"a la gano chingada voy me sino\") = 0.00000000000000000002 [20]\n",
      "Probabilidad de:\"a la gano me sino voy chingada\") = 0.00000000000000000002 [20]\n",
      "Probabilidad de:\"a la sino voy chingada me gano\") = 0.00000000000000000002 [20]\n",
      "Probabilidad de:\"a la gano voy chingada me sino\") = 0.00000000000000000002 [20]\n"
     ]
    }
   ],
   "source": [
    "test_tokens = [\"sino\",\n",
    "               \"gano\",\n",
    "               #\",\",\n",
    "               \"me\",\n",
    "               \"voy\",\n",
    "               \"a\",\n",
    "               \"la\",\n",
    "               \"chingada\",\n",
    "            #    \"ahora\",\n",
    "            #    \"si\"\n",
    "\n",
    "               ]\n",
    "\n",
    "precision_float = 17\n",
    "probabilidad_sencuencia = trigram_lm.secuence_probability(test_tokens)\n",
    "print(f'Probabilidad de la secuencia \"{' '.join(test_tokens)}\": {probabilidad_sencuencia:.{precision_float}f} [{precision_float}]')\n",
    "\n",
    "\n",
    "\n",
    "top_5, bottom_5 = trigram_lm.rank_permutations(test_tokens, top_k=5, bottom_k=5)\n",
    "print(\"\\nTop 5 permutaciones:\")\n",
    "for seq, p in top_5:\n",
    "    print(f'Probabilidad de:\"{seq}\") = {p:.{precision_float}f} [{precision_float}]')\n",
    "    \n",
    "precision_float = 20\n",
    "print(\"\\nBottom 5 permutaciones:\")\n",
    "for seq, p in bottom_5:\n",
    "    print(f'Probabilidad de:\"{seq}\") = {p:.{precision_float}f} [{precision_float}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "deba70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "puta madre congales esperate jugadas computadora traidores por trooper ganador reparti√≥ lacra 23 mamaste zaragoza espa√±ola ag√ºitas din√°mica emociono zoraida #llorodetristezacuando pinta empiezas nivel compadres ley #gocowboys noci√≥n ames foco real quisieras brindaron putote putoteee dentro joya cobarde jajajajajajajajajaja secretarios williams sotile unas suelo llegaste sent√≠a got rinden menees proponer hablen nativa nuca necesito @alcaudon23 simple creaturitas holanda cabify ingles pachanga lvg kmestapasando negras utilizan informense coger üç≤ baek pens√°bamos joter√≠a strangers lado completo hondure√±o ajeno ense√±ar c√∫pulas complacerlos gio suaves rt ahumada #guatemala salcedo calificaciones toparte colecciones pretende queridos viera entendere üí¶ vatos abusivos votando encabronada calabaza desperdiciando blanco chivando auditor√≠a\n"
     ]
    }
   ],
   "source": [
    "respuesta = trigram_lm.generate_from_tokens_as_chatgpt(\"puta\", \"madre\", max_length=100, delay=0.3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
