{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4261fdc",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO | PRACTICA MODELO DE LENGUAJE PROBABILISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709c1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fdc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
    "    porcentaje = (i + 1) / cantidad_registros * 100\n",
    "    # Con emojis\n",
    "    barra = int(50 * (i + 1) / cantidad_registros) * \"游릴\"\n",
    "    espacio = int(50 - len(barra)) * \"拘勇끂"\n",
    "\n",
    "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3464b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================= Limpieza de texto =======================\n",
    "def limpiar_texto(texto):\n",
    "    \n",
    "    texto = BeautifulSoup(texto, \"html.parser\").get_text()\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
    "    texto = re.sub(r\"@\\w+\", \"\", texto)\n",
    "    texto = re.sub(r\"#+\", \"\", texto)\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    texto_limpio = [word for word in texto.split() if word not in stop_words]\n",
    "    return \" \".join(texto_limpio)\n",
    "\n",
    "\n",
    "# ======================= Carga de datos =======================\n",
    "def get_texts_from_folder(path_folder):\n",
    "    tr_txt = []  # aqu칤 van los documentos\n",
    "    tr_y = []  # aqu칤 van las etiquetas\n",
    "\n",
    "    for file in os.listdir(path_folder):\n",
    "        if file.endswith(\".xml\"):\n",
    "            tree = ET.parse(os.path.join(path_folder, file))\n",
    "            root = tree.getroot()\n",
    "            docs = []\n",
    "            for doc in root.iter(\"document\"):\n",
    "                texto_limpio = limpiar_texto(doc.text)\n",
    "                docs.append(texto_limpio)\n",
    "    \n",
    "            tr_txt.append(\" \".join(docs))\n",
    "\n",
    "    truth_file = os.path.join(path_folder, \"truth.txt\")\n",
    "\n",
    "    file_to_label = {}\n",
    "    with open(truth_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\":::\")\n",
    "            # Pa칤s\n",
    "            \n",
    "            # pais = parts[2]\n",
    "            # file_to_label[parts[0]] = pais\n",
    "            \n",
    "            # Genero\n",
    "            genero = parts[1]\n",
    "            file_to_label[parts[0]] = genero\n",
    "\n",
    "\n",
    "\n",
    "    for file in os.listdir(path_folder):\n",
    "        if file.endswith(\".xml\"):\n",
    "            file_id = file.split(\".\")[0]\n",
    "            if file_id in file_to_label:\n",
    "                tr_y.append(file_to_label[file_id])\n",
    "\n",
    "        print_bar(len(tr_y), len(file_to_label), contexto=\"CARGA DE ETIQUETAS\")\n",
    "\n",
    "    return tr_txt, tr_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf67095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/21ypxx3x0019wgj7rqnlm2340000gn/T/ipykernel_70542/549244330.py:4: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  texto = BeautifulSoup(texto, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARGA DE ETIQUETAS: |游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴游릴| 100.02%\n",
      "Textos train: 4200, Etiquetas train: 4200\n"
     ]
    }
   ],
   "source": [
    "# ======================= Carga de datos =======================\n",
    "path_test = '/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test'\n",
    "path_train = '/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train'\n",
    "tr_txt_train, tr_y_train = get_texts_from_folder(path_train)\n",
    "#tr_txt_test, tr_y_test = get_texts_from_folder(path_test)\n",
    "\n",
    "print(f\"\\nTextos train: {len(tr_txt_train)}, Etiquetas train: {len(tr_y_train)}\")\n",
    "#print(f\"Textos test: {len(tr_txt_test)}, Etiquetas test: {len(tr_y_test)}\")\n",
    "\n",
    "\n",
    "paises = sorted(list(set(tr_y_train)))\n",
    "paises_numericas = {pais: idx for idx, pais in enumerate(paises)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train = [paises_numericas[pais] for pais in tr_y_train]\n",
    "#y_test = [paises_numericas[pais] for pais in tr_y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ac2a3",
   "metadata": {},
   "source": [
    "# PARTE 1: Procesamiento y tratamiento de los datos para un modelo de lenguaje probabilista\n",
    "- Necesitamos contar las ocurrencias de trigramas o de bigramas en el conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf50168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramData:\n",
    "    MODE = \"TRIGRAM\"\n",
    "    def __init__(self, vocab_max, tokenizer):\n",
    "        self.vocab_max = vocab_max\n",
    "        self.tokenizer = tokenizer\n",
    "        self.final_vocab = set()\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.UNK = \"<unk>\"\n",
    "        \n",
    "    def fit(self, raw_text): # En row_text recibir칠 los tweets\n",
    "        freq_dist = nltk.FreqDist()\n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        for txt in raw_text:\n",
    "            tokens = self.tokenizer.tokenize(txt.lower())\n",
    "            tokenized_corpus.append(tokens) # recordar que esta es una lista de listas de tweets tokenizados\n",
    "            for w in tokens:\n",
    "                freq_dist[w] += 1\n",
    "                \n",
    "        self.final_vocab = { token for token, _ in freq_dist.most_common(self.vocab_max) }\n",
    "        self.final_vocab.update([self.SOS, self.EOS, self.UNK])\n",
    "        \n",
    "        transform_corpus = []\n",
    "        for tokens in tokenized_corpus:\n",
    "            transform_corpus.append(self.transform(tokens)) # tokens es un tweet tokenizado\n",
    "        \n",
    "        return transform_corpus\n",
    "    \n",
    "    \n",
    "    def mask_out_of_vocab(self, word):\n",
    "        if word in self.final_vocab:\n",
    "            return word\n",
    "        else:\n",
    "            return self.UNK\n",
    "\n",
    "    def add_sos_eos(self, tokenized_text):\n",
    "        \n",
    "        if self.MODE == \"BIGRAM\":\n",
    "            return [self.SOS] + tokenized_text + [self.EOS]\n",
    "        elif self.MODE == \"TRIGRAM\":\n",
    "            return [self.SOS, self.SOS] + tokenized_text + [self.EOS]\n",
    "\n",
    "    def transform(self, tokenized_text):\n",
    "        transformed = [] # Tokens transformados\n",
    "        for w in tokenized_text:\n",
    "            transformed.append(self.mask_out_of_vocab(w)) # Mask  Out of Vocabulary Word\n",
    "        transformed = self.add_sos_eos(transformed)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057aaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_PALABRAS = 10_000\n",
    "tokenizador = TweetTokenizer()\n",
    "\n",
    "trigram_data = TrigramData(vocab_max=TOP_PALABRAS, tokenizer=tokenizador)\n",
    "\n",
    "transformed_corpus = trigram_data.fit(tr_txt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d976b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama침o del vocabulario final: 10,003\n"
     ]
    }
   ],
   "source": [
    "final_vocab = trigram_data.final_vocab\n",
    "print(f\"Tama침o del vocabulario final: {len(final_vocab):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0fae9",
   "metadata": {},
   "source": [
    "# BUILDING A TRIGRAM LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd26131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "    \"\"\" Modelo interpolado  unigramas + bigramas + trigramas \"\"\"\n",
    "    def __init__(self, lambda_1 = 0.4, lambda_2 = 0.3, lambda_3 = 0.3, vocab=None):\n",
    "        # Las lambdas deben sumar 1 y son los pesos de cada modelo\n",
    "        self.lambda_1 = lambda_1 # Trigramas\n",
    "        self.lambda_2 = lambda_2 # Bigramas\n",
    "        self.lambda_3 = lambda_3 # Unigramas\n",
    "        \n",
    "        # Contadores\n",
    "        self.unigram_counts = {} # Los unigramas con las palabras solitas\n",
    "        self.bigram_counts = {}  # Los bigramas subsecuencias de tama침o 2\n",
    "        self.trigram_counts = {} # Los trigramas subsecuencias de tama침o 3\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab) if vocab is not None else 0\n",
    "\n",
    "\n",
    "    def train(self, transformed_corpus):\n",
    "        for tokens in transformed_corpus: # primero recorro tweet por tweet\n",
    "            for i, word in enumerate(tokens): # Luego para cada tweet le reccorro sus palabras\n",
    "                \n",
    "                # Unigramas\n",
    "                self.unigram_counts[word] = self.unigram_counts.get(word, 0) + 1\n",
    "                \n",
    "                # Bigramas\n",
    "                if i > 0: # Solo si ya vi un palabra antes, puedo formar un bigrama\n",
    "                    bigrama = (tokens[i-1], word)\n",
    "                    self.bigram_counts[bigrama] = self.bigram_counts.get(bigrama, 0) + 1\n",
    "                \n",
    "                # Trigramas\n",
    "                if i > 1: # Solo si ya vi dos palabras antes, puedo formar un trigrama\n",
    "                    trigrama = (tokens[i-2], tokens[i-1], word)\n",
    "                    self.trigram_counts[trigrama] = self.trigram_counts.get(trigrama, 0) + 1\n",
    "            \n",
    "            self.total_tokens = sum(self.unigram_counts.values())\n",
    "    \n",
    "    \n",
    "    def unigram_probability(self, word):\n",
    "        numerador = self.unigram_counts[word]\n",
    "        denominador = self.total_tokens\n",
    "        # TODO: Validar palabras fuera de vocabulario\n",
    "        # TODO: Evitar multiplicar por cero para evitar probabilidad cero\n",
    "        return numerador / denominador\n",
    "                    \n",
    "                    \n",
    "    def bigram_probability(self, word_1, word_2):\n",
    "        numerador = 1\n",
    "        denominador = 1\n",
    "        return  numerador / denominador\n",
    "    \n",
    "    \n",
    "    def trigram_probability(self, word_1, word_2, word_3):\n",
    "        numerador = 1\n",
    "        denominador = 1\n",
    "        return  numerador / denominador\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
