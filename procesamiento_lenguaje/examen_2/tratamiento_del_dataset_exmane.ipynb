{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1823098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import openpyxl\n",
    "import polars as pl\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a85318",
   "metadata": {},
   "source": [
    "# HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cca3da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruta_parquet(nombre_archivo: str) -> Path:\n",
    "    ruta = f\"data/{nombre_archivo}.parquet\"\n",
    "    rutafinal = Path().parent / ruta\n",
    "    print(f\"Ruta final: {rutafinal}\")\n",
    "    return rutafinal\n",
    "\n",
    "def cargar_parquet(nombre_parquet: str) -> pl.LazyFrame:\n",
    "    ruta = ruta_parquet(nombre_parquet) \n",
    "    return pl.read_parquet(ruta).lazy()\n",
    "\n",
    "def guardar_parquet(data: pl.LazyFrame | pl.DataFrame, nombre_parquet: str) -> pl.LazyFrame:\n",
    "    ruta = ruta_parquet(nombre_parquet)\n",
    "    if isinstance(data, pl.LazyFrame):\n",
    "        data.sink_parquet(ruta)\n",
    "        return cargar_parquet(nombre_parquet)\n",
    "    else:\n",
    "        data.write_parquet(ruta)\n",
    "        return cargar_parquet(nombre_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96c33904",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPRIMIR = True\n",
    "\n",
    "def imprimir(mensaje: str, parametros=None, collect_lazy_frame=False, **kwargs):\n",
    "    if not IMPRIMIR:\n",
    "        return\n",
    "    # Normaliza la fuente de valores: dict explícito o kwargs\n",
    "    values = {}\n",
    "    if isinstance(parametros, dict):\n",
    "        values.update(parametros)\n",
    "    values.update(kwargs)\n",
    "\n",
    "    # Soporte opcional para LazyFrame\n",
    "    if collect_lazy_frame:\n",
    "        for k, v in list(values.items()):\n",
    "            if hasattr(v, \"collect\"):\n",
    "                values[k] = v.collect()\n",
    "\n",
    "    print(mensaje.format(**values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed7a6481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta del archivo Excel: data/opiniones.xlsx [exists: True]\n",
      ",basilica_colegiata_560,mercado_hidalgo_600,casa_de_diego_rivera_698,universidad_de_guanajuato_900,alhóndiga_930,teatro_juárez_1,010,jardín_de_la_unión_1,134,callejón_del_beso_1,360,monumento_pípila_1,620,museo_de_las_momias_1,650"
     ]
    }
   ],
   "source": [
    "\n",
    "# DEFINICIONES INICIALES\n",
    "ruta = Path().parent / \"data/opiniones.xlsx\"\n",
    "print(f\"Ruta del archivo Excel: {ruta} [exists: {ruta.exists()}]\")\n",
    "\n",
    "workbook = openpyxl.load_workbook(ruta, read_only=True)\n",
    "hojas = workbook.sheetnames\n",
    "hojas = [hoja.lower().replace(\" \", \"_\") for hoja in hojas]\n",
    "for hoja in hojas:\n",
    "    print(f\",{hoja}\",end=\"\", flush=True)\n",
    "\n",
    "def obtener_nombre_hoja(sheet_index: int) -> str:\n",
    "    \"\"\"Obtiene el nombre de la hoja dado su índice.\"\"\"\n",
    "    nombre_hoja = hojas[sheet_index]\n",
    "    return nombre_hoja\n",
    "\n",
    "\n",
    "def calificaciones(calificacion:str) -> int:\n",
    "    \"\"\"Convierte la calificación de string a int.\"\"\"\n",
    "    calificacion_map = {\n",
    "        \"Pésimo\": 1,\n",
    "        \"Malo\": 2,\n",
    "        \"Regular\": 3,\n",
    "        \"Muy bueno\": 4,\n",
    "        \"Excelente\": 5\n",
    "    }\n",
    "    return calificacion_map.get(calificacion, 0)\n",
    "\n",
    "\n",
    "def tratar_columna(col:str) -> str:\n",
    "    \"\"\"Trata el nombre de una columna para que sea válido en polars.\"\"\"\n",
    "    col = col.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    return col\n",
    "\n",
    "\n",
    "def añadir_columna_lugar_turistico(iteracion: int) -> pl.LazyFrame:\n",
    "    df_excel = pl.read_excel(ruta, sheet_id=iteracion+1).lazy()\n",
    "    nombre_hoja = obtener_nombre_hoja(iteracion)\n",
    "    \n",
    "    df_excel = (\n",
    "        df_excel\n",
    "        .with_columns([\n",
    "            pl.lit(nombre_hoja).alias(\"lugar_turistico\"),\n",
    "            pl.col(\"Calificación\").map_elements(calificaciones).alias(\"calificacion_numerica\"),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    df_excel = df_excel.rename({col: tratar_columna(col) for col in df_excel.collect_schema().names()})\n",
    "    return df_excel\n",
    "\n",
    "\n",
    "def juntar_df(lista_df: list[pl.LazyFrame]) -> pl.LazyFrame:\n",
    "    nuevo_df_excel = pl.concat(lista_df)\n",
    "    return nuevo_df_excel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a30247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d63498",
   "metadata": {},
   "source": [
    "# Limpiezsa y tratamiento del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dec8181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta final: data/basilica_colegiata_560.parquet\n",
      "Ruta final: data/basilica_colegiata_560.parquet\n",
      "Ruta final: data/mercado_hidalgo_600.parquet\n",
      "Ruta final: data/mercado_hidalgo_600.parquet\n",
      "Ruta final: data/casa_de_diego_rivera_698.parquet\n",
      "Ruta final: data/casa_de_diego_rivera_698.parquet\n",
      "Ruta final: data/universidad_de_guanajuato_900.parquet\n",
      "Ruta final: data/universidad_de_guanajuato_900.parquet\n",
      "Ruta final: data/alhóndiga_930.parquet\n",
      "Ruta final: data/alhóndiga_930.parquet\n",
      "Ruta final: data/teatro_juárez_1,010.parquet\n",
      "Ruta final: data/teatro_juárez_1,010.parquet\n",
      "Ruta final: data/jardín_de_la_unión_1,134.parquet\n",
      "Ruta final: data/jardín_de_la_unión_1,134.parquet\n",
      "Ruta final: data/callejón_del_beso_1,360.parquet\n",
      "Ruta final: data/callejón_del_beso_1,360.parquet\n",
      "Ruta final: data/monumento_pípila_1,620.parquet\n",
      "Ruta final: data/monumento_pípila_1,620.parquet\n",
      "Ruta final: data/museo_de_las_momias_1,650.parquet\n",
      "Ruta final: data/museo_de_las_momias_1,650.parquet\n",
      "shape: (5,)\n",
      "Series: 'opinión' [str]\n",
      "[\n",
      "\t\"\"Basílica muy bien conservada,…\n",
      "\t\"\"The Basilica (Guanajuato does…\n",
      "\t\"\"Edificio de la iglesia amaril…\n",
      "\t\"\"A must see place in town and …\n",
      "\t\"\"Not particularly impressive, …\n",
      "]\n",
      "Numero de opiniones: 10,462\n",
      "======================================================================================================================================================================================================================================\n",
      " Columnas: ['género', 'edad', 'nacional_ó_internacional', 'calificación', 'escala', 'número_de_aportaciones', 'título_de_la_opinión', 'opinión', 'país', 'idioma', 'dispositivo', 'fecha', 'lugar_turistico', 'calificacion_numerica']\n",
      "======================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "lista_df: list[pl.LazyFrame] = []\n",
    "for i, hoja_n in enumerate(hojas):\n",
    "    df = añadir_columna_lugar_turistico(i)\n",
    "    df_p = guardar_parquet(df, hoja_n)\n",
    "    lista_df.append(df)\n",
    "\n",
    "\n",
    "# Unir todos los DataFrames en uno solo (NO SE USA PERO POR SI ACASO)\n",
    "todos_los_df = juntar_df(lista_df)\n",
    "\n",
    "imprimir(f\"{todos_los_df.collect()['opinión'][:5]}\")\n",
    "print(f\"Numero de opiniones: {todos_los_df.collect().height:,}\")\n",
    "\n",
    "\n",
    "\n",
    "separador = f\" Columnas: {todos_los_df.collect().columns}\"\n",
    "imprimir(F\"{\"=\"*len(separador)}\")\n",
    "imprimir(separador)\n",
    "imprimir(F\"{\"=\"*len(separador)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31df28",
   "metadata": {},
   "source": [
    "# CONTRUCCION DEL VOCABULARIO, DISTRIBUCIÓN FRECUENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f155e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Texto:\n",
    "    def __init__(self, lazy_frame: pl.LazyFrame, TOP_PALABRAS:int | None = None):\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        self.lazy_frame = lazy_frame\n",
    "        self.TOP_PALABRAS = TOP_PALABRAS\n",
    "\n",
    "        # Guardamos estados\n",
    "        self._freq_dist: list[tuple[str, int]] = []\n",
    "        self._final_vocab: set[str] = set()\n",
    "        self._corpus: list[list[str]] = []\n",
    "        \n",
    "\n",
    "    \n",
    "    def _tokenizar(self, texto: str) -> list[str]:\n",
    "        tokens = self.tokenizer.tokenize(texto.lower().strip())\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def _tokenizar_data(self) -> pl.LazyFrame:\n",
    "        df_limpio = (\n",
    "            self.lazy_frame\n",
    "            .drop_nulls(subset=[\"opinión\"])\n",
    "            .with_columns(\n",
    "                pl.col(\"opinión\")\n",
    "                .cast(pl.Utf8)\n",
    "                .str.strip_chars()                                 # recorta espacios\n",
    "                .str.replace_all(r'^[“”\"\\'«»\\(\\)\\[\\]\\s]+', '')      # limpia INICIO\n",
    "                .str.replace_all(r'[“”\"\\'«»\\(\\)\\[\\]\\s]+$', '')      # limpia FINAL\n",
    "                .alias(\"opinión_limpia\")\n",
    "            )\n",
    "            .filter(pl.col(\"opinión_limpia\").str.len_chars() > 0)\n",
    "\n",
    "\n",
    "            .with_columns(\n",
    "                pl.col(\"opinión_limpia\")\n",
    "                .map_elements(self._tokenizar, return_dtype=pl.List(pl.Utf8))\n",
    "                .alias(\"opinión_tokenizada\")\n",
    "            )\n",
    "            .filter(pl.col(\"opinión_tokenizada\").list.len() > 0)\n",
    "        )\n",
    "        return df_limpio\n",
    "\n",
    "    \n",
    "    def _distribuir_frecuencias(self) -> pl.LazyFrame:\n",
    "        lf_counts = (\n",
    "            self._tokenizar_data()\n",
    "            .select(pl.col(\"opinión_tokenizada\"))\n",
    "            .explode(\"opinión_tokenizada\")\n",
    "            .group_by(\"opinión_tokenizada\")\n",
    "            .len()\n",
    "            .rename({\n",
    "                \"len\": \"frecuencia\",\n",
    "                \"opinión_tokenizada\": \"token\"\n",
    "                })\n",
    "            .sort(\"frecuencia\", descending=True)\n",
    "        )\n",
    "        return lf_counts\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # ============== VOCABULARIO FINAL ==============\n",
    "    def _vocabulario_final(self) -> pl.LazyFrame:\n",
    "        vocab = (\n",
    "            self._distribuir_frecuencias()\n",
    "            .select(\"token\")\n",
    "        )\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def __build_corpus(self) -> pl.LazyFrame:\n",
    "        vocab_set = self.vocabulario\n",
    "        \n",
    "        PREFIJO_SOS = pl.lit([self.SOS, self.SOS])\n",
    "        SUFIJO_EOS = pl.lit([self.EOS])\n",
    "        UNK_LIT = pl.lit(self.UNK)\n",
    "        VOCAB_LIT = pl.lit(list(vocab_set))\n",
    "\n",
    "        corpus = (\n",
    "            self._tokenizar_data()\n",
    "            .with_row_index(\"rid\")\n",
    "            .select(\"rid\", \"opinión_tokenizada\")\n",
    "            .explode(\"opinión_tokenizada\")\n",
    "            .with_columns(\n",
    "                # Aplica el enmascaramiento\n",
    "                pl.when(pl.col(\"opinión_tokenizada\").is_in(VOCAB_LIT))\n",
    "                .then(pl.col(\"opinión_tokenizada\"))\n",
    "                .otherwise(UNK_LIT)\n",
    "                .alias(\"tok_mask\")\n",
    "            )\n",
    "            # Aquí lo que hacemos es juntar todos los tokens (ya enmascarados) en UNA lista por opinión\n",
    "            .group_by(\"rid\")\n",
    "            .agg(pl.col(\"tok_mask\").alias(\"tokens\"))\n",
    "            .with_columns(\n",
    "                pl.concat_list([PREFIJO_SOS, pl.col(\"tokens\"), SUFIJO_EOS])\n",
    "                .alias(\"opinión_trigram\")\n",
    "            )\n",
    "            .select(\"opinión_trigram\")\n",
    "        )\n",
    "        return corpus\n",
    "    \n",
    "    \n",
    "    # ================ PROPIEDADES ==============\n",
    "\n",
    "    @property\n",
    "    def freq_dist(self) -> list[tuple[str, int]]:\n",
    "        if not self._freq_dist:\n",
    "            df = (\n",
    "                self._distribuir_frecuencias()\n",
    "                .limit(self.TOP_PALABRAS)  # Si es None, no limita\n",
    "                .collect(engine=\"streaming\")\n",
    "                .rows()\n",
    "            )\n",
    "            self._freq_dist = df\n",
    "            self.id_map = {token: idx for idx, (token, _) in enumerate(self._freq_dist)}\n",
    "                     \n",
    "        return self._freq_dist\n",
    "    \n",
    "    @property\n",
    "    def vocabulario(self) -> dict:\n",
    "        if not self._final_vocab:\n",
    "            df = (\n",
    "                self._vocabulario_final()\n",
    "                .select(\"token\")\n",
    "                .collect(engine=\"streaming\")\n",
    "                .to_series()\n",
    "                .to_list()\n",
    "            )\n",
    "            self._final_vocab = set(df)\n",
    "        return self._final_vocab\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def corpus(self) -> list[list[str]]:\n",
    "        \n",
    "        if not self._corpus:\n",
    "            self._corpus = (\n",
    "                self.__build_corpus()\n",
    "                .collect(engine=\"streaming\")\n",
    "                .get_column(\"opinión_trigram\")\n",
    "                .to_list()\n",
    "            )\n",
    "        return self._corpus\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e5a49",
   "metadata": {},
   "source": [
    "## Limpieza\n",
    "Se esta utilizando una limpieza de codigo de manera aislada y preparada, primero insertando token de inicio y de cierre preparando para poder hacer uso del mismo data set\n",
    "Lista de cosas:\n",
    "- Token de inicio y fin de oración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d2a7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = []\n",
    "for df in lista_df:\n",
    "    texto = Texto(df, TOP_PALABRAS=10)\n",
    "    textos.append(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd28c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 560 opiniones\n",
      "['<s>', '<s>', 'basílica', 'muy', 'bien', 'conservada', ',', 'punto', 'central', 'en', 'la', 'ciudad', ',', 'muy', 'linda', 'por', 'dentro', 'vale', 'la', 'pena', 'darse', 'una', 'vuelta', 'para', 'conocer', ',', 'alto', 'significado', 'religioso', 'y', 'arquitectónico', '.', '</s>']\n",
      "['<s>', '<s>', 'the', 'basilica', '(', 'guanajuato', 'does', 'not', 'have', 'a', 'cathedral', ',', 'it', 'is', 'situated', 'in', 'the', 'nearby', 'city', 'of', 'leon', ')', 'is', 'truly', 'imposing', 'both', 'during', 'the', 'day', 'and', 'when', 'illuminated', 'at', 'night', '.', 'it', 'also', 'has', 'some', 'lovely', 'polychrome', 'sculpture', 'and', 'a', 'great', 'organ', '.', '</s>']\n",
      "['<s>', '<s>', 'edificio', 'de', 'la', 'iglesia', 'amarilla', 'es', 'un', 'abigarrado', 'y', 'nahu', 'crear', 'una', 'más', 'bella', 'de', 'atocha', '.', 'he', 'venido', 'a', 'cabo', 'mejor', 'en', 'frente', 'de', 'la', 'misma', 'guanajuato', 'es', 'menor', 'cuando', 'se', 'toman', 'fotografías', 'que', 'gabolman', 'menos', 'una', '.', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 600 opiniones\n",
      "['<s>', '<s>', 'the', 'market', 'under', 'a', 'large', 'metal', 'building', 'is', 'bustling', 'with', 'merchants', 'and', 'noise', '.', 'feel', 'the', 'energy', 'of', 'the', 'city', 'and', 'smell', 'the', 'foods', '.', 'all', 'kinds', 'of', 'merchandise', ',', 'foods', ',', 'snacks', 'and', 'drinks', 'for', 'sale', '.', 'most', 'merchants', 'only', 'speak', 'spanish', 'and', 'take', 'cash', 'only', '.', 'very', 'reasonably', 'priced', '!', '</s>']\n",
      "['<s>', '<s>', 'nicely', 'done', 'market', 'with', 'lots', 'of', 'food', 'stalls', ',', 'produce', ',', 'and', 'meat', '/', 'fish', '/', 'poultry', '.', 'we', 'enjoyed', '(', 'very', 'inexpensive', ')', 'food', 'with', 'locals', '.', 'not', 'a', 'place', 'to', 'go', 'for', 'high', 'end', 'clothing', 'or', 'other', 'things', ',', 'but', 'a', 'variety', 'of', 'gift', 'items', '.', 'and', ',', 'fun', 'people', 'watching', ',', 'including', 'outside', 'the', 'building', '.', '</s>']\n",
      "['<s>', '<s>', 'el', 'primer', 'piso', 'de', 'la', 'segunda', 'planta', ',', 'comida', 'sencilla', 'que', 'están', 'vendiendo', 'recuerdos', '.', 'guanajuato', 'que', 'el', 'precio', 'de', 'las', 'cosas', 'mucho', 'más', 'barato', '.', 'y', 'mucho', 'nahu', 'de', 'las', 'figuras', 'de', 'cerámica', 'para', 'decorar', 'la', 'casa', 'de', 'atocha', '.', 'ir', 'a', 'la', 'ciudad', 'de', 'méxico', 'haría', 'mothaetneunde', 'no', 'comprar', 'un', 'montón', 'de', 'comprar', 'en', 'la', 'ciudad', 'de', 'méxico', 'lo', 'hizo', 'olgeol', 'cuatro', 'más', '.', 'había', 'un', 'pequeño', 'cráneo', 'visto', 'los', 'precios', 'de', 'venta', 'se', 'duplicó', 'en', 'teotihuacan', '.', 'visita', 'a', 'lugares', 'aquí', 'y', 'allá', 'llegó', 'la', 'noche', 'ni', 'para', 'comprar', 'solicitando', 'descender', 'dando', 'varias', 'cuidado', 'en', 'vivo', '.', 'en', 'el', 'primer', 'piso', 'de', 'aterrizaje', '15', 'dio', 'lugar', 'carta', 'copa', 'sama', 'pesos', 'corte', 'era', 'pequeño', ',', 'demasiado', 'bueno', 'dos', 'veces', '.', 'enmascarado', '5:00', 'de', 'la', 'mañana', 'también', 'estoy', 'cerrando', 'las', 'tiendas', 'de', 'las', 'puertas', '.', 'así', 'que', 'no', 'van', 'demasiado', 'tarde', '.', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 698 opiniones\n",
      "['<s>', '<s>', 'interesting', 'small', 'museum', '.', 'the', 'downstairs', 'is', 'the', 'house', 'that', 'he', 'was', 'born', 'in', ',', 'complete', 'with', 'turn-of-the', 'century', 'decor', 'and', 'furnishings', '.', 'upstairs', 'there', 'are', 'galleries', 'of', 'his', 'mostly', 'early', 'career', 'paintings', '.', 'many', 'impressionism', ',', 'post-impressionism', ',', 'and', 'cubist', 'works', '.', 'i', 'found', 'the', 'portraits', 'most', 'compelling', '.', '</s>']\n",
      "['<s>', '<s>', 'ubicado', 'en', 'la', 'zona', 'turística', 'la', 'casa', 'donde', 'el', 'famoso', 'pintor', 'vivió', 'una', 'etapa', 'de', 'su', 'infancia', ',', 'cuenta', 'con', 'réplicas', 'de', 'los', 'muebles', 'que', 'en', 'su', 'época', 'estuvieron', 'acomodados', 'en', 'cada', 'habitación', ',', 'pinturas', 'originales', ',', 'una', 'de', 'las', 'más', 'destacadas', 'la', 'que', 'realizó', 'a', 'los', '11', 'años', 'que', 'fue', 'la', 'que', 'lo', 'colocó', 'con', 'los', 'grandes', 'maestros', 'en', 'europa', ',', 'vale', 'la', 'pena', 'realizar', 'un', 'recorrido', 'con', 'guía', '.', '</s>']\n",
      "['<s>', '<s>', 'la', 'exposición', 'de', 'obras', 'de', 'diego', 'rivera', 'es', 'muy', 'buena', ',', 'que', 'es', 'el', 'atractivo', 'principal', 'del', 'museo', '...', 'sin', 'embargo', 'las', 'exposiciones', 'temporales', 'de', 'otros', 'artistas', 'en', 'las', 'salas', 'contiguas', 'también', 'son', 'una', 'buena', 'opción', '…', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 900 opiniones\n",
      "['<s>', '<s>', 'es', 'una', 'construcción', 'imponente', 'desde', 'que', 'te', 'vas', 'acercando', 'y', 'la', 'ves', 'a', 'lo', 'lejos', 'te', 'sorprende', 'su', 'magnifica', 'arquitectura', '.', 'tiene', 'una', 'gran', 'cantidad', 'de', 'escalones', 'en', 'los', 'cuales', 'al', 'llegar', 'a', 'lo', 'mas', 'alto', 'podrás', 'ver', 'una', 'parte', 'de', 'la', 'ciudad', '.', '</s>']\n",
      "['<s>', '<s>', 'the', 'steps', 'are', 'beautiful', 'for', 'views', 'and', 'exersize', 'but', 'use', 'the', 'opportunity', 'to', 'walk', 'inside', ',', 'watch', 'the', 'old', 'library', 'and', 'the', 'museum', ',', 'art', 'exhibitions', ',', 'the', 'old', 'fantastic', 'doors', 'and', 'architecture', ',', 'the', 'wandering', 'students', 'and', 'professors', ',', 'sometimes', 'they', 'show', 'movies', 'and', 'other', 'activities', 'for', 'free', 'that', 'may', 'be', 'of', 'interest', ',', 'and', 'if', 'you', 'speak', 'some', 'spanish', 'people', 'are', 'more', 'than', 'willing', 'to', 'give', 'you', 'information', 'about', 'the', 'attractions', 'of', 'this', 'marvellous', 'campus', '.', '</s>']\n",
      "['<s>', '<s>', 'el', 'lugar', 'es', 'fantástico', 'y', 'tiene', 'una', 'envidia', 'arquitectónico', '.', 'para', 'las', 'personas', 'que', 'disfrutan', 'de', 'los', 'edificios', 'que', 'miran', 'bien', 'hecho', 'y', 'tomar', 'fotos', 'con', 'un', 'fondo', 'hermoso', 'paisaje', ',', 'es', 'un', 'lugar', 'maravilloso', '.', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 930 opiniones\n",
      "['<s>', '<s>', 'fue', 'construida', 'a', 'finales', 'del', 'siglo', 'xviii', ',', 'en', 'el', 'mexico', 'virreinal', '.', 'su', 'función', 'era', 'como', 'almacén', 'y', 'tienda', 'de', 'comercio', 'para', 'los', 'habitantes', 'que', 'acudían', 'del', 'campo', 'a', 'comprar', 'o', 'de', 'las', 'haciendas', 'a', 'vender', 'sus', 'productos', '.', '</s>']\n",
      "['<s>', '<s>', 'parada', 'obligatoria', 'en', 'tu', 'visita', 'a', 'la', 'ciudad', 'de', 'guanajuato', ',', 'lugar', 'lleno', 'de', 'historia', 'y', 'apto', 'para', 'todas', 'las', 'edades', '.', '</s>']\n",
      "['<s>', '<s>', 'en', 'este', 'lugar', 'se', 'dio', 'una', 'de', 'las', 'batallas', 'mas', 'lamentables', 'de', 'la', 'guerra', 'de', 'independencia', '.', 'dentro', 'de', 'la', 'alhondiga', 'se', 'encontraban', 'familias', 'españolas', '.', 'las', 'las', 'cuales', 'murieron', 'cruelmente', 'a', 'manos', 'de', 'los', 'indios', 'a', 'cargo', 'de', 'hidalgo', '.', 'el', 'pararse', 'frente', 'a', 'este', 'imponente', 'monumento', 'e', 'imaginarse', 'los', 'ríos', 'de', 'sangre', 'de', 'hombres', 'y', 'niños', 'que', 'tuvieron', 'una', 'muerte', 'cruel', 'es', 'en', 'verdad', 'desgarrador.no', 'vayas', 'solamente', 'a', 'tomarte', 'la', 'foto', '.', 'observa', 'y', 'recrea', 'estos', 'hechos', 'en', 'tu', 'mente', 'para', 'que', 'así', 'te', 'des', 'cuenta', 'de', 'la', 'importancia', 'de', 'este', 'lugar', 'y', 'porque', 'no', 'debe', 'de', 'ser', 'olvidado', '.', 'y', 'claro', 'también', 'aquí', 'se', 'dio', 'la', 'famosa', 'historia', 'del', 'pipila', '.', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 1,010 opiniones\n",
      "['<s>', '<s>', 'beautiful', 'historic', 'theater', '-', 'great', 'place', 'for', 'a', 'concert', '.', 'unfortunately', 'street', 'performers', 'noise', 'bleeds', 'through', 'and', 'is', 'very', 'distracting', '.', '</s>']\n",
      "['<s>', '<s>', 'this', 'is', 'the', 'second', 'best', 'and', 'nice', 'theater', 'in', 'mexico', '.', 'love', 'this', 'place', 'you', 'can', 'take', 'pictures', 'whit', 'out', 'a', 'problem', 'not', 'like', \"bella's\", 'artes', 'they', 'charge', 'over', 'there', 'is', 'cheap', 'to', 'see', 'inside', 'this', 'gorgeous', 'place', '.', '</s>']\n",
      "['<s>', '<s>', 'para', 'aquellos', 'que', 'conocen', 'muchos', 'países', ',', 'el', 'teatro', 'juárez', 'es', 'más', 'que', 'otro', 'teatro', ',', 'bastante', 'dudoso', 'gusto', 'arquitectónico', 'y', 'decoración', '.', 'tiene', 'una', 'historia', 'interesante', ',', 'pero', 'lejos', 'de', 'merecer', 'el', 'primer', 'lugar', 'en', 'tripadvidor', '.', 'bien', 'vale', 'la', 'pena', 'la', 'visita', 'en', 'el', 'contexto', 'de', 'una', 'visita', 'al', 'centro', 'de', 'guanajuato', '..', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 1,134 opiniones\n",
      "['<s>', '<s>', 'a', 'picturesque', 'spot', 'to', 'relax', 'with', 'an', 'ice', 'cream', 'and', 'watch', 'the', 'residents', 'of', 'the', 'city', 'go', 'about', 'their', 'day', '.', '</s>']\n",
      "['<s>', '<s>', 'it', 'was', 'like', 'being', 'in', 'a', 'european', 'town', '.', 'when', 'i', 'stepped', 'out', 'of', 'the', 'cab', 'after', 'taking', 'a', 'bus', 'from', 'sma', 'to', 'the', 'bus', 'station', ',', 'i', 'was', 'completely', 'stunned', 'thinking', 'did', 'i', 'just', 'step', 'into', 'europe', '?', 'the', 'jardin', 'was', 'compact', ',', 'clean', ',', 'beautifully', 'landscaped', 'and', 'a', 'pleasant', 'surprise', '.', 'lovely', 'restaurants', 'around', 'it', 'and', 'plenty', 'of', 'benches', 'and', 'places', 'to', 'just', 'sit', 'and', 'enjoy', 'yourself', '.', 'must', 'see', 'attraction', '.', '</s>']\n",
      "['<s>', '<s>', 'very', 'scenic', 'plaza', 'with', 'many', 'bars', '/', 'restaurants', 'around', 'it', '.', 'a', 'lot', 'of', 'street', 'performers', 'and', 'merchants', 'in', 'this', 'area', '.', 'it', 'is', 'a', 'real', 'tourist', 'location', 'and', 'well', 'worth', 'spending', 'time', 'here', '.', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 1,360 opiniones\n",
      "['<s>', '<s>', 'la', 'ubicación', 'aquí', 'es', 'perfecta', 'para', 'tomar', 'fotos', '.', 'el', 'equipo', 'se', 'mueve', 'muy', 'rápido', ',', 'por', 'lo', 'que', 'no', 'tiene', 'que', 'sentirse', 'desanimado', 'por', 'la', 'cola', '.', 'tienen', 'un', 'fotógrafo', 'que', 'toma', 'fotos', 'con', 'una', 'cámara', 'profesional', 'por', 'una', 'tarifa', ',', 'pero', 'también', 'puede', 'tomar', 'fotos', 'con', 'su', 'propia', 'cámara', 'o', 'teléfono', 'móvil', '.', 'si', 'estás', 'en', 'guanajuato', ',', 'este', 'es', 'uno', 'de', 'los', 'lugares', 'que', 'debes', 'visitar', '.', '</s>']\n",
      "['<s>', '<s>', 'great', 'location', 'for', 'a', 'photo', '.', 'lines', 'move', 'quickly', 'so', \"don't\", 'get', 'discouraged', 'by', 'the', 'line', '.', 'they', 'have', 'photographer', 'that', 'will', 'take', 'your', 'photo', 'with', 'a', 'professional', 'camera', 'for', 'a', 'fee', 'but', 'you', 'can', 'also', 'have', 'someone', 'take', 'a', 'photo', 'with', 'your', 'own', 'camera', 'or', 'cell', 'phone', '.', 'one', 'of', 'those', 'sites', 'that', 'is', 'a', 'must', 'if', 'you', 'are', 'ever', 'in', 'guanajuato', '.', '</s>']\n",
      "['<s>', '<s>', 'por', 'lo', 'general', ',', 'los', 'edificios', 'están', 'muy', 'cerca', ',', 'pero', 'es', 'especial', 'porque', 'los', 'balcones', 'a', 'ambos', 'lados', 'de', 'la', 'acera', 'se', 'tocan', 'entre', 'sí', '.', 'es', 'muy', 'bueno', 'tomar', 'fotos', 'en', 'este', 'callejón', ',', 'los', 'colores', 'son', 'muy', 'coloridos', ',', 'de', 'pie', 'en', 'los', 'escalones', 'rojos', ',', 'no', 'hay', 'turistas', ',', 'si', 'se', 'pierde', 'un', 'grupo', 'de', 'turistas', 'que', 'pueden', 'ayudarlo', 'a', 'tomar', 'fotos', ',', 'habrá', 'otras', 'personas', ',', 'si', 'es', 'pareja', 'amantes', ',', 'puedes', 'pedirle', 'a', 'otros', 'turistas', 'que', 'te', 'ayuden', 'a', 'tomar', 'fotos', '.', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 1,620 opiniones\n",
      "['<s>', '<s>', 'hermosa', 'vista', 'de', 'la', 'ciudad', 'y', 'bonito', 'paseo', 'en', 'el', 'funicular', '.', 'seguro', 'para', 'viajar', 'en', 'familia', 'y', 'además', 'de', 'contar', 'con', 'diversos', 'antojitos', 'mexicanos', 'en', 'la', 'parte', 'superior', '.', '</s>']\n",
      "['<s>', '<s>', 'the', 'view', 'is', 'an', 'easy', '5', 'stars', '.', 'you', 'will', 'see', 'many', 'colorful', 'houses', 'and', 'brightly', 'painted', 'buildings', 'below', '.', 'also', ',', 'great', 'views', 'of', 'the', 'historical', 'town', 'center', '.', 'not', 'to', 'be', 'missed', '.', '</s>']\n",
      "['<s>', '<s>', '¡', 'nos', 'gusta', 'mucho', '!', 'aquí', 'verás', 'increíbles', 'vistas', 'de', 'la', 'ciudad', '.', 'asegúrese', 'de', 'venir', 'aquí', 'para', 'tomar', 'una', 'foto', '.', 'hay', 'varios', 'restaurantes', 'de', 'comida', 'rápida', 'y', 'tiendas', 'de', 'recuerdos', 'aquí', '.', 'llegamos', 'a', 'esta', 'atracción', 'en', 'teleférico', '(', 'verá', 'otra', 'gran', 'vista', 'en', 'el', 'teleférico', ')', '.', 'cuando', 'bajamos', 'de', 'la', 'montaña', ',', 'tomamos', 'un', 'taxi', 'porque', 'el', 'viaje', 'estaba', 'bastante', 'lejos', '.', '</s>']\n",
      "========== CORPUS ==========\n",
      "Tamaño del corpus: 1,650 opiniones\n",
      "['<s>', '<s>', 'the', 'museum', 'is', 'a', 'collection', 'of', 'bodies', 'that', 'were', 'unintentionally', 'mummified', '.', 'though', 'the', 'mummies', 'are', 'ok', ',', 'the', 'explanation', 'of', 'how', 'they', 'died', 'or', 'what', 'happened', 'to', 'them', 'after', 'is', 'a', 'bit', 'creepy', 'for', 'smaller', 'children', '.', 'interesting', 'thing', 'to', 'do', 'and', 'a', 'must', 'in', 'guanajuato', '.', '</s>']\n",
      "['<s>', '<s>', 'this', 'museum', 'is', 'essentially', 'a', 'very', 'long', 'display', 'of', 'mummies', '.', 'they', 'try', 'to', 'give', 'them', 'different', 'angles', ',', 'explaining', 'some', 'of', 'the', 'history', 'as', 'well', 'as', 'the', 'reasons', 'bodies', 'end', 'up', 'in', 'that', 'state', '.', 'it', 'gets', 'a', 'bit', 'repetitive', 'after', 'a', 'while', ',', 'but', \"it's\", 'still', 'a', 'cool', 'close', 'look', 'at', 'mummies', ',', 'which', \"isn't\", 'something', 'you', 'see', 'every', 'day', '.', '</s>']\n",
      "['<s>', '<s>', 'guanajuato', 'dijo', 'que', 'no', 'había', 'espacio', 'para', 'el', 'entierro', 'en', 'el', 'pueblo', 'de', 'la', 'montaña', ',', 'y', 'en', 'el', 'pasado', 'se', 'decía', 'que', 'en', 'lugar', 'de', 'enterrarlo', ',', 'el', 'cuerpo', 'se', 'colocaba', 'en', 'un', 'túnel', 'donde', 'se', 'terminaba', 'la', 'excavación', '.', 'fue', 'este', 'museo', 'que', 'una', 'persona', 'de', 'la', 'posteridad', 'que', 'vio', 'que', 'el', 'cuerpo', 'se', 'secaba', 'bien', 'con', 'un', 'clima', 'seco', 'luego', 'reunió', 'los', 'cuerpos', '.', 'todo', 'está', 'bien', 'seco', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for texto in textos:\n",
    "    print(f\"{\"=\"*10} CORPUS {\"=\"*10}\")\n",
    "    corpus = texto.corpus\n",
    "    print(f\"Tamaño del corpus: {len(corpus):,} opiniones\")\n",
    "    for opinion in corpus[:3]:\n",
    "        print(opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a1ae89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 1244), (',', 1035), ('de', 918), ('la', 751), ('the', 636), ('a', 470), ('y', 431), ('en', 415), ('es', 371), ('el', 323)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[(',', 1542), ('.', 1523), ('de', 914), ('the', 619), ('a', 591), ('y', 567), ('la', 478), ('and', 409), ('que', 398), ('es', 388)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 1979), (',', 1280), ('de', 1135), ('the', 1029), ('of', 732), ('la', 705), ('a', 695), ('and', 571), ('diego', 501), ('y', 495)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 1727), ('de', 1539), (',', 1486), ('la', 1301), ('y', 741), ('que', 684), ('es', 663), ('a', 605), ('the', 555), ('en', 520)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 2177), (',', 1928), ('de', 1924), ('la', 1430), ('the', 1120), ('que', 973), ('y', 821), ('a', 777), ('en', 673), ('es', 662)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 2418), (',', 1954), ('de', 1602), ('la', 1140), ('y', 913), ('the', 904), ('a', 891), ('que', 791), ('el', 786), ('es', 757)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[(',', 2847), ('.', 2694), ('de', 1682), ('the', 1443), ('a', 1177), ('y', 1080), ('la', 956), ('and', 921), ('el', 722), ('en', 702)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 2885), (',', 2639), ('de', 1978), ('la', 1801), ('que', 1657), ('y', 1474), ('a', 1359), ('es', 1171), ('el', 1076), ('en', 1054)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 4174), (',', 3275), ('la', 2853), ('de', 2815), ('the', 2422), ('el', 1517), ('a', 1488), ('que', 1411), ('y', 1339), ('es', 1320)]\n",
      "========== FREQ_DIST ==========\n",
      "Tamaño de la distribución de frecuencias: 10 tokens\n",
      "[('.', 5132), (',', 4342), ('de', 2831), ('que', 2273), ('the', 2027), ('a', 1923), ('la', 1746), ('y', 1733), ('es', 1483), ('el', 1408)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for texto in textos:\n",
    "    print(f\"{\"=\"*10} FREQ_DIST {\"=\"*10}\")\n",
    "    freq_dist = texto.freq_dist\n",
    "    print(f\"Tamaño de la distribución de frecuencias: {len(freq_dist):,} tokens\")\n",
    "    print(freq_dist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc2983e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 3,939\n",
      "['concentración', 'kind', 'ojo', 'incorporación', 'marzo', 'base', 'cantando', 'debe', 'aunque', 'tal']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 4,399\n",
      "['americanized', 'friendliness', 'kind', 'dry', 'ojo', 'vendan', 'base', 'debe', 'tal', 'aunque']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 4,535\n",
      "['concentración', 'kind', 'marzo', 'base', 'thorough', 'debe', 'aunque', 'tal', 'contiene', 'consiga']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 4,481\n",
      "['concentración', 'ojo', 'oustide', 'base', 'cardio', 'cantando', 'debe', 'aunque', 'tal', 'contiene']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 5,776\n",
      "['contiene', 'acompaña', 'rastros', 'informo', 'garcia', 'orientan', 'encajes', 'desk', 'artistas', 'tienda']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 5,420\n",
      "['sleep', 'contiene', 'mxn', 'rome', 'giff', 'artistas', 'saques', 'her', 'none', 'state']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 5,546\n",
      "['frecuentemente', 'inunda', 'participaste', 'rome', 'vie', 'artistas', 'her', 'longer', 'none', 'state']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 6,187\n",
      "['contiene', 'acompaña', 'tardecita', 'presentaron', 'tomarán', 'rome', 'tienda', 'her', 'callejons', 'cómico']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 6,918\n",
      "['plana', 'likes', 'liking', 'cardio', 'breathing', 'contiene', 'fanicula', 'tardecita', 'sacadas', 'seige']\n",
      "========== VOCABULARIO ==========\n",
      "Tamaño del vocabulario: 9,068\n",
      "['sleep', 'likes', 'liking', 'crematorio', 'contiene', 'breathing', 'acompaña', 'constantes', 'sentimental', 'mxn']\n"
     ]
    }
   ],
   "source": [
    "for texto in textos:\n",
    "    print(f\"{\"=\"*10} VOCABULARIO {\"=\"*10}\")\n",
    "    vocabulario = texto.vocabulario\n",
    "    print(f\"Tamaño del vocabulario: {len(vocabulario):,}\")\n",
    "    # Imprimir solo los primeros 20 elementos del vocabulario (es un set)\n",
    "    print(list(vocabulario)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbb543",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dde49ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecorPalabra:\n",
    "    def __init__(self, texto: Texto, top_k: int = 50):\n",
    "        self.texto = texto\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    @property # TODO: ESTO VA en lo de bow\n",
    "    def k_best(self):\n",
    "        \"basilica_colegiata_560,mercado_hidalgo_600,casa_de_diego_rivera_698,universidad_de_guanajuato_900,alhóndiga_930,teatro_juárez_1,010,jardín_de_la_unión_1,134,callejón_del_beso_1,360,monumento_pípila_1,620,museo_de_las_momias_1,650\"\n",
    "        y_train = hojas\n",
    "        \n",
    "        \n",
    "        feature_selector = SelectKBest(score_func=chi2, k=50)\n",
    "        feature_selector.fit(self._bow_train_frecuencia(), y_train)\n",
    "        return feature_selector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
