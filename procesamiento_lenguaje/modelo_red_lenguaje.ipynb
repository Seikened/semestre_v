{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb00ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601090d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedd = 1111\n",
    "random.seed(sedd)\n",
    "np.random.seed(sedd)\n",
    "torch.manual_seed(sedd)\n",
    "torch.backends.cudnn.benchmark = False # ????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b0ae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de ejemplos de entrenamiento: 5278\n",
      "['@USUARIO @USUARIO @USUARIO Q se puede esperar del maricon de closet de la Ya√±ez aun recuerdo esa ves q lo vi en zona rosa viendo quien lo levantada', '@USUARIO La piel nueva siempre arde un poquito los primeros d√≠as... y m√°s con este puto clima', 'Ustedes no se enamoran de m√≠‚Ä¶ por tontas.', 'Me las va a pagar esa puta gorda roba tuits...', '@USUARIO LA GENTE ES TONTA PORQUE NO SE DAN CUENTA QUE T√ö HACES A BATMAN AZUL', 'Estoy muy encabronada con las pseudo feministas por tontas e iletradas, a veces me averg√ºenza ser mujer; preferir√≠a tener un falo. #NiUnaMas', 'Anden putos, recuerdan el #noerapenal #Holanda fuera de #Rusia2018, esto se llama #karma ehhhhhhhh #puuuuuutos', 'Si no tienen chichis no traten de ense√±ar se ven muy mal y m√°s cuando son prietas.', 'Ojal√° asi me agarrars cuando te digo que me voy en lugar de correrme a la verga cada 5 minutos.', '@USUARIO @USUARIO @USUARIO @USUARIO Es solo un HDP aprovechado y que su \"Diosito Bimbo\" me perdone']\n",
      "N√∫mero de ejemplos de validaci√≥n: 587\n",
      "['Al perro que se te acerque le parto su madre a si de facil', '@USUARIO @USUARIO √âl supo sacar a su familia adelante en lo que sabe en el mundo existen muchas personas ardidas como t√∫ ü§∑üèª\\u200d‚ôÄÔ∏è', '@USUARIO Entonces para que quieres estar en sus paises?, mejor vente aca y chinguele cabrona de verdad, maldita sangana.', 'Que bueno que hoy juega M√©xico, porque t√∫ vales verga, hija de la chingada.', 'Ojal√° un d√≠a me valgas la misma verga que al vato de la cfe despelucando y ponchando a medio del puente del puente a Cd judicial', 'Hoy mi outfit fue de licenciado/ pap√°... Padre que no vale verga y licenciado sin trabajo.', 'Me pregunto donde Chucha esta el supertanker   de la loca @USUARIO apagando estos incendios', 'Por un mega-error escuch√© parte de una canci√≥n de banda y decia \"Demosle vuelo a la hilacha\" Que putas madres üòÇüòÇ #PenaAjena', 'En M√©xico estaremos de la verga pero al menos no estamos de la verga con Trump dirigiendo al pa√≠s.', 'Nomames al principio pense que cancer de pacientes con cancer, ya les iba a mentar la madrea no seas asi']\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.read_csv(\"data/mex/mex20_train.txt\", sep='\\r\\n', engine='python', header=None).loc[:,0].values.tolist()\n",
    "x_val = pd.read_csv(\"data/mex/mex20_val.txt\", sep='\\r\\n', engine='python', header=None).loc[:,0].values.tolist()\n",
    "print(\"N√∫mero de ejemplos de entrenamiento:\", len(x_train))\n",
    "print(x_train[:10])\n",
    "print(\"N√∫mero de ejemplos de validaci√≥n:\", len(x_val))\n",
    "print(x_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b718999",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833f5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_excluidas = set(['.', ',', ';', ':', '!', '?', '¬ø', '¬°', '\"', \"'\", '(', ')', '[', ']', '{', '}', '-', '_', '‚Äî', '...',\n",
    "                       '@', '#', '$', '%', '^', '&', '*', '/', '|', '~', '`', '<', '>', '¬´', '¬ª', '‚Äú', '‚Äù', '‚Äò', '‚Äô','<url>','<@usuario>',\n",
    "                       \n",
    "                       ])\n",
    "\n",
    "class NgramData:\n",
    "    def __init__(self, N: int, vocab_size: int, tokenizer = None, embeddinds_model = None):\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else self.default_tokenizer\n",
    "        self.punct = lista_excluidas\n",
    "        self.N = N\n",
    "        self.vocab_size = vocab_size\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.embeddinds_model = embeddinds_model # TODO: implementar\n",
    "    \n",
    "        \n",
    "    def default_tokenizer(self, text: str) -> list:\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isdigit()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist()\n",
    "        for sentence in corpus:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            tokens = [token.lower() for token in tokens if not self.remove_word(token)]\n",
    "            freq_dist.update(tokens)\n",
    "        most_common = freq_dist.most_common(self.vocab_size - 3)  # Tengo que reservar espacio para UNK, SOS, EOS\n",
    "        vocab = set([word for word, _ in most_common])\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "        \n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        \n",
    "        if self.embeddinds_model is not None:\n",
    "            self.embedding_matriz = np.empty([len(self.vocab), self.embeddinds_model.vector_size])\n",
    "            \n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and  word_ not in self.word_to_id:\n",
    "                    self.word_to_id[word_] = id\n",
    "                    self.id_to_word[id] = word_\n",
    "                    \n",
    "                    if self.embeddinds_model is not None:\n",
    "                        if word_ in self.embeddinds_model:\n",
    "                            self.embedding_matriz[id] = self.embeddinds_model[word_]\n",
    "                        else:\n",
    "                            self.embedding_matriz[id] = np.random.normal(self.embeddinds_model.vector_size)\n",
    "                    id += 1\n",
    "\n",
    "        self.word_to_id.update({self.UNK: id, self.SOS: id + 1, self.EOS: id + 2})\n",
    "        self.id_to_word.update({id: self.UNK, id + 1: self.SOS, id + 2: self.EOS})\n",
    "    \n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n",
    "    \n",
    "    \n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    \n",
    "    def transform(self, corpus: list) -> tuple[np.ndarray, np.ndarray]:\n",
    "        x_ngrams = []\n",
    "        y = []\n",
    "        \n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_in_window in doc_ngram:\n",
    "                words_in_window_ids = [self.word_to_id[w] for w in words_in_window]\n",
    "                x_ngrams.append(list(words_in_window_ids[:-1]))\n",
    "                y.append(words_in_window_ids[-1])\n",
    "        \n",
    "        return np.array(x_ngrams), np.array(y)\n",
    "    \n",
    "    \n",
    "    # =========== PROPOEDADES ===========    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "        \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cee7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "\n",
    "ngram_data = NgramData(args.N, 5_000, tokenizer=tk.tokenize)\n",
    "ngram_data.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15816d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del vocabulario: 5,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tama√±o del vocabulario: {ngram_data.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703ead21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ngram_train, y_ngram_train = ngram_data.transform(x_train)\n",
    "x_ngram_val, y_ngram_val = ngram_data.transform(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f10d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4998, 4998, 4998],\n",
       "       [4998, 4998,    0],\n",
       "       [4998,    0,    0],\n",
       "       ...,\n",
       "       [4997,  937,   32],\n",
       "       [ 937,   32, 2524],\n",
       "       [  32, 2524, 4997]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39281614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ..., 2524, 4997, 4999])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea0e37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAMA√ëO DE LOS NGRAMS DE ENTRENAMIENTO\n",
      "x_ngram_train: (102751, 3)\n",
      "y_ngram_train: (102751,)\n",
      "TAMA√ëO DE LOS NGRAMS DE VALIDACI√ìN\n",
      "x_ngram_val: (11558, 3)\n",
      "y_ngram_val: (11558,)\n"
     ]
    }
   ],
   "source": [
    "# Tama√±os de los ngrams\n",
    "x_train_shape = x_ngram_train.shape\n",
    "y_train_shape = y_ngram_train.shape\n",
    "\n",
    "x_val_shape = x_ngram_val.shape\n",
    "y_val_shape = y_ngram_val.shape\n",
    "print(\"TAMA√ëO DE LOS NGRAMS DE ENTRENAMIENTO\")\n",
    "print(f\"x_ngram_train: {x_train_shape}\")\n",
    "print(f\"y_ngram_train: {y_train_shape}\")\n",
    "print(\"TAMA√ëO DE LOS NGRAMS DE VALIDACI√ìN\")\n",
    "print(f\"x_ngram_val: {x_val_shape}\")\n",
    "print(f\"y_ngram_val: {y_val_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef90c97",
   "metadata": {},
   "source": [
    "nota: creo que los tama√±os varian segun la lista de palabras excluidas que tengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8303ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['<s>', '<s>', '<s>']\n",
      "2: ['<s>', '<s>', '@usuario']\n",
      "3: ['<s>', '@usuario', '@usuario']\n",
      "4: ['@usuario', '@usuario', '@usuario']\n",
      "5: ['@usuario', '@usuario', 'q']\n",
      "6: ['@usuario', 'q', 'se']\n",
      "7: ['q', 'se', 'puede']\n",
      "8: ['se', 'puede', 'esperar']\n",
      "9: ['puede', 'esperar', 'del']\n",
      "10: ['esperar', 'del', 'maricon']\n",
      "11: ['del', 'maricon', 'de']\n",
      "12: ['maricon', 'de', 'closet']\n",
      "13: ['de', 'closet', 'de']\n",
      "14: ['closet', 'de', 'la']\n",
      "15: ['de', 'la', 'ya√±ez']\n",
      "16: ['la', 'ya√±ez', 'aun']\n",
      "17: ['ya√±ez', 'aun', 'recuerdo']\n",
      "18: ['aun', 'recuerdo', 'esa']\n",
      "19: ['recuerdo', 'esa', 'ves']\n",
      "20: ['esa', 'ves', 'q']\n",
      "21: ['ves', 'q', 'lo']\n",
      "22: ['q', 'lo', 'vi']\n"
     ]
    }
   ],
   "source": [
    "lista_palabras = [[ngram_data.id_to_word[w]  for w in tw] for tw in x_ngram_train[:22]]\n",
    "for i, palabras in enumerate(lista_palabras):\n",
    "    print(f\"{i+1}: {palabras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40142756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ..., 2524, 4997, 4999])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed657786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: @usuario\n",
      "2: @usuario\n",
      "3: @usuario\n",
      "4: q\n",
      "5: se\n",
      "6: puede\n",
      "7: esperar\n",
      "8: del\n",
      "9: maricon\n",
      "10: de\n",
      "11: closet\n",
      "12: de\n",
      "13: la\n",
      "14: ya√±ez\n",
      "15: aun\n",
      "16: recuerdo\n",
      "17: esa\n",
      "18: ves\n",
      "19: q\n",
      "20: lo\n",
      "21: vi\n",
      "22: en\n"
     ]
    }
   ],
   "source": [
    "lista_palbras_en_sus_ys = [ngram_data.id_to_word[w] for w in y_ngram_train[:22]]\n",
    "for i, palabra in enumerate(lista_palbras_en_sus_ys):\n",
    "    print(f\"{i+1}: {palabra}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82224a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 64\n",
    "args.num_workers = 2\n",
    "DTYPE = torch.int64\n",
    "\n",
    "def tensor_dataset(x: np.ndarray, y: np.ndarray) -> TensorDataset:\n",
    "    tensor_data = TensorDataset(\n",
    "        torch.tensor(x, dtype=DTYPE),\n",
    "        torch.tensor(y, dtype=DTYPE))\n",
    "    return tensor_data\n",
    "    \n",
    "def data_loader(dataset: TensorDataset, shuffle: bool) -> DataLoader:\n",
    "    dataloader = DataLoader(dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=shuffle)\n",
    "    return dataloader\n",
    "    \n",
    "    \n",
    "# Crear los DataLoaders\n",
    "train_dataset = tensor_dataset(x_ngram_train, y_ngram_train)\n",
    "\n",
    "train_loader = data_loader(train_dataset, shuffle=True)\n",
    "\n",
    "val_dataset = tensor_dataset(x_ngram_val, y_ngram_val)\n",
    "\n",
    "val_loader = data_loader(val_dataset, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
