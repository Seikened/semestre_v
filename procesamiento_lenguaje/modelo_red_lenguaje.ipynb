{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb00ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601090d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedd = 1111\n",
    "random.seed(sedd)\n",
    "np.random.seed(sedd)\n",
    "torch.manual_seed(sedd)\n",
    "torch.backends.cudnn.benchmark = False # ????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b0ae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de ejemplos de entrenamiento: 5278\n",
      "['@USUARIO @USUARIO @USUARIO Q se puede esperar del maricon de closet de la Ya√±ez aun recuerdo esa ves q lo vi en zona rosa viendo quien lo levantada', '@USUARIO La piel nueva siempre arde un poquito los primeros d√≠as... y m√°s con este puto clima', 'Ustedes no se enamoran de m√≠‚Ä¶ por tontas.', 'Me las va a pagar esa puta gorda roba tuits...', '@USUARIO LA GENTE ES TONTA PORQUE NO SE DAN CUENTA QUE T√ö HACES A BATMAN AZUL', 'Estoy muy encabronada con las pseudo feministas por tontas e iletradas, a veces me averg√ºenza ser mujer; preferir√≠a tener un falo. #NiUnaMas', 'Anden putos, recuerdan el #noerapenal #Holanda fuera de #Rusia2018, esto se llama #karma ehhhhhhhh #puuuuuutos', 'Si no tienen chichis no traten de ense√±ar se ven muy mal y m√°s cuando son prietas.', 'Ojal√° asi me agarrars cuando te digo que me voy en lugar de correrme a la verga cada 5 minutos.', '@USUARIO @USUARIO @USUARIO @USUARIO Es solo un HDP aprovechado y que su \"Diosito Bimbo\" me perdone']\n",
      "N√∫mero de ejemplos de validaci√≥n: 587\n",
      "['Al perro que se te acerque le parto su madre a si de facil', '@USUARIO @USUARIO √âl supo sacar a su familia adelante en lo que sabe en el mundo existen muchas personas ardidas como t√∫ ü§∑üèª\\u200d‚ôÄÔ∏è', '@USUARIO Entonces para que quieres estar en sus paises?, mejor vente aca y chinguele cabrona de verdad, maldita sangana.', 'Que bueno que hoy juega M√©xico, porque t√∫ vales verga, hija de la chingada.', 'Ojal√° un d√≠a me valgas la misma verga que al vato de la cfe despelucando y ponchando a medio del puente del puente a Cd judicial', 'Hoy mi outfit fue de licenciado/ pap√°... Padre que no vale verga y licenciado sin trabajo.', 'Me pregunto donde Chucha esta el supertanker   de la loca @USUARIO apagando estos incendios', 'Por un mega-error escuch√© parte de una canci√≥n de banda y decia \"Demosle vuelo a la hilacha\" Que putas madres üòÇüòÇ #PenaAjena', 'En M√©xico estaremos de la verga pero al menos no estamos de la verga con Trump dirigiendo al pa√≠s.', 'Nomames al principio pense que cancer de pacientes con cancer, ya les iba a mentar la madrea no seas asi']\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.read_csv(\"data/mex/mex20_train.txt\", sep='\\r\\n', engine='python', header=None).loc[:,0].values.tolist()\n",
    "x_val = pd.read_csv(\"data/mex/mex20_val.txt\", sep='\\r\\n', engine='python', header=None).loc[:,0].values.tolist()\n",
    "print(\"N√∫mero de ejemplos de entrenamiento:\", len(x_train))\n",
    "print(x_train[:10])\n",
    "print(\"N√∫mero de ejemplos de validaci√≥n:\", len(x_val))\n",
    "print(x_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b718999",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833f5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_excluidas = set(['.', ',', ';', ':', '!', '?', '¬ø', '¬°', '\"', \"'\", '(', ')', '[', ']', '{', '}', '-', '_', '‚Äî', '...',\n",
    "                       '@', '#', '$', '%', '^', '&', '*', '/', '|', '~', '`', '<', '>', '¬´', '¬ª', '‚Äú', '‚Äù', '‚Äò', '‚Äô','<url>','<@usuario>',\n",
    "                       \n",
    "                       ])\n",
    "\n",
    "class NgramData:\n",
    "    def __init__(self, N: int, vocab_size: int, tokenizer = None, embeddinds_model = None):\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else self.default_tokenizer\n",
    "        self.punct = lista_excluidas\n",
    "        self.N = N\n",
    "        self.vocab_size = vocab_size\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.embeddinds_model = embeddinds_model # TODO: implementar\n",
    "    \n",
    "        \n",
    "    def default_tokenizer(self, text: str) -> list:\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isdigit()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "\n",
    "    def get_vocab(self, corpus: list) -> set:\n",
    "        freq_dist = FreqDist()\n",
    "        for sentence in corpus:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            tokens = [token.lower() for token in tokens if not self.remove_word(token)]\n",
    "            freq_dist.update(tokens)\n",
    "        most_common = freq_dist.most_common(self.vocab_size - 3)  # Tengo que reservar espacio para UNK, SOS, EOS\n",
    "        vocab = set([word for word, _ in most_common])\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "        \n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        \n",
    "        if self.embeddinds_model is not None:\n",
    "            self.embedding_matriz = np.empty([len(self.vocab), self.embeddinds_model.vector_size])\n",
    "            \n",
    "        id = 0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and  word_ not in self.word_to_id:\n",
    "                    self.word_to_id[word_] = id\n",
    "                    self.id_to_word[id] = word_\n",
    "                    \n",
    "                    if self.embeddinds_model is not None:\n",
    "                        if word_ in self.embeddinds_model:\n",
    "                            self.embedding_matriz[id] = self.embeddinds_model[word_]\n",
    "                        else:\n",
    "                            self.embedding_matriz[id] = np.random.normal(self.embeddinds_model.vector_size)\n",
    "                    id += 1\n",
    "\n",
    "        self.word_to_id.update({self.UNK: id, self.SOS: id + 1, self.EOS: id + 2})\n",
    "        self.id_to_word.update({id: self.UNK, id + 1: self.SOS, id + 2: self.EOS})\n",
    "    \n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        return doc_tokens\n",
    "    \n",
    "    \n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    \n",
    "    def transform(self, corpus: list) -> tuple[np.ndarray, np.ndarray]:\n",
    "        x_ngrams = []\n",
    "        y = []\n",
    "        \n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_in_window in doc_ngram:\n",
    "                words_in_window_ids = [self.word_to_id[w] for w in words_in_window]\n",
    "                x_ngrams.append(list(words_in_window_ids[:-1]))\n",
    "                y.append(words_in_window_ids[-1])\n",
    "        \n",
    "        return np.array(x_ngrams), np.array(y)\n",
    "    \n",
    "    \n",
    "    # =========== PROPOEDADES ===========    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "        \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cee7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "\n",
    "ngram_data = NgramData(args.N, 5_000, tokenizer=tk.tokenize)\n",
    "ngram_data.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15816d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del vocabulario: 5,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tama√±o del vocabulario: {ngram_data.size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703ead21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ngram_train, y_ngram_train = ngram_data.transform(x_train)\n",
    "x_ngram_val, y_ngram_val = ngram_data.transform(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f10d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4998, 4998, 4998],\n",
       "       [4998, 4998,    0],\n",
       "       [4998,    0,    0],\n",
       "       ...,\n",
       "       [4997,  937,   32],\n",
       "       [ 937,   32, 2524],\n",
       "       [  32, 2524, 4997]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39281614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ..., 2524, 4997, 4999])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea0e37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAMA√ëO DE LOS NGRAMS DE ENTRENAMIENTO\n",
      "x_ngram_train: (102751, 3)\n",
      "y_ngram_train: (102751,)\n",
      "TAMA√ëO DE LOS NGRAMS DE VALIDACI√ìN\n",
      "x_ngram_val: (11558, 3)\n",
      "y_ngram_val: (11558,)\n"
     ]
    }
   ],
   "source": [
    "# Tama√±os de los ngrams\n",
    "x_train_shape = x_ngram_train.shape\n",
    "y_train_shape = y_ngram_train.shape\n",
    "\n",
    "x_val_shape = x_ngram_val.shape\n",
    "y_val_shape = y_ngram_val.shape\n",
    "print(\"TAMA√ëO DE LOS NGRAMS DE ENTRENAMIENTO\")\n",
    "print(f\"x_ngram_train: {x_train_shape}\")\n",
    "print(f\"y_ngram_train: {y_train_shape}\")\n",
    "print(\"TAMA√ëO DE LOS NGRAMS DE VALIDACI√ìN\")\n",
    "print(f\"x_ngram_val: {x_val_shape}\")\n",
    "print(f\"y_ngram_val: {y_val_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef90c97",
   "metadata": {},
   "source": [
    "nota: creo que los tama√±os varian segun la lista de palabras excluidas que tengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8303ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['<s>', '<s>', '<s>']\n",
      "2: ['<s>', '<s>', '@usuario']\n",
      "3: ['<s>', '@usuario', '@usuario']\n",
      "4: ['@usuario', '@usuario', '@usuario']\n",
      "5: ['@usuario', '@usuario', 'q']\n",
      "6: ['@usuario', 'q', 'se']\n",
      "7: ['q', 'se', 'puede']\n",
      "8: ['se', 'puede', 'esperar']\n",
      "9: ['puede', 'esperar', 'del']\n",
      "10: ['esperar', 'del', 'maricon']\n",
      "11: ['del', 'maricon', 'de']\n",
      "12: ['maricon', 'de', 'closet']\n",
      "13: ['de', 'closet', 'de']\n",
      "14: ['closet', 'de', 'la']\n",
      "15: ['de', 'la', 'ya√±ez']\n",
      "16: ['la', 'ya√±ez', 'aun']\n",
      "17: ['ya√±ez', 'aun', 'recuerdo']\n",
      "18: ['aun', 'recuerdo', 'esa']\n",
      "19: ['recuerdo', 'esa', 'ves']\n",
      "20: ['esa', 'ves', 'q']\n",
      "21: ['ves', 'q', 'lo']\n",
      "22: ['q', 'lo', 'vi']\n"
     ]
    }
   ],
   "source": [
    "lista_palabras = [[ngram_data.id_to_word[w]  for w in tw] for tw in x_ngram_train[:22]]\n",
    "for i, palabras in enumerate(lista_palabras):\n",
    "    print(f\"{i+1}: {palabras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40142756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ..., 2524, 4997, 4999])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ngram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed657786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: @usuario\n",
      "2: @usuario\n",
      "3: @usuario\n",
      "4: q\n",
      "5: se\n",
      "6: puede\n",
      "7: esperar\n",
      "8: del\n",
      "9: maricon\n",
      "10: de\n",
      "11: closet\n",
      "12: de\n",
      "13: la\n",
      "14: ya√±ez\n",
      "15: aun\n",
      "16: recuerdo\n",
      "17: esa\n",
      "18: ves\n",
      "19: q\n",
      "20: lo\n",
      "21: vi\n",
      "22: en\n"
     ]
    }
   ],
   "source": [
    "lista_palbras_en_sus_ys = [ngram_data.id_to_word[w] for w in y_ngram_train[:22]]\n",
    "for i, palabra in enumerate(lista_palbras_en_sus_ys):\n",
    "    print(f\"{i+1}: {palabra}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82224a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 64\n",
    "args.num_workers = 2\n",
    "DTYPE = torch.int64\n",
    "\n",
    "\n",
    "def tensor_dataset(x: np.ndarray, y: np.ndarray) -> TensorDataset:\n",
    "    tensor_data = TensorDataset(\n",
    "        torch.tensor(x, dtype=DTYPE),\n",
    "        torch.tensor(y, dtype=DTYPE))\n",
    "    return tensor_data\n",
    "    \n",
    "    \n",
    "def data_loader(dataset: TensorDataset, shuffle: bool) -> DataLoader:\n",
    "    dataloader = DataLoader(dataset, \n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "    \n",
    "# Crear los DataLoaders\n",
    "train_dataset = tensor_dataset(x_ngram_train, y_ngram_train)\n",
    "\n",
    "train_loader = data_loader(train_dataset, shuffle=True)\n",
    "\n",
    "val_dataset = tensor_dataset(x_ngram_val, y_ngram_val)\n",
    "\n",
    "val_loader = data_loader(val_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa46d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([64, 3])\n",
      "Y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"X shape: {batch[0].shape}\")\n",
    "print(f\"Y shape: {batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b15c81fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4998,    0,  235],\n",
       "        [  32,   27, 4997],\n",
       "        [   7,  218,  161],\n",
       "        [ 109,    7, 1139],\n",
       "        [4998, 4998, 4998],\n",
       "        [  48,  342,   45],\n",
       "        [  43,    9, 1325],\n",
       "        [4997, 4997,    7],\n",
       "        [4997, 1799,  225],\n",
       "        [4998, 4997,   48],\n",
       "        [   7,    9,  565],\n",
       "        [4998,  700, 4997],\n",
       "        [4997, 4997, 4997],\n",
       "        [3538, 4997,  165],\n",
       "        [4998, 4998, 4998],\n",
       "        [  15, 4997, 4997],\n",
       "        [ 114,    2, 3261],\n",
       "        [ 455,   51,  456],\n",
       "        [  45, 4997,   83],\n",
       "        [  48,  375,   48],\n",
       "        [ 942,   43, 2500],\n",
       "        [  60,   45, 2302],\n",
       "        [ 253,   48,  255],\n",
       "        [  55,   48,   21],\n",
       "        [ 419, 2462,   33],\n",
       "        [4998, 4998, 4998],\n",
       "        [4997, 4997,   66],\n",
       "        [ 931, 4997, 4997],\n",
       "        [  48,  166,  128],\n",
       "        [1595,  337,   17],\n",
       "        [ 114,   48,   46],\n",
       "        [ 980, 4997,  696],\n",
       "        [4998, 4998, 4998],\n",
       "        [ 112,  272, 4997],\n",
       "        [1559,  192,  705],\n",
       "        [ 106, 4997,   93],\n",
       "        [ 266,   17,  919],\n",
       "        [4998, 1250,   45],\n",
       "        [4997,   60, 4142],\n",
       "        [1595,  318, 2392],\n",
       "        [4997,   39, 4274],\n",
       "        [1228,   83, 4997],\n",
       "        [ 160, 4997,   34],\n",
       "        [4998,   48,    9],\n",
       "        [ 253,   27, 4467],\n",
       "        [4998, 4998,  865],\n",
       "        [  32,  124,  766],\n",
       "        [4997,   55,   27],\n",
       "        [  39, 2318, 1880],\n",
       "        [ 635,    0,   32],\n",
       "        [  65, 2204,    7],\n",
       "        [4998, 4998,   39],\n",
       "        [3176, 4997, 4997],\n",
       "        [   7,    9, 1264],\n",
       "        [4998, 4998, 4998],\n",
       "        [ 318,  501, 4997],\n",
       "        [2136,  128,  161],\n",
       "        [1579,   60, 1235],\n",
       "        [4998,  252, 4997],\n",
       "        [   7,  117,   50],\n",
       "        [ 128, 4997,   32],\n",
       "        [ 615,  615,   81],\n",
       "        [4997,  244,   60],\n",
       "        [4998,  420,  197]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c30fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['<s>', '@usuario', 'luchona']\n",
      "2: ['y', 'un', '<unk>']\n",
      "3: ['de', 'perra', 'como']\n",
      "4: ['lugar', 'de', 'unas']\n",
      "5: ['<s>', '<s>', '<s>']\n",
      "6: ['a', 'mi', 'me']\n",
      "7: ['por', 'la', 'noche']\n",
      "8: ['<unk>', '<unk>', 'de']\n",
      "9: ['<unk>', 'instagram', 'para']\n",
      "10: ['<s>', '<unk>', 'a']\n",
      "11: ['de', 'la', 'palabra']\n",
      "12: ['<s>', 'ah', '<unk>']\n",
      "13: ['<unk>', '<unk>', '<unk>']\n",
      "14: ['favoritas', '<unk>', 'pero']\n",
      "15: ['<s>', '<s>', '<s>']\n",
      "16: ['lo', '<unk>', '<unk>']\n",
      "17: ['solo', 'se', 'escucha']\n",
      "18: ['dise', 'gorda', 'nena']\n",
      "19: ['me', '<unk>', 'el']\n",
      "20: ['a', 'comprar', 'a']\n",
      "21: ['triste', 'por', 'joe']\n",
      "22: ['que', 'me', 'anda']\n",
      "23: ['tiene', 'a', 'gusto']\n",
      "24: ['es', 'a', 'quien']\n",
      "25: ['hay', 'momento', 'm√°s']\n",
      "26: ['<s>', '<s>', '<s>']\n",
      "27: ['<unk>', '<unk>', 'muy']\n",
      "28: ['jugar', '<unk>', '<unk>']\n",
      "29: ['a', 'tu', 'madre']\n",
      "30: ['a√∫n', 'sigo', 'en']\n",
      "31: ['solo', 'a', 'las']\n",
      "32: ['soluci√≥n', '<unk>', 'luego']\n",
      "33: ['<s>', '<s>', '<s>']\n",
      "34: ['cada', 'foto', '<unk>']\n",
      "35: ['mamadas', '..', 'eso']\n",
      "36: ['te', '<unk>', 'si']\n",
      "37: ['hoy', 'en', 'd√≠a']\n",
      "38: ['<s>', 'uuuugh', 'me']\n",
      "39: ['<unk>', 'que', 'regresen']\n",
      "40: ['a√∫n', 'as√≠', 'encuentro']\n",
      "41: ['<unk>', 'no', 'fumo']\n",
      "42: ['llegar', 'el', '<unk>']\n",
      "43: ['putas', '<unk>', 'con']\n",
      "44: ['<s>', 'a', 'la']\n",
      "45: ['tiene', 'un', 'genio']\n",
      "46: ['<s>', '<s>', 'ver']\n",
      "47: ['y', 'est√°', 'bien']\n",
      "48: ['<unk>', 'es', 'un']\n",
      "49: ['no', 't', 'sacan']\n",
      "50: ['üòÇ', '@usuario', 'y']\n",
      "51: ['estoy', 'loco', 'de']\n",
      "52: ['<s>', '<s>', 'no']\n",
      "53: ['largo', '<unk>', '<unk>']\n",
      "54: ['de', 'la', 'selecci√≥n']\n",
      "55: ['<s>', '<s>', '<s>']\n",
      "56: ['as√≠', 'dice', '<unk>']\n",
      "57: ['valgo', 'madre', 'como']\n",
      "58: ['quieren', 'que', 'uno']\n",
      "59: ['<s>', 'ni', '<unk>']\n",
      "60: ['de', 'su', 'puta']\n",
      "61: ['madre', '<unk>', 'y']\n",
      "62: ['m√©xico', 'm√©xico', 'putos']\n",
      "63: ['<unk>', 'ni√±o', 'que']\n",
      "64: ['<s>', 'cosas', 'feas']\n"
     ]
    }
   ],
   "source": [
    "lista_palabras = [[ngram_data.id_to_word[w]  for w in tw] for tw in batch[0].tolist()]\n",
    "for i, palabras in enumerate(lista_palabras):\n",
    "    print(f\"{i+1}: {palabras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8dae3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  32, 4997,   15, 4997,    7, 3093, 1304,  575,  160, 2407, 4999,  165,\n",
       "        3840,  955,   35, 4999,   60,  242, 1207,    9, 3119, 3050, 4997,   33,\n",
       "        4997,    0, 4045,   45,  166, 4997,   33,   57,   83,   55,   39,  106,\n",
       "         101,   65, 4997,  109, 4997,  100,   46,  111,    7,   48, 4997, 1916,\n",
       "          34,    0, 4997,  419,   48, 4997,    0, 4997,   93,   70, 4997,  128,\n",
       "         318,  385,  923,   32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734c8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: y\n",
      "2: <unk>\n",
      "3: lo\n",
      "4: <unk>\n",
      "5: de\n",
      "6: estuviera\n",
      "7: dejan\n",
      "8: lado\n",
      "9: putas\n",
      "10: pura\n",
      "11: </s>\n",
      "12: pero\n",
      "13: escribe\n",
      "14: antes\n",
      "15: este\n",
      "16: </s>\n",
      "17: que\n",
      "18: eres\n",
      "19: saludo\n",
      "20: la\n",
      "21: jonas\n",
      "22: jodiendo\n",
      "23: <unk>\n",
      "24: m√°s\n",
      "25: <unk>\n",
      "26: @usuario\n",
      "27: caro\n",
      "28: me\n",
      "29: tu\n",
      "30: <unk>\n",
      "31: m√°s\n",
      "32: porque\n",
      "33: el\n",
      "34: es\n",
      "35: no\n",
      "36: te\n",
      "37: son\n",
      "38: estoy\n",
      "39: <unk>\n",
      "40: lugar\n",
      "41: <unk>\n",
      "42: cuando\n",
      "43: las\n",
      "44: verga\n",
      "45: de\n",
      "46: a\n",
      "47: <unk>\n",
      "48: don\n",
      "49: con\n",
      "50: @usuario\n",
      "51: <unk>\n",
      "52: hay\n",
      "53: a\n",
      "54: <unk>\n",
      "55: @usuario\n",
      "56: <unk>\n",
      "57: si\n",
      "58: e\n",
      "59: <unk>\n",
      "60: madre\n",
      "61: as√≠\n",
      "62: ‚ô•\n",
      "63: nadie\n",
      "64: y\n"
     ]
    }
   ],
   "source": [
    "lista_palbras_en_sus_ys = [ngram_data.id_to_word[w] for w in batch[1].tolist()]\n",
    "for i, palabra in enumerate(lista_palbras_en_sus_ys):\n",
    "    print(f\"{i+1}: {palabra}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce09ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tama√±o del vocabulario\n",
    "args.vocab_size = ngram_data.size\n",
    "\n",
    "# Dimensionalidad del word embedding\n",
    "args.d = 50\n",
    "\n",
    "# Dimension por capa oculta\n",
    "args.d_h = 100\n",
    "\n",
    "# Dropout\n",
    "args.dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f312af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self,args , embeddings = None):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.window_size = args.N - 1\n",
    "        self.embedding_dim = args.d\n",
    "        self.emb = embeddings if embeddings is not None else nn.Embedding(args.vocab_size, args.d)\n",
    "        # Capas fully connected 1\n",
    "        self.fc1 = nn.Linear(args.d * (args.N - 1), args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        # Capa fully connected 2 que va a predecir la siguiente palabra\n",
    "        self.fc2 = nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        h = F.relu(self.fc1(x)) # relu(z) = max(0, z)\n",
    "        h = self.drop1(h)\n",
    "        out = self.fc2(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e70003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6397f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data,model, gpu = False):\n",
    "    with torch.no_grad():\n",
    "        preds, tgts = [], []\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model(window_words)\n",
    "            # Obtener predicci√≥n\n",
    "            y_pred = get_preds(outputs)\n",
    "            \n",
    "            tgt = labels.cpu().numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "            \n",
    "    tgts = [e for seq in tgts for e in seq ]\n",
    "    preds = [e for seq in preds for e in seq ]\n",
    "    acurracy = accuracy_score(tgts, preds)\n",
    "    return acurracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9db7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, check_point_path, filename='checkpoint.pt'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(check_point_path, 'model_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2c93a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "args.vocab_size = ngram_data.size\n",
    "args.d = 100 # Dimensionalidad del word embedding\n",
    "args.d_h = 200 # Dimension por capa oculta\n",
    "args.dropout = 0.1 \n",
    "\n",
    "# Training hyperparameters\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "# Scheduler hyperparameters\n",
    "args.lr_patience = 10 # N√∫mero de √©pocas sin mejora para reducir lr\n",
    "args.lr_factor = 0.5  # Factor de reducci√≥n del lr\n",
    "\n",
    "# Saving directory\n",
    "args.savedir = 'model'\n",
    "os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "# Create model\n",
    "model = NeuralLM(args)\n",
    "\n",
    "# Send to GPU if available\n",
    "args.gpu = torch.cuda.is_available()\n",
    "if args.gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "# Loss, Optimizer and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=args.lr_patience,\n",
    "    #verbose=True,\n",
    "    factor=args.lr_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e15c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing accuracy: 0.41084058520467603\n",
      "√âpoca [1/100] Tiempo: 6.32s P√©rdida: 2.4705 Precisi√≥n Entrenamiento: 0.4108 Precisi√≥n Validaci√≥n: 0.2035 MEJORA\n",
      "Traing accuracy: 0.40885458512433215\n",
      "√âpoca [2/100] Tiempo: 5.23s P√©rdida: 2.4723 Precisi√≥n Entrenamiento: 0.4089 Precisi√≥n Validaci√≥n: 0.2055 MEJORA\n",
      "Traing accuracy: 0.4123517404491222\n",
      "√âpoca [3/100] Tiempo: 5.82s P√©rdida: 2.4645 Precisi√≥n Entrenamiento: 0.4124 Precisi√≥n Validaci√≥n: 0.2060 MEJORA\n",
      "Traing accuracy: 0.4112887533643997\n",
      "√âpoca [4/100] Tiempo: 5.21s P√©rdida: 2.4626 Precisi√≥n Entrenamiento: 0.4113 Precisi√≥n Validaci√≥n: 0.2074 MEJORA\n",
      "Traing accuracy: 0.41057915377817056\n",
      "√âpoca [5/100] Tiempo: 5.03s P√©rdida: 2.4656 Precisi√≥n Entrenamiento: 0.4106 Precisi√≥n Validaci√≥n: 0.2064 \n",
      "Traing accuracy: 0.40935861738239665\n",
      "√âpoca [6/100] Tiempo: 5.13s P√©rdida: 2.4654 Precisi√≥n Entrenamiento: 0.4094 Precisi√≥n Validaci√≥n: 0.2031 \n",
      "Traing accuracy: 0.4141899203591371\n",
      "√âpoca [7/100] Tiempo: 5.41s P√©rdida: 2.4389 Precisi√≥n Entrenamiento: 0.4142 Precisi√≥n Validaci√≥n: 0.2093 MEJORA\n",
      "Traing accuracy: 0.4160858474269875\n",
      "√âpoca [8/100] Tiempo: 5.06s P√©rdida: 2.4395 Precisi√≥n Entrenamiento: 0.4161 Precisi√≥n Validaci√≥n: 0.2142 MEJORA\n",
      "Traing accuracy: 0.4171438130197244\n",
      "√âpoca [9/100] Tiempo: 5.23s P√©rdida: 2.4362 Precisi√≥n Entrenamiento: 0.4171 Precisi√≥n Validaci√≥n: 0.2117 \n",
      "Traing accuracy: 0.4157732595508778\n",
      "√âpoca [10/100] Tiempo: 5.18s P√©rdida: 2.4373 Precisi√≥n Entrenamiento: 0.4158 Precisi√≥n Validaci√≥n: 0.2126 \n",
      "Traing accuracy: 0.41679230858072547\n",
      "√âpoca [11/100] Tiempo: 5.11s P√©rdida: 2.4365 Precisi√≥n Entrenamiento: 0.4168 Precisi√≥n Validaci√≥n: 0.2059 \n",
      "Traing accuracy: 0.4193749246776202\n",
      "√âpoca [12/100] Tiempo: 5.24s P√©rdida: 2.4303 Precisi√≥n Entrenamiento: 0.4194 Precisi√≥n Validaci√≥n: 0.2077 \n",
      "Traing accuracy: 0.4163485342264894\n",
      "√âpoca [13/100] Tiempo: 5.12s P√©rdida: 2.4325 Precisi√≥n Entrenamiento: 0.4163 Precisi√≥n Validaci√≥n: 0.2078 \n",
      "Traing accuracy: 0.41808942272928135\n",
      "√âpoca [14/100] Tiempo: 5.15s P√©rdida: 2.4323 Precisi√≥n Entrenamiento: 0.4181 Precisi√≥n Validaci√≥n: 0.2121 \n",
      "Traing accuracy: 0.4177027678463825\n",
      "√âpoca [15/100] Tiempo: 5.12s P√©rdida: 2.4295 Precisi√≥n Entrenamiento: 0.4177 Precisi√≥n Validaci√≥n: 0.2020 \n",
      "Traing accuracy: 0.4188250713051862\n",
      "√âpoca [16/100] Tiempo: 5.73s P√©rdida: 2.4322 Precisi√≥n Entrenamiento: 0.4188 Precisi√≥n Validaci√≥n: 0.2053 \n",
      "Traing accuracy: 0.4175737782709999\n",
      "√âpoca [17/100] Tiempo: 5.15s P√©rdida: 2.4303 Precisi√≥n Entrenamiento: 0.4176 Precisi√≥n Validaci√≥n: 0.2068 \n",
      "Traing accuracy: 0.4203622755393082\n",
      "√âpoca [18/100] Tiempo: 5.10s P√©rdida: 2.4162 Precisi√≥n Entrenamiento: 0.4204 Precisi√≥n Validaci√≥n: 0.2064 \n",
      "Traing accuracy: 0.4197590688145262\n",
      "√âpoca [19/100] Tiempo: 5.13s P√©rdida: 2.4178 Precisi√≥n Entrenamiento: 0.4198 Precisi√≥n Validaci√≥n: 0.2096 \n",
      "Traing accuracy: 0.42197009450447914\n",
      "√âpoca [20/100] Tiempo: 5.13s P√©rdida: 2.4118 Precisi√≥n Entrenamiento: 0.4220 Precisi√≥n Validaci√≥n: 0.2053 \n",
      "Traing accuracy: 0.42157119973486523\n",
      "√âpoca [21/100] Tiempo: 5.11s P√©rdida: 2.4105 Precisi√≥n Entrenamiento: 0.4216 Precisi√≥n Validaci√≥n: 0.2054 \n",
      "Traing accuracy: 0.4216204731249749\n",
      "√âpoca [22/100] Tiempo: 5.16s P√©rdida: 2.4110 Precisi√≥n Entrenamiento: 0.4216 Precisi√≥n Validaci√≥n: 0.2070 \n",
      "Traing accuracy: 0.42180783754469126\n",
      "√âpoca [23/100] Tiempo: 5.12s P√©rdida: 2.4112 Precisi√≥n Entrenamiento: 0.4218 Precisi√≥n Validaci√≥n: 0.2083 \n",
      "Traing accuracy: 0.4204517208653035\n",
      "√âpoca [24/100] Tiempo: 5.24s P√©rdida: 2.4150 Precisi√≥n Entrenamiento: 0.4205 Precisi√≥n Validaci√≥n: 0.2096 \n",
      "Traing accuracy: 0.42192207648736596\n",
      "√âpoca [25/100] Tiempo: 5.15s P√©rdida: 2.4100 Precisi√≥n Entrenamiento: 0.4219 Precisi√≥n Validaci√≥n: 0.2102 \n",
      "Traing accuracy: 0.42075771803318196\n",
      "√âpoca [26/100] Tiempo: 5.09s P√©rdida: 2.4128 Precisi√≥n Entrenamiento: 0.4208 Precisi√≥n Validaci√≥n: 0.2144 MEJORA\n",
      "Traing accuracy: 0.4218112898204315\n",
      "√âpoca [27/100] Tiempo: 5.11s P√©rdida: 2.4102 Precisi√≥n Entrenamiento: 0.4218 Precisi√≥n Validaci√≥n: 0.2089 \n",
      "Traing accuracy: 0.4214064320290845\n",
      "√âpoca [28/100] Tiempo: 5.25s P√©rdida: 2.4078 Precisi√≥n Entrenamiento: 0.4214 Precisi√≥n Validaci√≥n: 0.2072 \n",
      "Traing accuracy: 0.42350541567910654\n",
      "√âpoca [29/100] Tiempo: 5.10s P√©rdida: 2.4023 Precisi√≥n Entrenamiento: 0.4235 Precisi√≥n Validaci√≥n: 0.2074 \n",
      "Traing accuracy: 0.42117889567348255\n",
      "√âpoca [30/100] Tiempo: 5.19s P√©rdida: 2.4055 Precisi√≥n Entrenamiento: 0.4212 Precisi√≥n Validaci√≥n: 0.2098 \n",
      "Traing accuracy: 0.424207169184108\n",
      "√âpoca [31/100] Tiempo: 5.18s P√©rdida: 2.4012 Precisi√≥n Entrenamiento: 0.4242 Precisi√≥n Validaci√≥n: 0.2073 \n",
      "Traing accuracy: 0.42472595207488045\n",
      "√âpoca [32/100] Tiempo: 6.43s P√©rdida: 2.4014 Precisi√≥n Entrenamiento: 0.4247 Precisi√≥n Validaci√≥n: 0.2077 \n",
      "Traing accuracy: 0.42431670047804604\n",
      "√âpoca [33/100] Tiempo: 5.28s P√©rdida: 2.3988 Precisi√≥n Entrenamiento: 0.4243 Precisi√≥n Validaci√≥n: 0.2091 \n",
      "Traing accuracy: 0.4239677067850399\n",
      "√âpoca [34/100] Tiempo: 5.28s P√©rdida: 2.4031 Precisi√≥n Entrenamiento: 0.4240 Precisi√≥n Validaci√≥n: 0.2088 \n",
      "Traing accuracy: 0.4245197570602178\n",
      "√âpoca [35/100] Tiempo: 5.49s P√©rdida: 2.3992 Precisi√≥n Entrenamiento: 0.4245 Precisi√≥n Validaci√≥n: 0.2064 \n",
      "Traing accuracy: 0.4239852820069899\n",
      "√âpoca [36/100] Tiempo: 5.21s P√©rdida: 2.4050 Precisi√≥n Entrenamiento: 0.4240 Precisi√≥n Validaci√≥n: 0.2075 \n",
      "Traing accuracy: 0.4237790869923272\n",
      "√âpoca [37/100] Tiempo: 5.07s P√©rdida: 2.4005 Precisi√≥n Entrenamiento: 0.4238 Precisi√≥n Validaci√≥n: 0.2060 \n",
      "Traing accuracy: 0.4236240484272687\n",
      "√âpoca [38/100] Tiempo: 5.06s P√©rdida: 2.4006 Precisi√≥n Entrenamiento: 0.4236 Precisi√≥n Validaci√≥n: 0.2072 \n",
      "Traing accuracy: 0.42387888914554295\n",
      "√âpoca [39/100] Tiempo: 5.06s P√©rdida: 2.3988 Precisi√≥n Entrenamiento: 0.4239 Precisi√≥n Validaci√≥n: 0.2061 \n",
      "Traing accuracy: 0.4232646979070422\n",
      "√âpoca [40/100] Tiempo: 5.08s P√©rdida: 2.3982 Precisi√≥n Entrenamiento: 0.4233 Precisi√≥n Validaci√≥n: 0.2078 \n",
      "Traing accuracy: 0.4232373935443699\n",
      "√âpoca [41/100] Tiempo: 5.12s P√©rdida: 2.3972 Precisi√≥n Entrenamiento: 0.4232 Precisi√≥n Validaci√≥n: 0.2080 \n",
      "Traing accuracy: 0.42351828325231994\n",
      "√âpoca [42/100] Tiempo: 5.42s P√©rdida: 2.3985 Precisi√≥n Entrenamiento: 0.4235 Precisi√≥n Validaci√≥n: 0.2073 \n",
      "Traing accuracy: 0.42330361446993137\n",
      "√âpoca [43/100] Tiempo: 5.56s P√©rdida: 2.3989 Precisi√≥n Entrenamiento: 0.4233 Precisi√≥n Validaci√≥n: 0.2064 \n",
      "Traing accuracy: 0.42591102418350546\n",
      "√âpoca [44/100] Tiempo: 5.45s P√©rdida: 2.3971 Precisi√≥n Entrenamiento: 0.4259 Precisi√≥n Validaci√≥n: 0.2080 \n",
      "Traing accuracy: 0.4236058455188206\n",
      "√âpoca [45/100] Tiempo: 5.80s P√©rdida: 2.3961 Precisi√≥n Entrenamiento: 0.4236 Precisi√≥n Validaci√≥n: 0.2092 \n",
      "No hubo mejora en las √∫ltimas 20 √©pocas. Terminando entrenamiento.\n",
      "Tiempo total de entrenamiento: 243.11s\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.metrics._classification\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "base_metric = 0\n",
    "n_no_improve = 0\n",
    "metric_history = []\n",
    "tran_metric_history = []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.perf_counter()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    # ======== Training ========\n",
    "    for window_word, labels in train_loader:\n",
    "        if args.gpu:\n",
    "            window_word = window_word.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(window_word)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        # Get training metric\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    tran_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    \n",
    "    # Get metric in validation dataset\n",
    "    model.eval()\n",
    "    tunning_metric = model_eval(val_loader, model, gpu=args.gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(tunning_metric)\n",
    "    \n",
    "    # Check for improvement\n",
    "    is_improvement = tunning_metric > base_metric\n",
    "    if is_improvement:\n",
    "        base_metric = tunning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "        \n",
    "    # Save checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'base_metric': base_metric,\n",
    "    }, is_improvement, \n",
    "        args.savedir,)\n",
    "    \n",
    "    # Early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(f\"No hubo mejora en las √∫ltimas {args.patience} √©pocas. Terminando entrenamiento.\")\n",
    "        break\n",
    "    print(f'Traing accuracy: {mean_epoch_metric}')\n",
    "    print(f\"√âpoca [{epoch+1}/{args.num_epochs}] \"\n",
    "          f\"Tiempo: {time.perf_counter() - epoch_start_time:.2f}s \"\n",
    "          f\"P√©rdida: {np.mean(loss_epoch):.4f} \"\n",
    "          f\"Precisi√≥n Entrenamiento: {mean_epoch_metric:.4f} \"\n",
    "          f\"Precisi√≥n Validaci√≥n: {tunning_metric:.4f} \"\n",
    "          f\"{'MEJORA' if is_improvement else ''}\")\n",
    "print(f\"Tiempo total de entrenamiento: {time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c498f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings, ngram_data, word, k=5):\n",
    "    if word not in ngram_data.vocab:\n",
    "        print(f\"La palabra '{word}' no est√° en el vocabulario.\")\n",
    "        return\n",
    "    \n",
    "    word_id = torch.tensor([ngram_data.word_to_id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dist = torch.norm(embeddings.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dist.numpy()), key=lambda x: x[1]) # Ordenar por la menor distancia\n",
    "    for idx, difference in lst[1:k+1]:  # Saltar el primero porque es la misma palabra\n",
    "        print(f\"Palabra: {ngram_data.id_to_word[idx]}, Distancia: {difference:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "729a62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learning word embeddings\n",
      "------------------------------\n",
      "Palabra: <unk>, Distancia: 10.9262\n",
      "Palabra: examen, Distancia: 11.0856\n",
      "Palabra: pens√©, Distancia: 11.0972\n",
      "Palabra: ojotes, Distancia: 11.1100\n",
      "Palabra: el, Distancia: 11.1328\n",
      "Palabra: mordidas, Distancia: 11.1361\n",
      "Palabra: l, Distancia: 11.2239\n",
      "Palabra: #aborto, Distancia: 11.2842\n",
      "Palabra: reglas, Distancia: 11.3070\n",
      "Palabra: üë¨, Distancia: 11.3209\n"
     ]
    }
   ],
   "source": [
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load('model/model_best.pt')[\"state_dict\"])\n",
    "best_model.train(False)\n",
    "\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learning word embeddings\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "print_closest_words(best_model.emb, ngram_data, word=\"jaja\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5673bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizador):\n",
    "    all_tokens = [w.lower() if w in ngram_data.word_to_id else '<unk>' for w in tokenizador(text)]\n",
    "    tokens_ids = [ngram_data.word_to_id[w] for w in all_tokens]\n",
    "    return all_tokens, tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "994c113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(raw_logits, temperature=1.0):\n",
    "    logits = np.asanyarray(raw_logits).astype(np.float64)\n",
    "    \n",
    "    preds = logits / temperature\n",
    "    expo_preds = np.exp(preds)\n",
    "    preds = expo_preds / np.sum(expo_preds)\n",
    "    \n",
    "    proabs = np.random.multinomial(1, preds)\n",
    "    return np.argmax(proabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4cc2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next_token(model, token_ids):\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "\n",
    "    \n",
    "    y_pred = sample_next_word(y_raw_pred, temperature=1.0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5746b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_texto(model, initial_text, tokenizador):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizador)\n",
    "    for i in range(100):\n",
    "        y_pred = pred_next_token(best_model, window_word_ids)\n",
    "        next_word = ngram_data.id_to_word[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        if next_word == ngram_data.EOS:\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "            \n",
    "    return ' '.join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02e0951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learning word embeddings\n",
      "------------------------------\n",
      "<s> <s> fernando sus putas <unk> para que vean que ya te gano a <unk> o estar mamando <unk> gracias <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> <s> fernando \"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learning word embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generar_texto(best_model, initial_tokens, tk.tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10ff4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    x,y = ngram_model.transform([text])\n",
    "    x,y = x[2:], y[2:]\n",
    "    x = torch.LongTensor(x).unsqueeze(0)\n",
    "    logits = model(x).detach()\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    y = np.sum([np.log(probs[i][word]) for i, word in enumerate(y)])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17f4b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.62249\n",
      "-18.758595\n"
     ]
    }
   ],
   "source": [
    "print(log_likelihood(best_model, \"hola como estas\", ngram_data))\n",
    "print(log_likelihood(best_model, \"estas hola como \", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7c8b5",
   "metadata": {},
   "source": [
    "# ESTRUCTURAS SINTACTICAS CORRECTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3e19a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "MEJORES PERMUTACIONES SEG√öN LOG-LIKELIHOOD\n",
      "------------------------------\n",
      "Log-Likelihood: -23.7495 | Texto: si gano no me voy a la chingada\n",
      "Log-Likelihood: -24.1518 | Texto: no gano si me voy a la chingada\n",
      "Log-Likelihood: -25.5432 | Texto: gano no me voy a la chingada si\n",
      "Log-Likelihood: -25.8474 | Texto: no gano me voy a la chingada si\n",
      "Log-Likelihood: -27.2372 | Texto: gano si no me voy a la chingada\n",
      "------------------------------\n",
      "PEORES PERMUTACIONES SEG√öN LOG-LIKELIHOOD\n",
      "------------------------------\n",
      "Log-Likelihood: -109.0404 | Texto: si no gano voy chingada la me a\n",
      "Log-Likelihood: -109.8371 | Texto: no la voy chingada gano me a si\n",
      "Log-Likelihood: -110.4188 | Texto: a no gano voy chingada la me si\n",
      "Log-Likelihood: -111.0062 | Texto: la no a voy chingada gano me si\n",
      "Log-Likelihood: -112.5344 | Texto: a no la voy chingada gano me si\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = \"si no gano me voy a la chingada\".split(' ')\n",
    "permutaciones = [\" \".join(p) for p in permutations(word_list)]\n",
    "\n",
    "permutaciones_ordenadas = sorted([(log_likelihood(best_model, text, ngram_data), text) for text in permutaciones], reverse=True)[:5]\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"MEJORES PERMUTACIONES SEG√öN LOG-LIKELIHOOD\")\n",
    "print(\"-\"*30)\n",
    "for p, t in permutaciones_ordenadas:\n",
    "    print(f\"Log-Likelihood: {p:.4f} | Texto: {t}\")\n",
    "    \n",
    "peores_permutaciones_ordenadas = sorted([(log_likelihood(best_model, text, ngram_data), text) for text in permutaciones], reverse=True)[-5:]\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"PEORES PERMUTACIONES SEG√öN LOG-LIKELIHOOD\")\n",
    "print(\"-\"*30)\n",
    "for p, t in peores_permutaciones_ordenadas:\n",
    "    print(f\"Log-Likelihood: {p:.4f} | Texto: {t}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
