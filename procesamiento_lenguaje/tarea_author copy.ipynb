{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da3245f",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae49b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from colorstreak import Logger\n",
    "import re\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b42045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
    "    porcentaje = (i + 1) / cantidad_registros * 100\n",
    "    # Con emojis\n",
    "    barra = int(50 * (i + 1) / cantidad_registros) * \"ğŸŸ©\"\n",
    "    espacio = int(50 - len(barra)) * \"â¬›ï¸\"\n",
    "\n",
    "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a55da",
   "metadata": {},
   "source": [
    "# MEJORAR LA CARGA DESDE UN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a4e63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test/truth_test.csv\"\u001b[0m\n",
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train/truth_train.csv\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generar_csv(path_lectura, path_guardado):\n",
    "    # Obtener la ruta absoluta del directorio actual\n",
    "    # Leerlo y convertirlo en un DataFrame .csv para manipularlo mejor\n",
    "    with open(path_lectura, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    data = [line.strip().split(\":::\") for line in lines]\n",
    "\n",
    "\n",
    "    df = pl.DataFrame(data, schema=[\"id\", \"genero\", \"pais\"], orient=\"row\")\n",
    "\n",
    "\n",
    "    # Guardar el DataFrame como un archivo .csv\n",
    "    df.write_csv(path_guardado)\n",
    "    return True\n",
    "\n",
    "# ==================== ConfiguraciÃ³n de rutas =====================\n",
    "\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "# ===================== CreaciÃ³n de CSV de datos de prueba =====================\n",
    "ruta_lectura_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth.txt\")\n",
    "ruta_guardado_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth_test.csv\")\n",
    "\n",
    "creado = generar_csv(ruta_lectura_prueba, ruta_guardado_prueba)\n",
    "\n",
    "if creado:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_prueba}\"')\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# ===================== CreaciÃ³n de CSV de datos de entrenamiento ==============\n",
    "ruta_lectura_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth.txt\")\n",
    "ruta_guardado_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth_train.csv\")\n",
    "\n",
    "creado_entrenamiento = generar_csv(ruta_lectura_entrenamiento, ruta_guardado_entrenamiento)\n",
    "if creado_entrenamiento:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_entrenamiento}\"')\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e521660",
   "metadata": {},
   "source": [
    "# CARGAR LOS INDICES DESDE TRUTH DESDE EL CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8e4c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id                              â”† genero â”† pais     â”‚\n",
      "â”‚ ---                             â”† ---    â”† ---      â”‚\n",
      "â”‚ str                             â”† str    â”† str      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† female â”† colombia â”‚\n",
      "â”‚ 4639c055f34ca1f944d0137a5aeb79â€¦ â”† female â”† colombia â”‚\n",
      "â”‚ 92ffa98bade702b86417b118e8aca3â€¦ â”† female â”† colombia â”‚\n",
      "â”‚ 4560c6567afcccef265f048ed117d0â€¦ â”† female â”† colombia â”‚\n",
      "â”‚ 393866dfaa80d414c9896cf8723932â€¦ â”† female â”† colombia â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "CARGA XML: |ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©| 100.00%\n",
      "\u001b[94m[INFO] Carga de archivos XML completada.\n",
      "\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets crudos cargados: 4200 mostrando 1 registro:\n",
      " id:74bcc9b0882c8440716ff370494aea09\n",
      " pais:colombia\n",
      " genero:female\n",
      " xml_text:\n",
      "<author lang=\"es\">\n",
      "\t<documents>\n",
      "\t\t<document><![CDA...\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def cargar_xml(id_archivo, train=True) -> str:\n",
    "    ruta_base = os.path.join(base_path, \"data/author_profiling/es_test\" if not train else \"data/author_profiling/es_train\")\n",
    "    ruta_archivo = os.path.join(ruta_base, f\"{id_archivo}.xml\")\n",
    "    # Logger.info(f\"ruta archivo: {ruta_archivo}\")\n",
    "    \n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as file:\n",
    "        xml_text = file.read()\n",
    "\n",
    "    return xml_text\n",
    "\n",
    "# ===================== Cargar CSV de datos  ==================\n",
    " \n",
    "df_indices = pl.read_csv(ruta_guardado_entrenamiento)\n",
    "print(df_indices.head())\n",
    "\n",
    "cantidad_registros = len(df_indices)\n",
    "\n",
    "registros_crudos = [(\"id_user\",\"xml_doc\",\"pais\",\"genero\") for i in range(cantidad_registros)]\n",
    "\n",
    "\n",
    "for i, reg in enumerate(registros_crudos):\n",
    "    id_archivo = df_indices['id'][i]\n",
    "    id_user = id_archivo\n",
    "    pais = df_indices['pais'][i]\n",
    "    genero = df_indices['genero'][i]\n",
    "\n",
    "    xml_text: str = cargar_xml(id_archivo)\n",
    "    registros_crudos[i] = (id_user, xml_text, pais, genero)\n",
    "\n",
    "    print_bar(i, cantidad_registros, contexto=\"CARGA XML\")\n",
    "\n",
    "print()\n",
    "Logger.info(\"Carga de archivos XML completada.\\n\")\n",
    "id_user, xml_text, pais, genero = registros_crudos[0]\n",
    "\n",
    "\n",
    "Logger.debug(f\"Total de tweets crudos cargados: {len(registros_crudos)} mostrando 1 registro:\\n id:{id_user}\\n pais:{pais}\\n genero:{genero}\\n xml_text:\\n{xml_text[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4685f1",
   "metadata": {},
   "source": [
    "# PROCESAMOS LOS TEXTOS XML Y LOS DEJAMOS EN UN DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0287180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso registros limpiados: |ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©| 100.00%\n",
      "\u001b[94m[INFO] Guardando archivo\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets procesados: 419998 mostrando 5 primeros registros:\n",
      " shape: (5, 5)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id_user                         â”† tweet_crudo                     â”† pais     â”† genero â”† idioma  â”‚\n",
      "â”‚ ---                             â”† ---                             â”† ---      â”† ---    â”† ---     â”‚\n",
      "â”‚ str                             â”† str                             â”† str      â”† str    â”† str     â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† Tiene que valer la pena que esâ€¦ â”† colombia â”† female â”† EspaÃ±ol â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† Tintas chinas, si ven Ã©sto, esâ€¦ â”† colombia â”† female â”† EspaÃ±ol â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† \"Maestro no le abriÃ³!\" -Ay quÃ©â€¦ â”† colombia â”† female â”† EspaÃ±ol â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† Lo bueno de no estar enamoradoâ€¦ â”† colombia â”† female â”† EspaÃ±ol â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† RecordarÃ© este dÃ­a como el dÃ­aâ€¦ â”† colombia â”† female â”† EspaÃ±ol â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "diccionario_idioma = {\n",
    "    'es': 'EspaÃ±ol', \n",
    "    'en': 'InglÃ©s', \n",
    "    'fr': 'FrancÃ©s', \n",
    "    'de': 'AlemÃ¡n', \n",
    "    'it': 'Italiano', \n",
    "    'nl': 'NeerlandÃ©s'\n",
    "}\n",
    "\n",
    "def limpiar_xml(texto_xml:str):\n",
    "    soup = BeautifulSoup(texto_xml, 'lxml-xml')   \n",
    "    lang = str(soup.author.get('lang'))\n",
    "    idioma = diccionario_idioma[lang] if lang in diccionario_idioma else lang\n",
    "    \n",
    "    documentos = soup.find_all('document') # Obtenemos todsa las etiquetas <document>\n",
    "    tweets = [doc.get_text(separator=\" \", strip=True) for doc in documentos] # Extraemos el texto de cada documento\n",
    "    \n",
    "    return tweets, idioma\n",
    "\n",
    "\n",
    "\n",
    "# ===================== Procesamiento de limpieza del XML =====================\n",
    "cantidad_registros = len(registros_crudos)\n",
    "\n",
    "registros_procesados = []\n",
    "\n",
    "for i, (id_user, doc_crudo, pais, genero) in enumerate(registros_crudos):\n",
    "    lista_tweets_por_usuario, idioma = limpiar_xml(doc_crudo)\n",
    "    \n",
    "    for tweet in lista_tweets_por_usuario:\n",
    "        registros_procesados.append({\n",
    "            \"id_user\": id_user,\n",
    "            \"tweet_crudo\": tweet,\n",
    "            \"pais\": pais,\n",
    "            \"genero\": genero,\n",
    "            \"idioma\": idioma\n",
    "        })\n",
    "    print_bar(i, cantidad_registros, contexto=\"Progreso registros limpiados\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ===================== Creamos dataset persistente =====================\n",
    "\n",
    "df_registros = pl.DataFrame(registros_procesados)\n",
    "df_registros.write_parquet(\"data/author_profiling/registros_procesados.parquet\")\n",
    "Logger.info(\"Guardando archivo\")\n",
    "\n",
    "lf_registros = df_registros.lazy()\n",
    "\n",
    "# ===================== Muestra =====================\n",
    "\n",
    "\n",
    "muestra = lf_registros.limit(5).collect()\n",
    "Logger.debug(f\"Total de tweets procesados: {len(registros_procesados)} mostrando 5 primeros registros:\\n {muestra}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45b49c",
   "metadata": {},
   "source": [
    "\n",
    "# Capa de dataclass de utilidd para normalizaciÃ³n y limpieza de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15eed5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import unicodedata\n",
    "# ===================== ConfiguraciÃ³n de Limpieza de tweets =====================\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfigLimpieza:\n",
    "    normalizar_unicode: bool = True\n",
    "    a_minusculas: bool = True\n",
    "    quitar_urls: bool = True\n",
    "    quitar_menciones: bool = True\n",
    "    quitar_hashtags: bool = False \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    URL = re.compile(r'https?://\\S+', re.I)\n",
    "    MENCION = re.compile(r'@\\w+')\n",
    "    HASHTAG = re.compile(r'#\\w+')\n",
    "    ESPACIO = re.compile(r'\\s+')\n",
    "    TT = TweetTokenizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def limpiar(texto: str) -> str:\n",
    "        cfg = ConfigLimpieza()\n",
    "        if cfg.normalizar_unicode:\n",
    "            texto = unicodedata.normalize(\"NFKC\", texto)\n",
    "        if cfg.a_minusculas:\n",
    "            texto = texto.lower()\n",
    "        if cfg.quitar_urls:\n",
    "            texto = Tweet.URL.sub(\" \", texto)\n",
    "        if cfg.quitar_menciones:\n",
    "            texto = Tweet.MENCION.sub(\" \", texto)\n",
    "        if cfg.quitar_hashtags:\n",
    "            texto = Tweet.HASHTAG.sub(\" \", texto)\n",
    "\n",
    "        texto_limpio = Tweet.ESPACIO.sub(\" \", texto).strip()\n",
    "        return texto_limpio\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizar(texto: str) -> list[str]:\n",
    "        return [t for t in Tweet.TT.tokenize(texto)]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eca741",
   "metadata": {},
   "source": [
    "# PIPELINE DE LIMPIEZA Y/O TOKENIZACIÃ“N DE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3714c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] PIPELINE: LIMPIEZA/ TOKENIZACIÃ“N\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe limpio:\n",
      " shape: (2, 6)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id_user              â”† tweet_crudo          â”† pais     â”† genero â”† idioma  â”† texto_limpio         â”‚\n",
      "â”‚ ---                  â”† ---                  â”† ---      â”† ---    â”† ---     â”† ---                  â”‚\n",
      "â”‚ str                  â”† str                  â”† str      â”† str    â”† str     â”† str                  â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 74bcc9b0882c8440716f â”† Tiene que valer la   â”† colombia â”† female â”† EspaÃ±ol â”† tiene que valer la   â”‚\n",
      "â”‚ f370494aeaâ€¦          â”† pena que esâ€¦         â”†          â”†        â”†         â”† pena que esâ€¦         â”‚\n",
      "â”‚ 74bcc9b0882c8440716f â”† Tintas chinas, si    â”† colombia â”† female â”† EspaÃ±ol â”† tintas chinas, si    â”‚\n",
      "â”‚ f370494aeaâ€¦          â”† ven Ã©sto, esâ€¦        â”†          â”†        â”†         â”† ven Ã©sto, esâ€¦        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet limpio: tiene que valer la pena que estÃ© despierta a esta hora #hoylosgrammycon40\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado:\n",
      " shape: (2, 7)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id_user         â”† tweet_crudo    â”† pais     â”† genero â”† idioma  â”† texto_limpio   â”† tweet_tokeniza â”‚\n",
      "â”‚ ---             â”† ---            â”† ---      â”† ---    â”† ---     â”† ---            â”† do             â”‚\n",
      "â”‚ str             â”† str            â”† str      â”† str    â”† str     â”† str            â”† ---            â”‚\n",
      "â”‚                 â”†                â”†          â”†        â”†         â”†                â”† list[str]      â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 74bcc9b0882c844 â”† Tiene que      â”† colombia â”† female â”† EspaÃ±ol â”† tiene que      â”† [\"tiene\",      â”‚\n",
      "â”‚ 0716ff370494aea â”† valer la pena  â”†          â”†        â”†         â”† valer la pena  â”† \"que\", â€¦       â”‚\n",
      "â”‚ â€¦               â”† que esâ€¦        â”†          â”†        â”†         â”† que esâ€¦        â”† \"#hoylosgraâ€¦   â”‚\n",
      "â”‚ 74bcc9b0882c844 â”† Tintas chinas, â”† colombia â”† female â”† EspaÃ±ol â”† tintas chinas, â”† [\"tintas\",     â”‚\n",
      "â”‚ 0716ff370494aea â”† si ven Ã©sto,   â”†          â”†        â”†         â”† si ven Ã©sto,   â”† \"chinas\", â€¦    â”‚\n",
      "â”‚ â€¦               â”† esâ€¦            â”†          â”†        â”†         â”† esâ€¦            â”† \"xoxo\"]        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet tokenizado: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tiene\"\n",
      "\t\"que\"\n",
      "\t\"valer\"\n",
      "\t\"la\"\n",
      "\t\"pena\"\n",
      "\tâ€¦\n",
      "\t\"despierta\"\n",
      "\t\"a\"\n",
      "\t\"esta\"\n",
      "\t\"hora\"\n",
      "\t\"#hoylosgrammycon40\"\n",
      "]\u001b[0m\n",
      "\u001b[94m[INFO] Token: tiene\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: valer\u001b[0m\n",
      "\u001b[94m[INFO] Token: la\u001b[0m\n",
      "\u001b[94m[INFO] Token: pena\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: estÃ©\u001b[0m\n",
      "\u001b[94m[INFO] Token: despierta\u001b[0m\n",
      "\u001b[94m[INFO] Token: a\u001b[0m\n",
      "\u001b[94m[INFO] Token: esta\u001b[0m\n",
      "\u001b[94m[INFO] Token: hora\u001b[0m\n",
      "\u001b[94m[INFO] Token: #hoylosgrammycon40\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== TRANSFORMACIONES EN PIPELINE =====================\n",
    "\"\"\"\n",
    "Recordemos que los pipelines de polars si es un lazyframe no se ejecutan hasta que se les pida con .collect() (DocoumentaciÃ³n)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lf_registros_crudos = lf_registros\n",
    "\n",
    "\n",
    "Logger.info(\"PIPELINE: LIMPIEZA/ TOKENIZACIÃ“N\")\n",
    "lf_limpio = (\n",
    "    lf_registros_crudos.with_columns(\n",
    "        pl.col(\"tweet_crudo\")\n",
    "          .map_elements(Tweet.limpiar)\n",
    "          .alias(\"texto_limpio\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe limpio:\\n {lf_limpio.limit(2).collect()}\")\n",
    "\n",
    "primer_tweet_limpio = (\n",
    "    lf_limpio\n",
    "    .select('texto_limpio')\n",
    "    .limit(1)\n",
    "    .collect()\n",
    "    .to_series()\n",
    "    .to_list()[0]\n",
    ")\n",
    "\n",
    "Logger.info(f\"Primer tweet limpio: {primer_tweet_limpio}\")\n",
    "        \n",
    "lf_tokens = (\n",
    "    lf_limpio.with_columns(\n",
    "        pl.col(\"texto_limpio\")\n",
    "          .map_elements(\n",
    "              Tweet.tokenizar, \n",
    "              return_dtype=pl.List(pl.Utf8), \n",
    "              skip_nulls=True\n",
    "            )\n",
    "          .alias(\"tweet_tokenizado\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado:\\n {lf_tokens.limit(2).collect()}\")\n",
    "\n",
    "\n",
    "# ======================== Metodo para extraer el primer tweet tokenizado con polars =====================\n",
    "primer_tweet_tokenizado = (\n",
    "    lf_tokens\n",
    "    .select(\"tweet_tokenizado\") # Seleccionamos la columna que nos interesa\n",
    "    .limit(1) \n",
    "    .collect()\n",
    "    .item()  # Sirve para extraer el valor cuando el DataFrame es 1x1\n",
    ")\n",
    "# ========================================================================================================\n",
    "\n",
    "Logger.info(f\"Primer tweet tokenizado: {primer_tweet_tokenizado}\")\n",
    "\n",
    "for token in primer_tweet_tokenizado:\n",
    "    Logger.info(f\"Token: {token}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a6e4b",
   "metadata": {},
   "source": [
    "# VAMOS A GENERAR UN VOCABULARIO GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a514f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] CONSTRUCCIÃ“N DE VOCABULARIO GLOBAL\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado y aplanado:\n",
      " shape: (5_908_700, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ id_user                         â”† tokens_del_tweet â”‚\n",
      "â”‚ ---                             â”† ---              â”‚\n",
      "â”‚ str                             â”† str              â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† tiene            â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† que              â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† valer            â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† la               â”‚\n",
      "â”‚ 74bcc9b0882c8440716ff370494aeaâ€¦ â”† pena             â”‚\n",
      "â”‚ â€¦                               â”† â€¦                â”‚\n",
      "â”‚ 7ea28a182539c384d69cba3d10623fâ€¦ â”† recupere         â”‚\n",
      "â”‚ 7ea28a182539c384d69cba3d10623fâ€¦ â”† ,                â”‚\n",
      "â”‚ 7ea28a182539c384d69cba3d10623fâ€¦ â”† saludos          â”‚\n",
      "â”‚ 7ea28a182539c384d69cba3d10623fâ€¦ â”† !                â”‚\n",
      "â”‚ 7ea28a182539c384d69cba3d10623fâ€¦ â”† !                â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando vocabulario:\n",
      " shape: (205_231, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ tokens_del_tweet  â”† frecuencia_global â”† documentos_unicos â”‚\n",
      "â”‚ ---               â”† ---               â”† ---               â”‚\n",
      "â”‚ str               â”† u32               â”† u32               â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ calentitooo       â”† 1                 â”† 1                 â”‚\n",
      "â”‚ #latinossigamonos â”† 1                 â”† 1                 â”‚\n",
      "â”‚ #epica            â”† 1                 â”† 1                 â”‚\n",
      "â”‚ cargante          â”† 3                 â”† 3                 â”‚\n",
      "â”‚ 1,4               â”† 2                 â”† 2                 â”‚\n",
      "â”‚ â€¦                 â”† â€¦                 â”† â€¦                 â”‚\n",
      "â”‚ nadadora          â”† 3                 â”† 3                 â”‚\n",
      "â”‚ nbs               â”† 1                 â”† 1                 â”‚\n",
      "â”‚ #citlaltepetl     â”† 1                 â”† 1                 â”‚\n",
      "â”‚ #cineteca         â”† 1                 â”† 1                 â”‚\n",
      "â”‚ moquitos          â”† 1                 â”† 1                 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== VOCABULARIO GLOBAL =====================\n",
    "\n",
    "Logger.info(\"CONSTRUCCIÃ“N DE VOCABULARIO GLOBAL\")\n",
    "\n",
    "\n",
    "lf_tokens_aplanados = (\n",
    "    lf_tokens\n",
    "    .select(\n",
    "        'id_user', \n",
    "        pl.col('tweet_tokenizado')\n",
    "    )\n",
    "    .explode('tweet_tokenizado')\n",
    "    .rename({'tweet_tokenizado':'tokens_del_tweet'})\n",
    ")\n",
    "\n",
    "\n",
    "lf_vocabulario = (\n",
    "    lf_tokens_aplanados\n",
    "    .group_by('tokens_del_tweet')\n",
    "    .agg([\n",
    "        pl.len().alias('frecuencia_global'),              # Conteo total de apariciones de cada token\n",
    "        pl.n_unique('id_user').alias('documentos_unicos') # En cuÃ¡ntos documentos aparece ese token\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado y aplanado:\\n {lf_tokens_aplanados.collect()}\")\n",
    "Logger.debug(f\"Mostrando vocabulario:\\n {lf_vocabulario.collect()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2054173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] === Ensamble mÃ­nimo: reusar vocabulario y tokens para TR (con lÃ­mites y CSR) ===\u001b[0m\n",
      "\u001b[94m[INFO] Total de documentos (usuarios): 4,200\u001b[0m\n",
      "\u001b[92m[DEBUG] Muestra de etiquetas (y): ['spain', 'peru', 'venezuela', 'peru', 'spain']\u001b[0m\n",
      "\u001b[94m[INFO] Construyendo vocabulario reducido desde lf_vocabularioâ€¦\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/21ypxx3x0019wgj7rqnlm2340000gn/T/ipykernel_22901/3498009944.py:28: DeprecationWarning: `LazyFrame.with_row_count` is deprecated; use `LazyFrame.with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  .with_row_count('indice_doc')       # Ã­ndice de fila en TR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] [DIM-RED] Vocabulario reducido: 38,575 tÃ©rminos (K=50000, min_docs=5)\u001b[0m\n",
      "\u001b[94m[INFO] Contando frecuencias por (id_user, token) y mapeando a Ã­ndicesâ€¦\u001b[0m\n",
      "\u001b[94m[INFO] Tripletas no-cero (indice_doc, indice_token, frecuencia): 2,129,949\u001b[0m\n",
      "\u001b[94m[INFO] Construyendo TR en formato disperso (CSR)â€¦\u001b[0m\n",
      "\u001b[94m[INFO] [CSR] TR_frecuencia: shape=(4200, 38575), nnz=2,129,949\u001b[0m\n",
      "\u001b[94m[INFO] [CSR] TR_binaria:    shape=(4200, 38575), nnz=2,129,949\u001b[0m\n",
      "\u001b[94m[INFO] Normalizando L2 (CSR)â€¦\u001b[0m\n",
      "\u001b[94m[INFO] Split 80/20 estratificado (CSR)â€¦\u001b[0m\n",
      "\u001b[94m[INFO] Shapes â†’ Frecuencia tr=(3360, 38575) va=(840, 38575) | Binaria tr=(3360, 38575) va=(840, 38575)\u001b[0m\n",
      "\u001b[94m[INFO] [LinearSVC] Binario SIN L2\u001b[0m\n",
      "\n",
      "=== Binario SIN L2 ===\n",
      "Accuracy (VA): 0.9381 | Macro-F1 (VA): 0.9381\n",
      "\u001b[94m[INFO] [LinearSVC] Frecuencia SIN L2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ferleon/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Frecuencia SIN L2 ===\n",
      "Accuracy (VA): 0.8857 | Macro-F1 (VA): 0.8856\n",
      "\u001b[94m[INFO] [LinearSVC] Binario CON L2\u001b[0m\n",
      "\n",
      "=== Binario CON L2 ===\n",
      "Accuracy (VA): 0.9452 | Macro-F1 (VA): 0.9453\n",
      "\u001b[94m[INFO] [LinearSVC] Frecuencia CON L2\u001b[0m\n",
      "\n",
      "=== Frecuencia CON L2 ===\n",
      "Accuracy (VA): 0.8655 | Macro-F1 (VA): 0.8651\n",
      "\n",
      "\n",
      "================= RESUMEN VALIDACIÃ“N (80/20) =================\n",
      "Binario SIN L2     -> Acc: 0.9381 | Macro-F1: 0.9381\n",
      "Frecuencia SIN L2  -> Acc: 0.8857 | Macro-F1: 0.8856\n",
      "Binario CON L2     -> Acc: 0.9452 | Macro-F1: 0.9453\n",
      "Frecuencia CON L2  -> Acc: 0.8655 | Macro-F1: 0.8651\n"
     ]
    }
   ],
   "source": [
    "# ===================== TR desde TU pipeline (con lÃ­mites y CSR) =====================\n",
    "# Requisitos previos en tu notebook:\n",
    "# - lf_tokens_aplanados: ['id_user','tokens_del_tweet']\n",
    "# - lf_vocabulario:      ['tokens_del_tweet','frecuencia_global','documentos_unicos']\n",
    "# - lf_limpio:           para obtener el orden de documentos (id_user)\n",
    "# - df_indices:          para etiquetas (pais)\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "Logger.info(\"=== Ensamble mÃ­nimo: reusar vocabulario y tokens para TR (con lÃ­mites y CSR) ===\")\n",
    "\n",
    "# -------------------- ParÃ¡metros de reducciÃ³n de dimensiÃ³n --------------------\n",
    "MAX_TERMINOS = 50_000   # Top-K por frecuencia_global (ajÃºstalo segÃºn tiempo/RAM)\n",
    "MIN_DOCS     = 5        # MÃ­nimo de documentos_unicos por token (descarta tokens ultra raros)\n",
    "\n",
    "# 1) Orden estable de documentos (id_user) para alinear filas de TR e y_train\n",
    "lf_docs = (\n",
    "    lf_limpio\n",
    "    .select(['id_user'])\n",
    "    .unique(maintain_order=True)\n",
    "    .sort('id_user')                    # si prefieres orden â€œnaturalâ€, puedes quitar este sort\n",
    "    .with_row_count('indice_doc')       # Ã­ndice de fila en TR\n",
    ")\n",
    "df_docs = lf_docs.collect()\n",
    "orden_id_user = df_docs['id_user'].to_list()\n",
    "num_docs = len(orden_id_user)\n",
    "Logger.info(f\"Total de documentos (usuarios): {num_docs:,}\")\n",
    "\n",
    "# 2) Etiquetas alineadas (y_train)\n",
    "lf_etiquetas = (\n",
    "    lf_docs.select(['id_user'])\n",
    "    .join(\n",
    "        df_indices.lazy().rename({'id':'id_user'}).select(['id_user','pais']),\n",
    "        on='id_user',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "df_etiquetas = lf_etiquetas.collect()\n",
    "y_train = df_etiquetas['pais'].to_list()\n",
    "Logger.debug(f\"Muestra de etiquetas (y): {y_train[:5]}\")\n",
    "\n",
    "# 3) Vocabulario reducido â†’ diccionario de Ã­ndices (token â†’ columna)\n",
    "Logger.info(\"Construyendo vocabulario reducido desde lf_vocabularioâ€¦\")\n",
    "lf_vocabulario_reducido = (\n",
    "    lf_vocabulario\n",
    "    .filter(pl.col('documentos_unicos') >= MIN_DOCS)      # filtra tokens raros\n",
    "    .sort('frecuencia_global', descending=True)\n",
    "    .limit(MAX_TERMINOS)                                   # tope duro Top-K\n",
    ")\n",
    "\n",
    "tokens_ordenados = (\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    "    .collect()['tokens_del_tweet']\n",
    "    .to_list()\n",
    ")\n",
    "V = len(tokens_ordenados)\n",
    "dicc_indices = {tok: j for j, tok in enumerate(tokens_ordenados)}\n",
    "Logger.info(f\"[DIM-RED] Vocabulario reducido: {V:,} tÃ©rminos (K={MAX_TERMINOS}, min_docs={MIN_DOCS})\")\n",
    "\n",
    "# 4) Conteos por (documento, token) y mapeo a Ã­ndices de fila/columna (todo LazyFrame)\n",
    "Logger.info(\"Contando frecuencias por (id_user, token) y mapeando a Ã­ndicesâ€¦\")\n",
    "\n",
    "# Tablas auxiliares como LazyFrame para joins LFâ†”LF\n",
    "lf_tokens_permitidos = pl.DataFrame({'tokens_del_tweet': tokens_ordenados}).lazy()\n",
    "lf_mapa_tokens = pl.DataFrame({\n",
    "    'tokens_del_tweet': tokens_ordenados,\n",
    "    'indice_token': list(range(V))\n",
    "}).lazy()\n",
    "lf_mapa_docs = lf_docs.select(['id_user','indice_doc'])\n",
    "\n",
    "df_conteos = (\n",
    "    lf_tokens_aplanados\n",
    "    # filtra a tokens que estÃ¡n en el vocabulario reducido\n",
    "    .join(lf_tokens_permitidos, on='tokens_del_tweet', how='inner')\n",
    "    .group_by(['id_user','tokens_del_tweet'])\n",
    "    .agg(pl.len().alias('frecuencia_en_documento'))\n",
    "    # mapear id_user â†’ indice_doc\n",
    "    .join(lf_mapa_docs, on='id_user', how='inner')\n",
    "    # mapear token â†’ indice_token\n",
    "    .join(lf_mapa_tokens, on='tokens_del_tweet', how='inner')\n",
    "    .select(['indice_doc','indice_token','frecuencia_en_documento'])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "Logger.info(f\"Tripletas no-cero (indice_doc, indice_token, frecuencia): {df_conteos.height:,}\")\n",
    "\n",
    "# 5) ConstrucciÃ³n de TR en formato disperso (CSR)\n",
    "Logger.info(\"Construyendo TR en formato disperso (CSR)â€¦\")\n",
    "filas = df_conteos['indice_doc'].to_numpy()\n",
    "cols  = df_conteos['indice_token'].to_numpy()\n",
    "vals  = df_conteos['frecuencia_en_documento'].to_numpy()\n",
    "\n",
    "TR_frecuencia_csr = coo_matrix((vals, (filas, cols)), shape=(num_docs, V)).tocsr()\n",
    "TR_binaria_csr    = TR_frecuencia_csr.copy()\n",
    "TR_binaria_csr.data[:] = 1\n",
    "\n",
    "Logger.info(f\"[CSR] TR_frecuencia: shape={TR_frecuencia_csr.shape}, nnz={TR_frecuencia_csr.nnz:,}\")\n",
    "Logger.info(f\"[CSR] TR_binaria:    shape={TR_binaria_csr.shape}, nnz={TR_binaria_csr.nnz:,}\")\n",
    "\n",
    "# 6) (Opcional) DOR del profe sobre TU TR reducida (si lo necesitas en el reporte)\n",
    "def compute_dor_profe(TR_densa: np.ndarray) -> np.ndarray:\n",
    "    DTR = np.zeros((TR_densa.shape[1], TR_densa.shape[0]), dtype=float)\n",
    "    tam_v = TR_densa.shape[1]\n",
    "    for i, doc in enumerate(TR_densa):\n",
    "        pos_no_cero = np.nonzero(doc)[0]\n",
    "        tam_vocab_doc = len(pos_no_cero)\n",
    "        for termino in pos_no_cero:\n",
    "            DTR[termino, i] = doc[termino] * np.log(tam_v / max(1, tam_vocab_doc))\n",
    "    return DTR\n",
    "\n",
    "# Nota: DOR requiere matriz densa. Si vas justo de tiempo/RAM, sÃ¡ltalo o aplÃ­calo sobre un subset.\n",
    "# Ejemplo (solo si te alcanza memoria):\n",
    "# TR_frecuencia_densa = TR_frecuencia_csr.toarray()\n",
    "# TR_binaria_densa    = TR_binaria_csr.toarray()\n",
    "# DTR_frecuencia = compute_dor_profe(TR_frecuencia_densa)\n",
    "# DTR_binaria    = compute_dor_profe(TR_binaria_densa)\n",
    "# Logger.info(f\"DOR frecuencia: {DTR_frecuencia.shape} | DOR binaria: {DTR_binaria.shape}\")\n",
    "\n",
    "# 7) NormalizaciÃ³n L2 directamente en CSR\n",
    "Logger.info(\"Normalizando L2 (CSR)â€¦\")\n",
    "TR_frecuencia_L2_csr = normalize(TR_frecuencia_csr, norm='l2', copy=True)\n",
    "TR_binaria_L2_csr    = normalize(TR_binaria_csr,    norm='l2', copy=True)\n",
    "\n",
    "# 8) Split 80/20 y entrenamiento con LinearSVC (rÃ¡pido en alto-dim y disperso)\n",
    "Logger.info(\"Split 80/20 estratificado (CSR)â€¦\")\n",
    "Xf_tr, Xf_va, y_tr, y_va   = train_test_split(TR_frecuencia_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_tr, Xb_va, _, _         = train_test_split(TR_binaria_csr,       y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xf_trL2, Xf_vaL2, _, _     = train_test_split(TR_frecuencia_L2_csr, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_trL2, Xb_vaL2, _, _     = train_test_split(TR_binaria_L2_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "Logger.info(f\"Shapes â†’ Frecuencia tr={Xf_tr.shape} va={Xf_va.shape} | Binaria tr={Xb_tr.shape} va={Xb_va.shape}\")\n",
    "\n",
    "def entrenar_y_evaluar(nombre_experimento, Xtr, Xva, ytr, yva):\n",
    "    Logger.info(f\"[LinearSVC] {nombre_experimento}\")\n",
    "    clf = LinearSVC(C=1.0)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    pred = clf.predict(Xva)\n",
    "    acc = accuracy_score(yva, pred)\n",
    "    f1m = f1_score(yva, pred, average='macro')\n",
    "    print(f\"\\n=== {nombre_experimento} ===\")\n",
    "    print(f\"Accuracy (VA): {acc:.4f} | Macro-F1 (VA): {f1m:.4f}\")\n",
    "    return clf, acc, f1m\n",
    "\n",
    "clf_bin,    acc_bin,    f1_bin    = entrenar_y_evaluar(\"Binario SIN L2\",     Xb_tr,    Xb_va,    y_tr, y_va)\n",
    "clf_fre,    acc_fre,    f1_fre    = entrenar_y_evaluar(\"Frecuencia SIN L2\",  Xf_tr,    Xf_va,    y_tr, y_va)\n",
    "clf_bin_l2, acc_bin_l2, f1_bin_l2 = entrenar_y_evaluar(\"Binario CON L2\",     Xb_trL2,  Xb_vaL2,  y_tr, y_va)\n",
    "clf_fre_l2, acc_fre_l2, f1_fre_l2 = entrenar_y_evaluar(\"Frecuencia CON L2\",  Xf_trL2,  Xf_vaL2,  y_tr, y_va)\n",
    "\n",
    "print(\"\\n\\n================= RESUMEN VALIDACIÃ“N (80/20) =================\")\n",
    "print(f\"Binario SIN L2     -> Acc: {acc_bin:.4f} | Macro-F1: {f1_bin:.4f}\")\n",
    "print(f\"Frecuencia SIN L2  -> Acc: {acc_fre:.4f} | Macro-F1: {f1_fre:.4f}\")\n",
    "print(f\"Binario CON L2     -> Acc: {acc_bin_l2:.4f} | Macro-F1: {f1_bin_l2:.4f}\")\n",
    "print(f\"Frecuencia CON L2  -> Acc: {acc_fre_l2:.4f} | Macro-F1: {f1_fre_l2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dee5a",
   "metadata": {},
   "source": [
    "Ok, tenemos que la cura sÃ­ en binario es buena. Cuando tenemos frecuencia baja un poco. El binario con normalizaciÃ³n mejora. Y creemos que tambiÃ©n el de frecuencia pero con normalizaciÃ³n L2 no mejora mucho. Tienen los resultados un poco peorcitos. La validaciÃ³n binaria funciona bien para muestras pequeÃ±as que es de 50.000 en el vocabulario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
