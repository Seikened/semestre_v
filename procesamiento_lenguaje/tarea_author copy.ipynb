{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da3245f",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae49b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from colorstreak import Logger\n",
    "import re\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b42045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
    "    porcentaje = (i + 1) / cantidad_registros * 100\n",
    "    # Con emojis\n",
    "    barra = int(50 * (i + 1) / cantidad_registros) * \"🟩\"\n",
    "    espacio = int(50 - len(barra)) * \"⬛️\"\n",
    "\n",
    "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a55da",
   "metadata": {},
   "source": [
    "# MEJORAR LA CARGA DESDE UN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a4e63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test/truth_test.csv\"\u001b[0m\n",
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train/truth_train.csv\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generar_csv(path_lectura, path_guardado):\n",
    "    # Obtener la ruta absoluta del directorio actual\n",
    "    # Leerlo y convertirlo en un DataFrame .csv para manipularlo mejor\n",
    "    with open(path_lectura, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    data = [line.strip().split(\":::\") for line in lines]\n",
    "\n",
    "\n",
    "    df = pl.DataFrame(data, schema=[\"id\", \"genero\", \"pais\"], orient=\"row\")\n",
    "\n",
    "\n",
    "    # Guardar el DataFrame como un archivo .csv\n",
    "    df.write_csv(path_guardado)\n",
    "    return True\n",
    "\n",
    "# ==================== Configuración de rutas =====================\n",
    "\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "# ===================== Creación de CSV de datos de prueba =====================\n",
    "ruta_lectura_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth.txt\")\n",
    "ruta_guardado_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth_test.csv\")\n",
    "\n",
    "creado = generar_csv(ruta_lectura_prueba, ruta_guardado_prueba)\n",
    "\n",
    "if creado:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_prueba}\"')\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# ===================== Creación de CSV de datos de entrenamiento ==============\n",
    "ruta_lectura_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth.txt\")\n",
    "ruta_guardado_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth_train.csv\")\n",
    "\n",
    "creado_entrenamiento = generar_csv(ruta_lectura_entrenamiento, ruta_guardado_entrenamiento)\n",
    "if creado_entrenamiento:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_entrenamiento}\"')\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e521660",
   "metadata": {},
   "source": [
    "# CARGAR LOS INDICES DESDE TRUTH DESDE EL CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8e4c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬────────┬──────────┐\n",
      "│ id                              ┆ genero ┆ pais     │\n",
      "│ ---                             ┆ ---    ┆ ---      │\n",
      "│ str                             ┆ str    ┆ str      │\n",
      "╞═════════════════════════════════╪════════╪══════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ female ┆ colombia │\n",
      "│ 4639c055f34ca1f944d0137a5aeb79… ┆ female ┆ colombia │\n",
      "│ 92ffa98bade702b86417b118e8aca3… ┆ female ┆ colombia │\n",
      "│ 4560c6567afcccef265f048ed117d0… ┆ female ┆ colombia │\n",
      "│ 393866dfaa80d414c9896cf8723932… ┆ female ┆ colombia │\n",
      "└─────────────────────────────────┴────────┴──────────┘\n",
      "CARGA XML: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Carga de archivos XML completada.\n",
      "\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets crudos cargados: 4200 mostrando 1 registro:\n",
      " id:74bcc9b0882c8440716ff370494aea09\n",
      " pais:colombia\n",
      " genero:female\n",
      " xml_text:\n",
      "<author lang=\"es\">\n",
      "\t<documents>\n",
      "\t\t<document><![CDA...\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def cargar_xml(id_archivo, train=True) -> str:\n",
    "    ruta_base = os.path.join(base_path, \"data/author_profiling/es_test\" if not train else \"data/author_profiling/es_train\")\n",
    "    ruta_archivo = os.path.join(ruta_base, f\"{id_archivo}.xml\")\n",
    "    # Logger.info(f\"ruta archivo: {ruta_archivo}\")\n",
    "    \n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as file:\n",
    "        xml_text = file.read()\n",
    "\n",
    "    return xml_text\n",
    "\n",
    "# ===================== Cargar CSV de datos  ==================\n",
    " \n",
    "df_indices = pl.read_csv(ruta_guardado_entrenamiento)\n",
    "print(df_indices.head())\n",
    "\n",
    "cantidad_registros = len(df_indices)\n",
    "\n",
    "registros_crudos = [(\"id_user\",\"xml_doc\",\"pais\",\"genero\") for i in range(cantidad_registros)]\n",
    "\n",
    "\n",
    "for i, reg in enumerate(registros_crudos):\n",
    "    id_archivo = df_indices['id'][i]\n",
    "    id_user = id_archivo\n",
    "    pais = df_indices['pais'][i]\n",
    "    genero = df_indices['genero'][i]\n",
    "\n",
    "    xml_text: str = cargar_xml(id_archivo)\n",
    "    registros_crudos[i] = (id_user, xml_text, pais, genero)\n",
    "\n",
    "    print_bar(i, cantidad_registros, contexto=\"CARGA XML\")\n",
    "\n",
    "print()\n",
    "Logger.info(\"Carga de archivos XML completada.\\n\")\n",
    "id_user, xml_text, pais, genero = registros_crudos[0]\n",
    "\n",
    "\n",
    "Logger.debug(f\"Total de tweets crudos cargados: {len(registros_crudos)} mostrando 1 registro:\\n id:{id_user}\\n pais:{pais}\\n genero:{genero}\\n xml_text:\\n{xml_text[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4685f1",
   "metadata": {},
   "source": [
    "# PROCESAMOS LOS TEXTOS XML Y LOS DEJAMOS EN UN DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0287180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso registros limpiados: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Guardando archivo\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets procesados: 419998 mostrando 5 primeros registros:\n",
      " shape: (5, 5)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬──────────┬────────┬─────────┐\n",
      "│ id_user                         ┆ tweet_crudo                     ┆ pais     ┆ genero ┆ idioma  │\n",
      "│ ---                             ┆ ---                             ┆ ---      ┆ ---    ┆ ---     │\n",
      "│ str                             ┆ str                             ┆ str      ┆ str    ┆ str     │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪══════════╪════════╪═════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tiene que valer la pena que es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tintas chinas, si ven ésto, es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ \"Maestro no le abrió!\" -Ay qué… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Lo bueno de no estar enamorado… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Recordaré este día como el día… ┆ colombia ┆ female ┆ Español │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴──────────┴────────┴─────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "diccionario_idioma = {\n",
    "    'es': 'Español', \n",
    "    'en': 'Inglés', \n",
    "    'fr': 'Francés', \n",
    "    'de': 'Alemán', \n",
    "    'it': 'Italiano', \n",
    "    'nl': 'Neerlandés'\n",
    "}\n",
    "\n",
    "def limpiar_xml(texto_xml:str):\n",
    "    soup = BeautifulSoup(texto_xml, 'lxml-xml')   \n",
    "    lang = str(soup.author.get('lang'))\n",
    "    idioma = diccionario_idioma[lang] if lang in diccionario_idioma else lang\n",
    "    \n",
    "    documentos = soup.find_all('document') # Obtenemos todsa las etiquetas <document>\n",
    "    tweets = [doc.get_text(separator=\" \", strip=True) for doc in documentos] # Extraemos el texto de cada documento\n",
    "    \n",
    "    return tweets, idioma\n",
    "\n",
    "\n",
    "\n",
    "# ===================== Procesamiento de limpieza del XML =====================\n",
    "cantidad_registros = len(registros_crudos)\n",
    "\n",
    "registros_procesados = []\n",
    "\n",
    "for i, (id_user, doc_crudo, pais, genero) in enumerate(registros_crudos):\n",
    "    lista_tweets_por_usuario, idioma = limpiar_xml(doc_crudo)\n",
    "    \n",
    "    for tweet in lista_tweets_por_usuario:\n",
    "        registros_procesados.append({\n",
    "            \"id_user\": id_user,\n",
    "            \"tweet_crudo\": tweet,\n",
    "            \"pais\": pais,\n",
    "            \"genero\": genero,\n",
    "            \"idioma\": idioma\n",
    "        })\n",
    "    print_bar(i, cantidad_registros, contexto=\"Progreso registros limpiados\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ===================== Creamos dataset persistente =====================\n",
    "\n",
    "df_registros = pl.DataFrame(registros_procesados)\n",
    "df_registros.write_parquet(\"data/author_profiling/registros_procesados.parquet\")\n",
    "Logger.info(\"Guardando archivo\")\n",
    "\n",
    "lf_registros = df_registros.lazy()\n",
    "\n",
    "# ===================== Muestra =====================\n",
    "\n",
    "\n",
    "muestra = lf_registros.limit(5).collect()\n",
    "Logger.debug(f\"Total de tweets procesados: {len(registros_procesados)} mostrando 5 primeros registros:\\n {muestra}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45b49c",
   "metadata": {},
   "source": [
    "\n",
    "# Capa de dataclass de utilidd para normalización y limpieza de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15eed5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import unicodedata\n",
    "# ===================== Configuración de Limpieza de tweets =====================\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfigLimpieza:\n",
    "    normalizar_unicode: bool = True\n",
    "    a_minusculas: bool = True\n",
    "    quitar_urls: bool = True\n",
    "    quitar_menciones: bool = True\n",
    "    quitar_hashtags: bool = False \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    URL = re.compile(r'https?://\\S+', re.I)\n",
    "    MENCION = re.compile(r'@\\w+')\n",
    "    HASHTAG = re.compile(r'#\\w+')\n",
    "    ESPACIO = re.compile(r'\\s+')\n",
    "    TT = TweetTokenizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def limpiar(texto: str) -> str:\n",
    "        cfg = ConfigLimpieza()\n",
    "        if cfg.normalizar_unicode:\n",
    "            texto = unicodedata.normalize(\"NFKC\", texto)\n",
    "        if cfg.a_minusculas:\n",
    "            texto = texto.lower()\n",
    "        if cfg.quitar_urls:\n",
    "            texto = Tweet.URL.sub(\" \", texto)\n",
    "        if cfg.quitar_menciones:\n",
    "            texto = Tweet.MENCION.sub(\" \", texto)\n",
    "        if cfg.quitar_hashtags:\n",
    "            texto = Tweet.HASHTAG.sub(\" \", texto)\n",
    "\n",
    "        texto_limpio = Tweet.ESPACIO.sub(\" \", texto).strip()\n",
    "        return texto_limpio\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizar(texto: str) -> list[str]:\n",
    "        return [t for t in Tweet.TT.tokenize(texto)]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eca741",
   "metadata": {},
   "source": [
    "# PIPELINE DE LIMPIEZA Y/O TOKENIZACIÓN DE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3714c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] PIPELINE: LIMPIEZA/ TOKENIZACIÓN\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe limpio:\n",
      " shape: (2, 6)\n",
      "┌──────────────────────┬──────────────────────┬──────────┬────────┬─────────┬──────────────────────┐\n",
      "│ id_user              ┆ tweet_crudo          ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio         │\n",
      "│ ---                  ┆ ---                  ┆ ---      ┆ ---    ┆ ---     ┆ ---                  │\n",
      "│ str                  ┆ str                  ┆ str      ┆ str    ┆ str     ┆ str                  │\n",
      "╞══════════════════════╪══════════════════════╪══════════╪════════╪═════════╪══════════════════════╡\n",
      "│ 74bcc9b0882c8440716f ┆ Tiene que valer la   ┆ colombia ┆ female ┆ Español ┆ tiene que valer la   │\n",
      "│ f370494aea…          ┆ pena que es…         ┆          ┆        ┆         ┆ pena que es…         │\n",
      "│ 74bcc9b0882c8440716f ┆ Tintas chinas, si    ┆ colombia ┆ female ┆ Español ┆ tintas chinas, si    │\n",
      "│ f370494aea…          ┆ ven ésto, es…        ┆          ┆        ┆         ┆ ven ésto, es…        │\n",
      "└──────────────────────┴──────────────────────┴──────────┴────────┴─────────┴──────────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet limpio: tiene que valer la pena que esté despierta a esta hora #hoylosgrammycon40\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado:\n",
      " shape: (2, 7)\n",
      "┌─────────────────┬────────────────┬──────────┬────────┬─────────┬────────────────┬────────────────┐\n",
      "│ id_user         ┆ tweet_crudo    ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio   ┆ tweet_tokeniza │\n",
      "│ ---             ┆ ---            ┆ ---      ┆ ---    ┆ ---     ┆ ---            ┆ do             │\n",
      "│ str             ┆ str            ┆ str      ┆ str    ┆ str     ┆ str            ┆ ---            │\n",
      "│                 ┆                ┆          ┆        ┆         ┆                ┆ list[str]      │\n",
      "╞═════════════════╪════════════════╪══════════╪════════╪═════════╪════════════════╪════════════════╡\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ [\"tiene\",      │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆ \"que\", …       │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆ \"#hoylosgra…   │\n",
      "│ 74bcc9b0882c844 ┆ Tintas chinas, ┆ colombia ┆ female ┆ Español ┆ tintas chinas, ┆ [\"tintas\",     │\n",
      "│ 0716ff370494aea ┆ si ven ésto,   ┆          ┆        ┆         ┆ si ven ésto,   ┆ \"chinas\", …    │\n",
      "│ …               ┆ es…            ┆          ┆        ┆         ┆ es…            ┆ \"xoxo\"]        │\n",
      "└─────────────────┴────────────────┴──────────┴────────┴─────────┴────────────────┴────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet tokenizado: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tiene\"\n",
      "\t\"que\"\n",
      "\t\"valer\"\n",
      "\t\"la\"\n",
      "\t\"pena\"\n",
      "\t…\n",
      "\t\"despierta\"\n",
      "\t\"a\"\n",
      "\t\"esta\"\n",
      "\t\"hora\"\n",
      "\t\"#hoylosgrammycon40\"\n",
      "]\u001b[0m\n",
      "\u001b[94m[INFO] Token: tiene\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: valer\u001b[0m\n",
      "\u001b[94m[INFO] Token: la\u001b[0m\n",
      "\u001b[94m[INFO] Token: pena\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: esté\u001b[0m\n",
      "\u001b[94m[INFO] Token: despierta\u001b[0m\n",
      "\u001b[94m[INFO] Token: a\u001b[0m\n",
      "\u001b[94m[INFO] Token: esta\u001b[0m\n",
      "\u001b[94m[INFO] Token: hora\u001b[0m\n",
      "\u001b[94m[INFO] Token: #hoylosgrammycon40\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== TRANSFORMACIONES EN PIPELINE =====================\n",
    "\"\"\"\n",
    "Recordemos que los pipelines de polars si es un lazyframe no se ejecutan hasta que se les pida con .collect() (Docoumentación)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lf_registros_crudos = lf_registros\n",
    "\n",
    "\n",
    "Logger.info(\"PIPELINE: LIMPIEZA/ TOKENIZACIÓN\")\n",
    "lf_limpio = (\n",
    "    lf_registros_crudos.with_columns(\n",
    "        pl.col(\"tweet_crudo\")\n",
    "          .map_elements(Tweet.limpiar)\n",
    "          .alias(\"texto_limpio\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe limpio:\\n {lf_limpio.limit(2).collect()}\")\n",
    "\n",
    "primer_tweet_limpio = (\n",
    "    lf_limpio\n",
    "    .select('texto_limpio')\n",
    "    .limit(1)\n",
    "    .collect()\n",
    "    .to_series()\n",
    "    .to_list()[0]\n",
    ")\n",
    "\n",
    "Logger.info(f\"Primer tweet limpio: {primer_tweet_limpio}\")\n",
    "        \n",
    "lf_tokens = (\n",
    "    lf_limpio.with_columns(\n",
    "        pl.col(\"texto_limpio\")\n",
    "          .map_elements(\n",
    "              Tweet.tokenizar, \n",
    "              return_dtype=pl.List(pl.Utf8), \n",
    "              skip_nulls=True\n",
    "            )\n",
    "          .alias(\"tweet_tokenizado\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado:\\n {lf_tokens.limit(2).collect()}\")\n",
    "\n",
    "\n",
    "# ======================== Metodo para extraer el primer tweet tokenizado con polars =====================\n",
    "primer_tweet_tokenizado = (\n",
    "    lf_tokens\n",
    "    .select(\"tweet_tokenizado\") # Seleccionamos la columna que nos interesa\n",
    "    .limit(1) \n",
    "    .collect()\n",
    "    .item()  # Sirve para extraer el valor cuando el DataFrame es 1x1\n",
    ")\n",
    "# ========================================================================================================\n",
    "\n",
    "Logger.info(f\"Primer tweet tokenizado: {primer_tweet_tokenizado}\")\n",
    "\n",
    "for token in primer_tweet_tokenizado:\n",
    "    Logger.info(f\"Token: {token}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a6e4b",
   "metadata": {},
   "source": [
    "# VAMOS A GENERAR UN VOCABULARIO GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a514f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] CONSTRUCCIÓN DE VOCABULARIO GLOBAL\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado y aplanado:\n",
      " shape: (5_908_700, 2)\n",
      "┌─────────────────────────────────┬──────────────────┐\n",
      "│ id_user                         ┆ tokens_del_tweet │\n",
      "│ ---                             ┆ ---              │\n",
      "│ str                             ┆ str              │\n",
      "╞═════════════════════════════════╪══════════════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ tiene            │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ que              │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ valer            │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ la               │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ pena             │\n",
      "│ …                               ┆ …                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ recupere         │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ ,                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ saludos          │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ !                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ !                │\n",
      "└─────────────────────────────────┴──────────────────┘\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando vocabulario:\n",
      " shape: (205_231, 3)\n",
      "┌───────────────────┬───────────────────┬───────────────────┐\n",
      "│ tokens_del_tweet  ┆ frecuencia_global ┆ documentos_unicos │\n",
      "│ ---               ┆ ---               ┆ ---               │\n",
      "│ str               ┆ u32               ┆ u32               │\n",
      "╞═══════════════════╪═══════════════════╪═══════════════════╡\n",
      "│ calentitooo       ┆ 1                 ┆ 1                 │\n",
      "│ #latinossigamonos ┆ 1                 ┆ 1                 │\n",
      "│ #epica            ┆ 1                 ┆ 1                 │\n",
      "│ cargante          ┆ 3                 ┆ 3                 │\n",
      "│ 1,4               ┆ 2                 ┆ 2                 │\n",
      "│ …                 ┆ …                 ┆ …                 │\n",
      "│ nadadora          ┆ 3                 ┆ 3                 │\n",
      "│ nbs               ┆ 1                 ┆ 1                 │\n",
      "│ #citlaltepetl     ┆ 1                 ┆ 1                 │\n",
      "│ #cineteca         ┆ 1                 ┆ 1                 │\n",
      "│ moquitos          ┆ 1                 ┆ 1                 │\n",
      "└───────────────────┴───────────────────┴───────────────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== VOCABULARIO GLOBAL =====================\n",
    "\n",
    "Logger.info(\"CONSTRUCCIÓN DE VOCABULARIO GLOBAL\")\n",
    "\n",
    "\n",
    "lf_tokens_aplanados = (\n",
    "    lf_tokens\n",
    "    .select(\n",
    "        'id_user', \n",
    "        pl.col('tweet_tokenizado')\n",
    "    )\n",
    "    .explode('tweet_tokenizado')\n",
    "    .rename({'tweet_tokenizado':'tokens_del_tweet'})\n",
    ")\n",
    "\n",
    "\n",
    "lf_vocabulario = (\n",
    "    lf_tokens_aplanados\n",
    "    .group_by('tokens_del_tweet')\n",
    "    .agg([\n",
    "        pl.len().alias('frecuencia_global'),              # Conteo total de apariciones de cada token\n",
    "        pl.n_unique('id_user').alias('documentos_unicos') # En cuántos documentos aparece ese token\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado y aplanado:\\n {lf_tokens_aplanados.collect()}\")\n",
    "Logger.debug(f\"Mostrando vocabulario:\\n {lf_vocabulario.collect()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2054173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] === Ensamble mínimo: reusar vocabulario y tokens para TR (con límites y CSR) ===\u001b[0m\n",
      "\u001b[94m[INFO] Total de documentos (usuarios): 4,200\u001b[0m\n",
      "\u001b[92m[DEBUG] Muestra de etiquetas (y): ['spain', 'peru', 'venezuela', 'peru', 'spain']\u001b[0m\n",
      "\u001b[94m[INFO] Construyendo vocabulario reducido desde lf_vocabulario…\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/21ypxx3x0019wgj7rqnlm2340000gn/T/ipykernel_22901/3498009944.py:28: DeprecationWarning: `LazyFrame.with_row_count` is deprecated; use `LazyFrame.with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  .with_row_count('indice_doc')       # índice de fila en TR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] [DIM-RED] Vocabulario reducido: 38,575 términos (K=50000, min_docs=5)\u001b[0m\n",
      "\u001b[94m[INFO] Contando frecuencias por (id_user, token) y mapeando a índices…\u001b[0m\n",
      "\u001b[94m[INFO] Tripletas no-cero (indice_doc, indice_token, frecuencia): 2,129,949\u001b[0m\n",
      "\u001b[94m[INFO] Construyendo TR en formato disperso (CSR)…\u001b[0m\n",
      "\u001b[94m[INFO] [CSR] TR_frecuencia: shape=(4200, 38575), nnz=2,129,949\u001b[0m\n",
      "\u001b[94m[INFO] [CSR] TR_binaria:    shape=(4200, 38575), nnz=2,129,949\u001b[0m\n",
      "\u001b[94m[INFO] Normalizando L2 (CSR)…\u001b[0m\n",
      "\u001b[94m[INFO] Split 80/20 estratificado (CSR)…\u001b[0m\n",
      "\u001b[94m[INFO] Shapes → Frecuencia tr=(3360, 38575) va=(840, 38575) | Binaria tr=(3360, 38575) va=(840, 38575)\u001b[0m\n",
      "\u001b[94m[INFO] [LinearSVC] Binario SIN L2\u001b[0m\n",
      "\n",
      "=== Binario SIN L2 ===\n",
      "Accuracy (VA): 0.9381 | Macro-F1 (VA): 0.9381\n",
      "\u001b[94m[INFO] [LinearSVC] Frecuencia SIN L2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ferleon/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Frecuencia SIN L2 ===\n",
      "Accuracy (VA): 0.8857 | Macro-F1 (VA): 0.8856\n",
      "\u001b[94m[INFO] [LinearSVC] Binario CON L2\u001b[0m\n",
      "\n",
      "=== Binario CON L2 ===\n",
      "Accuracy (VA): 0.9452 | Macro-F1 (VA): 0.9453\n",
      "\u001b[94m[INFO] [LinearSVC] Frecuencia CON L2\u001b[0m\n",
      "\n",
      "=== Frecuencia CON L2 ===\n",
      "Accuracy (VA): 0.8655 | Macro-F1 (VA): 0.8651\n",
      "\n",
      "\n",
      "================= RESUMEN VALIDACIÓN (80/20) =================\n",
      "Binario SIN L2     -> Acc: 0.9381 | Macro-F1: 0.9381\n",
      "Frecuencia SIN L2  -> Acc: 0.8857 | Macro-F1: 0.8856\n",
      "Binario CON L2     -> Acc: 0.9452 | Macro-F1: 0.9453\n",
      "Frecuencia CON L2  -> Acc: 0.8655 | Macro-F1: 0.8651\n"
     ]
    }
   ],
   "source": [
    "# ===================== TR desde TU pipeline (con límites y CSR) =====================\n",
    "# Requisitos previos en tu notebook:\n",
    "# - lf_tokens_aplanados: ['id_user','tokens_del_tweet']\n",
    "# - lf_vocabulario:      ['tokens_del_tweet','frecuencia_global','documentos_unicos']\n",
    "# - lf_limpio:           para obtener el orden de documentos (id_user)\n",
    "# - df_indices:          para etiquetas (pais)\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "Logger.info(\"=== Ensamble mínimo: reusar vocabulario y tokens para TR (con límites y CSR) ===\")\n",
    "\n",
    "# -------------------- Parámetros de reducción de dimensión --------------------\n",
    "MAX_TERMINOS = 50_000   # Top-K por frecuencia_global (ajústalo según tiempo/RAM)\n",
    "MIN_DOCS     = 5        # Mínimo de documentos_unicos por token (descarta tokens ultra raros)\n",
    "\n",
    "# 1) Orden estable de documentos (id_user) para alinear filas de TR e y_train\n",
    "lf_docs = (\n",
    "    lf_limpio\n",
    "    .select(['id_user'])\n",
    "    .unique(maintain_order=True)\n",
    "    .sort('id_user')                    # si prefieres orden “natural”, puedes quitar este sort\n",
    "    .with_row_count('indice_doc')       # índice de fila en TR\n",
    ")\n",
    "df_docs = lf_docs.collect()\n",
    "orden_id_user = df_docs['id_user'].to_list()\n",
    "num_docs = len(orden_id_user)\n",
    "Logger.info(f\"Total de documentos (usuarios): {num_docs:,}\")\n",
    "\n",
    "# 2) Etiquetas alineadas (y_train)\n",
    "lf_etiquetas = (\n",
    "    lf_docs.select(['id_user'])\n",
    "    .join(\n",
    "        df_indices.lazy().rename({'id':'id_user'}).select(['id_user','pais']),\n",
    "        on='id_user',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "df_etiquetas = lf_etiquetas.collect()\n",
    "y_train = df_etiquetas['pais'].to_list()\n",
    "Logger.debug(f\"Muestra de etiquetas (y): {y_train[:5]}\")\n",
    "\n",
    "# 3) Vocabulario reducido → diccionario de índices (token → columna)\n",
    "Logger.info(\"Construyendo vocabulario reducido desde lf_vocabulario…\")\n",
    "lf_vocabulario_reducido = (\n",
    "    lf_vocabulario\n",
    "    .filter(pl.col('documentos_unicos') >= MIN_DOCS)      # filtra tokens raros\n",
    "    .sort('frecuencia_global', descending=True)\n",
    "    .limit(MAX_TERMINOS)                                   # tope duro Top-K\n",
    ")\n",
    "\n",
    "tokens_ordenados = (\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    "    .collect()['tokens_del_tweet']\n",
    "    .to_list()\n",
    ")\n",
    "V = len(tokens_ordenados)\n",
    "dicc_indices = {tok: j for j, tok in enumerate(tokens_ordenados)}\n",
    "Logger.info(f\"[DIM-RED] Vocabulario reducido: {V:,} términos (K={MAX_TERMINOS}, min_docs={MIN_DOCS})\")\n",
    "\n",
    "# 4) Conteos por (documento, token) y mapeo a índices de fila/columna (todo LazyFrame)\n",
    "Logger.info(\"Contando frecuencias por (id_user, token) y mapeando a índices…\")\n",
    "\n",
    "# Tablas auxiliares como LazyFrame para joins LF↔LF\n",
    "lf_tokens_permitidos = pl.DataFrame({'tokens_del_tweet': tokens_ordenados}).lazy()\n",
    "lf_mapa_tokens = pl.DataFrame({\n",
    "    'tokens_del_tweet': tokens_ordenados,\n",
    "    'indice_token': list(range(V))\n",
    "}).lazy()\n",
    "lf_mapa_docs = lf_docs.select(['id_user','indice_doc'])\n",
    "\n",
    "df_conteos = (\n",
    "    lf_tokens_aplanados\n",
    "    # filtra a tokens que están en el vocabulario reducido\n",
    "    .join(lf_tokens_permitidos, on='tokens_del_tweet', how='inner')\n",
    "    .group_by(['id_user','tokens_del_tweet'])\n",
    "    .agg(pl.len().alias('frecuencia_en_documento'))\n",
    "    # mapear id_user → indice_doc\n",
    "    .join(lf_mapa_docs, on='id_user', how='inner')\n",
    "    # mapear token → indice_token\n",
    "    .join(lf_mapa_tokens, on='tokens_del_tweet', how='inner')\n",
    "    .select(['indice_doc','indice_token','frecuencia_en_documento'])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "Logger.info(f\"Tripletas no-cero (indice_doc, indice_token, frecuencia): {df_conteos.height:,}\")\n",
    "\n",
    "# 5) Construcción de TR en formato disperso (CSR)\n",
    "Logger.info(\"Construyendo TR en formato disperso (CSR)…\")\n",
    "filas = df_conteos['indice_doc'].to_numpy()\n",
    "cols  = df_conteos['indice_token'].to_numpy()\n",
    "vals  = df_conteos['frecuencia_en_documento'].to_numpy()\n",
    "\n",
    "TR_frecuencia_csr = coo_matrix((vals, (filas, cols)), shape=(num_docs, V)).tocsr()\n",
    "TR_binaria_csr    = TR_frecuencia_csr.copy()\n",
    "TR_binaria_csr.data[:] = 1\n",
    "\n",
    "Logger.info(f\"[CSR] TR_frecuencia: shape={TR_frecuencia_csr.shape}, nnz={TR_frecuencia_csr.nnz:,}\")\n",
    "Logger.info(f\"[CSR] TR_binaria:    shape={TR_binaria_csr.shape}, nnz={TR_binaria_csr.nnz:,}\")\n",
    "\n",
    "# 6) (Opcional) DOR del profe sobre TU TR reducida (si lo necesitas en el reporte)\n",
    "def compute_dor_profe(TR_densa: np.ndarray) -> np.ndarray:\n",
    "    DTR = np.zeros((TR_densa.shape[1], TR_densa.shape[0]), dtype=float)\n",
    "    tam_v = TR_densa.shape[1]\n",
    "    for i, doc in enumerate(TR_densa):\n",
    "        pos_no_cero = np.nonzero(doc)[0]\n",
    "        tam_vocab_doc = len(pos_no_cero)\n",
    "        for termino in pos_no_cero:\n",
    "            DTR[termino, i] = doc[termino] * np.log(tam_v / max(1, tam_vocab_doc))\n",
    "    return DTR\n",
    "\n",
    "# Nota: DOR requiere matriz densa. Si vas justo de tiempo/RAM, sáltalo o aplícalo sobre un subset.\n",
    "# Ejemplo (solo si te alcanza memoria):\n",
    "# TR_frecuencia_densa = TR_frecuencia_csr.toarray()\n",
    "# TR_binaria_densa    = TR_binaria_csr.toarray()\n",
    "# DTR_frecuencia = compute_dor_profe(TR_frecuencia_densa)\n",
    "# DTR_binaria    = compute_dor_profe(TR_binaria_densa)\n",
    "# Logger.info(f\"DOR frecuencia: {DTR_frecuencia.shape} | DOR binaria: {DTR_binaria.shape}\")\n",
    "\n",
    "# 7) Normalización L2 directamente en CSR\n",
    "Logger.info(\"Normalizando L2 (CSR)…\")\n",
    "TR_frecuencia_L2_csr = normalize(TR_frecuencia_csr, norm='l2', copy=True)\n",
    "TR_binaria_L2_csr    = normalize(TR_binaria_csr,    norm='l2', copy=True)\n",
    "\n",
    "# 8) Split 80/20 y entrenamiento con LinearSVC (rápido en alto-dim y disperso)\n",
    "Logger.info(\"Split 80/20 estratificado (CSR)…\")\n",
    "Xf_tr, Xf_va, y_tr, y_va   = train_test_split(TR_frecuencia_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_tr, Xb_va, _, _         = train_test_split(TR_binaria_csr,       y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xf_trL2, Xf_vaL2, _, _     = train_test_split(TR_frecuencia_L2_csr, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_trL2, Xb_vaL2, _, _     = train_test_split(TR_binaria_L2_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "Logger.info(f\"Shapes → Frecuencia tr={Xf_tr.shape} va={Xf_va.shape} | Binaria tr={Xb_tr.shape} va={Xb_va.shape}\")\n",
    "\n",
    "def entrenar_y_evaluar(nombre_experimento, Xtr, Xva, ytr, yva):\n",
    "    Logger.info(f\"[LinearSVC] {nombre_experimento}\")\n",
    "    clf = LinearSVC(C=1.0)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    pred = clf.predict(Xva)\n",
    "    acc = accuracy_score(yva, pred)\n",
    "    f1m = f1_score(yva, pred, average='macro')\n",
    "    print(f\"\\n=== {nombre_experimento} ===\")\n",
    "    print(f\"Accuracy (VA): {acc:.4f} | Macro-F1 (VA): {f1m:.4f}\")\n",
    "    return clf, acc, f1m\n",
    "\n",
    "clf_bin,    acc_bin,    f1_bin    = entrenar_y_evaluar(\"Binario SIN L2\",     Xb_tr,    Xb_va,    y_tr, y_va)\n",
    "clf_fre,    acc_fre,    f1_fre    = entrenar_y_evaluar(\"Frecuencia SIN L2\",  Xf_tr,    Xf_va,    y_tr, y_va)\n",
    "clf_bin_l2, acc_bin_l2, f1_bin_l2 = entrenar_y_evaluar(\"Binario CON L2\",     Xb_trL2,  Xb_vaL2,  y_tr, y_va)\n",
    "clf_fre_l2, acc_fre_l2, f1_fre_l2 = entrenar_y_evaluar(\"Frecuencia CON L2\",  Xf_trL2,  Xf_vaL2,  y_tr, y_va)\n",
    "\n",
    "print(\"\\n\\n================= RESUMEN VALIDACIÓN (80/20) =================\")\n",
    "print(f\"Binario SIN L2     -> Acc: {acc_bin:.4f} | Macro-F1: {f1_bin:.4f}\")\n",
    "print(f\"Frecuencia SIN L2  -> Acc: {acc_fre:.4f} | Macro-F1: {f1_fre:.4f}\")\n",
    "print(f\"Binario CON L2     -> Acc: {acc_bin_l2:.4f} | Macro-F1: {f1_bin_l2:.4f}\")\n",
    "print(f\"Frecuencia CON L2  -> Acc: {acc_fre_l2:.4f} | Macro-F1: {f1_fre_l2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dee5a",
   "metadata": {},
   "source": [
    "Ok, tenemos que la cura sí en binario es buena. Cuando tenemos frecuencia baja un poco. El binario con normalización mejora. Y creemos que también el de frecuencia pero con normalización L2 no mejora mucho. Tienen los resultados un poco peorcitos. La validación binaria funciona bien para muestras pequeñas que es de 50.000 en el vocabulario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
