{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da3245f",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae49b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from colorstreak import Logger\n",
    "import re\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b42045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
    "    porcentaje = (i + 1) / cantidad_registros * 100\n",
    "    # Con emojis\n",
    "    barra = int(50 * (i + 1) / cantidad_registros) * \"🟩\"\n",
    "    espacio = int(50 - len(barra)) * \"⬛️\"\n",
    "\n",
    "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a55da",
   "metadata": {},
   "source": [
    "# MEJORAR LA CARGA DESDE UN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4e63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test/truth_test.csv\"\u001b[0m\n",
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train/truth_train.csv\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generar_csv(path_lectura, path_guardado):\n",
    "    # Obtener la ruta absoluta del directorio actual\n",
    "    # Leerlo y convertirlo en un DataFrame .csv para manipularlo mejor\n",
    "    with open(path_lectura, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    data = [line.strip().split(\":::\") for line in lines]\n",
    "\n",
    "\n",
    "    df = pl.DataFrame(data, schema=[\"id\", \"genero\", \"pais\"], orient=\"row\")\n",
    "\n",
    "\n",
    "    # Guardar el DataFrame como un archivo .csv\n",
    "    df.write_csv(path_guardado)\n",
    "    return True\n",
    "\n",
    "# ==================== Configuración de rutas =====================\n",
    "\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "# ===================== Creación de CSV de datos de prueba =====================\n",
    "ruta_lectura_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth.txt\")\n",
    "ruta_guardado_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth_test.csv\")\n",
    "\n",
    "creado = generar_csv(ruta_lectura_prueba, ruta_guardado_prueba)\n",
    "\n",
    "if creado:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_prueba}\"')\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# ===================== Creación de CSV de datos de entrenamiento ==============\n",
    "ruta_lectura_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth.txt\")\n",
    "ruta_guardado_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth_train.csv\")\n",
    "\n",
    "creado_entrenamiento = generar_csv(ruta_lectura_entrenamiento, ruta_guardado_entrenamiento)\n",
    "if creado_entrenamiento:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_entrenamiento}\"')\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e521660",
   "metadata": {},
   "source": [
    "# CARGAR LOS INDICES DESDE TRUTH DESDE EL CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e4c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬────────┬──────────┐\n",
      "│ id                              ┆ genero ┆ pais     │\n",
      "│ ---                             ┆ ---    ┆ ---      │\n",
      "│ str                             ┆ str    ┆ str      │\n",
      "╞═════════════════════════════════╪════════╪══════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ female ┆ colombia │\n",
      "│ 4639c055f34ca1f944d0137a5aeb79… ┆ female ┆ colombia │\n",
      "│ 92ffa98bade702b86417b118e8aca3… ┆ female ┆ colombia │\n",
      "│ 4560c6567afcccef265f048ed117d0… ┆ female ┆ colombia │\n",
      "│ 393866dfaa80d414c9896cf8723932… ┆ female ┆ colombia │\n",
      "└─────────────────────────────────┴────────┴──────────┘\n",
      "CARGA XML: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Carga de archivos XML completada.\n",
      "\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets crudos cargados: 4200 mostrando 1 registro:\n",
      " id:74bcc9b0882c8440716ff370494aea09\n",
      " pais:colombia\n",
      " genero:female\n",
      " xml_text:\n",
      "<author lang=\"es\">\n",
      "\t<documents>\n",
      "\t\t<document><![CDA...\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def cargar_xml(id_archivo, train=True) -> str:\n",
    "    ruta_base = os.path.join(base_path, \"data/author_profiling/es_test\" if not train else \"data/author_profiling/es_train\")\n",
    "    ruta_archivo = os.path.join(ruta_base, f\"{id_archivo}.xml\")\n",
    "    # Logger.info(f\"ruta archivo: {ruta_archivo}\")\n",
    "    \n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as file:\n",
    "        xml_text = file.read()\n",
    "\n",
    "    return xml_text\n",
    "\n",
    "# ===================== Cargar CSV de datos  ==================\n",
    " \n",
    "df_indices = pl.read_csv(ruta_guardado_entrenamiento)\n",
    "print(df_indices.head())\n",
    "\n",
    "cantidad_registros = len(df_indices)\n",
    "\n",
    "registros_crudos = [(\"id_user\",\"xml_doc\",\"pais\",\"genero\") for i in range(cantidad_registros)]\n",
    "\n",
    "\n",
    "for i, reg in enumerate(registros_crudos):\n",
    "    id_archivo = df_indices['id'][i]\n",
    "    id_user = id_archivo\n",
    "    pais = df_indices['pais'][i]\n",
    "    genero = df_indices['genero'][i]\n",
    "\n",
    "    xml_text: str = cargar_xml(id_archivo)\n",
    "    registros_crudos[i] = (id_user, xml_text, pais, genero)\n",
    "\n",
    "    print_bar(i, cantidad_registros, contexto=\"CARGA XML\")\n",
    "\n",
    "print()\n",
    "Logger.info(\"Carga de archivos XML completada.\\n\")\n",
    "id_user, xml_text, pais, genero = registros_crudos[0]\n",
    "\n",
    "\n",
    "Logger.debug(f\"Total de tweets crudos cargados: {len(registros_crudos)} mostrando 1 registro:\\n id:{id_user}\\n pais:{pais}\\n genero:{genero}\\n xml_text:\\n{xml_text[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4685f1",
   "metadata": {},
   "source": [
    "# PROCESAMOS LOS TEXTOS XML Y LOS DEJAMOS EN UN DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0287180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso registros limpiados: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Guardando archivo\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets procesados: 419998 mostrando 5 primeros registros:\n",
      " shape: (5, 5)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬──────────┬────────┬─────────┐\n",
      "│ id_user                         ┆ tweet_crudo                     ┆ pais     ┆ genero ┆ idioma  │\n",
      "│ ---                             ┆ ---                             ┆ ---      ┆ ---    ┆ ---     │\n",
      "│ str                             ┆ str                             ┆ str      ┆ str    ┆ str     │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪══════════╪════════╪═════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tiene que valer la pena que es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tintas chinas, si ven ésto, es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ \"Maestro no le abrió!\" -Ay qué… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Lo bueno de no estar enamorado… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Recordaré este día como el día… ┆ colombia ┆ female ┆ Español │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴──────────┴────────┴─────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "diccionario_idioma = {\n",
    "    'es': 'Español', \n",
    "    'en': 'Inglés', \n",
    "    'fr': 'Francés', \n",
    "    'de': 'Alemán', \n",
    "    'it': 'Italiano', \n",
    "    'nl': 'Neerlandés'\n",
    "}\n",
    "\n",
    "def limpiar_xml(texto_xml:str):\n",
    "    soup = BeautifulSoup(texto_xml, 'lxml-xml')   \n",
    "    lang = str(soup.author.get('lang'))\n",
    "    idioma = diccionario_idioma[lang] if lang in diccionario_idioma else lang\n",
    "    \n",
    "    documentos = soup.find_all('document') # Obtenemos todsa las etiquetas <document>\n",
    "    tweets = [doc.get_text(separator=\" \", strip=True) for doc in documentos] # Extraemos el texto de cada documento\n",
    "    \n",
    "    return tweets, idioma\n",
    "\n",
    "\n",
    "\n",
    "# ===================== Procesamiento de limpieza del XML =====================\n",
    "cantidad_registros = len(registros_crudos)\n",
    "\n",
    "registros_procesados = []\n",
    "\n",
    "for i, (id_user, doc_crudo, pais, genero) in enumerate(registros_crudos):\n",
    "    lista_tweets_por_usuario, idioma = limpiar_xml(doc_crudo)\n",
    "    \n",
    "    for tweet in lista_tweets_por_usuario:\n",
    "        registros_procesados.append({\n",
    "            \"id_user\": id_user,\n",
    "            \"tweet_crudo\": tweet,\n",
    "            \"pais\": pais,\n",
    "            \"genero\": genero,\n",
    "            \"idioma\": idioma\n",
    "        })\n",
    "    print_bar(i, cantidad_registros, contexto=\"Progreso registros limpiados\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ===================== Creamos dataset persistente =====================\n",
    "\n",
    "df_registros = pl.DataFrame(registros_procesados)\n",
    "df_registros.write_parquet(\"data/author_profiling/registros_procesados.parquet\")\n",
    "Logger.info(\"Guardando archivo\")\n",
    "\n",
    "lf_registros = df_registros.lazy()\n",
    "\n",
    "# ===================== Muestra =====================\n",
    "\n",
    "\n",
    "muestra = lf_registros.limit(5).collect()\n",
    "Logger.debug(f\"Total de tweets procesados: {len(registros_procesados)} mostrando 5 primeros registros:\\n {muestra}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45b49c",
   "metadata": {},
   "source": [
    "\n",
    "# Capa de dataclass de utilidd para normalización y limpieza de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15eed5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import unicodedata\n",
    "# ===================== Configuración de Limpieza de tweets =====================\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfigLimpieza:\n",
    "    normalizar_unicode: bool = True\n",
    "    a_minusculas: bool = True\n",
    "    quitar_urls: bool = True\n",
    "    quitar_menciones: bool = True\n",
    "    quitar_hashtags: bool = False \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    URL = re.compile(r'https?://\\S+', re.I)\n",
    "    MENCION = re.compile(r'@\\w+')\n",
    "    HASHTAG = re.compile(r'#\\w+')\n",
    "    ESPACIO = re.compile(r'\\s+')\n",
    "    TT = TweetTokenizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def limpiar(texto: str) -> str:\n",
    "        cfg = ConfigLimpieza()\n",
    "        if cfg.normalizar_unicode:\n",
    "            texto = unicodedata.normalize(\"NFKC\", texto)\n",
    "        if cfg.a_minusculas:\n",
    "            texto = texto.lower()\n",
    "        if cfg.quitar_urls:\n",
    "            texto = Tweet.URL.sub(\" \", texto)\n",
    "        if cfg.quitar_menciones:\n",
    "            texto = Tweet.MENCION.sub(\" \", texto)\n",
    "        if cfg.quitar_hashtags:\n",
    "            texto = Tweet.HASHTAG.sub(\" \", texto)\n",
    "\n",
    "        texto_limpio = Tweet.ESPACIO.sub(\" \", texto).strip()\n",
    "        return texto_limpio\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizar(texto: str) -> list[str]:\n",
    "        return [t for t in Tweet.TT.tokenize(texto)]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eca741",
   "metadata": {},
   "source": [
    "# PIPELINE DE LIMPIEZA Y/O TOKENIZACIÓN DE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3714c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] PIPELINE: LIMPIEZA/ TOKENIZACIÓN\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe limpio:\n",
      " shape: (2, 6)\n",
      "┌──────────────────────┬──────────────────────┬──────────┬────────┬─────────┬──────────────────────┐\n",
      "│ id_user              ┆ tweet_crudo          ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio         │\n",
      "│ ---                  ┆ ---                  ┆ ---      ┆ ---    ┆ ---     ┆ ---                  │\n",
      "│ str                  ┆ str                  ┆ str      ┆ str    ┆ str     ┆ str                  │\n",
      "╞══════════════════════╪══════════════════════╪══════════╪════════╪═════════╪══════════════════════╡\n",
      "│ 74bcc9b0882c8440716f ┆ Tiene que valer la   ┆ colombia ┆ female ┆ Español ┆ tiene que valer la   │\n",
      "│ f370494aea…          ┆ pena que es…         ┆          ┆        ┆         ┆ pena que es…         │\n",
      "│ 74bcc9b0882c8440716f ┆ Tintas chinas, si    ┆ colombia ┆ female ┆ Español ┆ tintas chinas, si    │\n",
      "│ f370494aea…          ┆ ven ésto, es…        ┆          ┆        ┆         ┆ ven ésto, es…        │\n",
      "└──────────────────────┴──────────────────────┴──────────┴────────┴─────────┴──────────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet limpio: tiene que valer la pena que esté despierta a esta hora #hoylosgrammycon40\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado:\n",
      " shape: (2, 7)\n",
      "┌─────────────────┬────────────────┬──────────┬────────┬─────────┬────────────────┬────────────────┐\n",
      "│ id_user         ┆ tweet_crudo    ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio   ┆ tweets_tokeniz │\n",
      "│ ---             ┆ ---            ┆ ---      ┆ ---    ┆ ---     ┆ ---            ┆ ados           │\n",
      "│ str             ┆ str            ┆ str      ┆ str    ┆ str     ┆ str            ┆ ---            │\n",
      "│                 ┆                ┆          ┆        ┆         ┆                ┆ list[str]      │\n",
      "╞═════════════════╪════════════════╪══════════╪════════╪═════════╪════════════════╪════════════════╡\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ [\"tiene\",      │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆ \"que\", …       │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆ \"#hoylosgra…   │\n",
      "│ 74bcc9b0882c844 ┆ Tintas chinas, ┆ colombia ┆ female ┆ Español ┆ tintas chinas, ┆ [\"tintas\",     │\n",
      "│ 0716ff370494aea ┆ si ven ésto,   ┆          ┆        ┆         ┆ si ven ésto,   ┆ \"chinas\", …    │\n",
      "│ …               ┆ es…            ┆          ┆        ┆         ┆ es…            ┆ \"xoxo\"]        │\n",
      "└─────────────────┴────────────────┴──────────┴────────┴─────────┴────────────────┴────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet tokenizado: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tiene\"\n",
      "\t\"que\"\n",
      "\t\"valer\"\n",
      "\t\"la\"\n",
      "\t\"pena\"\n",
      "\t…\n",
      "\t\"despierta\"\n",
      "\t\"a\"\n",
      "\t\"esta\"\n",
      "\t\"hora\"\n",
      "\t\"#hoylosgrammycon40\"\n",
      "]\u001b[0m\n",
      "\u001b[94m[INFO] Token: tiene\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: valer\u001b[0m\n",
      "\u001b[94m[INFO] Token: la\u001b[0m\n",
      "\u001b[94m[INFO] Token: pena\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: esté\u001b[0m\n",
      "\u001b[94m[INFO] Token: despierta\u001b[0m\n",
      "\u001b[94m[INFO] Token: a\u001b[0m\n",
      "\u001b[94m[INFO] Token: esta\u001b[0m\n",
      "\u001b[94m[INFO] Token: hora\u001b[0m\n",
      "\u001b[94m[INFO] Token: #hoylosgrammycon40\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== TRANSFORMACIONES EN PIPELINE =====================\n",
    "\"\"\"\n",
    "Recordemos que los pipelines de polars si es un lazyframe no se ejecutan hasta que se les pida con .collect() (Docoumentación)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lf_registros_crudos = lf_registros\n",
    "\n",
    "\n",
    "Logger.info(\"PIPELINE: LIMPIEZA/ TOKENIZACIÓN\")\n",
    "lf_limpio = (\n",
    "    lf_registros_crudos.with_columns(\n",
    "        pl.col(\"tweet_crudo\")\n",
    "          .map_elements(Tweet.limpiar)\n",
    "          .alias(\"texto_limpio\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe limpio:\\n {lf_limpio.limit(2).collect()}\")\n",
    "\n",
    "primer_tweet_limpio = (\n",
    "    lf_limpio\n",
    "    .select('texto_limpio')\n",
    "    .limit(1)\n",
    "    .collect()\n",
    "    .to_series()\n",
    "    .to_list()[0]\n",
    ")\n",
    "\n",
    "Logger.info(f\"Primer tweet limpio: {primer_tweet_limpio}\")\n",
    "        \n",
    "lf_tokens = (\n",
    "    lf_limpio.with_columns(\n",
    "        pl.col(\"texto_limpio\")\n",
    "          .map_elements(\n",
    "              Tweet.tokenizar, \n",
    "              return_dtype=pl.List(pl.Utf8), \n",
    "              skip_nulls=True\n",
    "            )\n",
    "          .alias(\"tweets_tokenizados\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado:\\n {lf_tokens.limit(2).collect()}\")\n",
    "\n",
    "\n",
    "# ======================== Metodo para extraer el primer tweet tokenizado con polars =====================\n",
    "primer_tweet_tokenizado = (\n",
    "    lf_tokens\n",
    "    .select(\"tweets_tokenizados\") # Seleccionamos la columna que nos interesa\n",
    "    .limit(1) \n",
    "    .collect()\n",
    "    .item()  # Sirve para extraer el valor cuando el DataFrame es 1x1\n",
    ")\n",
    "# ========================================================================================================\n",
    "\n",
    "Logger.info(f\"Primer tweet tokenizado: {primer_tweet_tokenizado}\")\n",
    "\n",
    "for token in primer_tweet_tokenizado:\n",
    "    Logger.info(f\"Token: {token}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a514f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] CONSTRUCCIÓN DE VOCABULARIO GLOBAL\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe explotado:\n",
      " shape: (5, 7)\n",
      "┌─────────────────┬────────────────┬──────────┬────────┬─────────┬────────────────┬────────────────┐\n",
      "│ id_user         ┆ tweet_crudo    ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio   ┆ tweets_tokeniz │\n",
      "│ ---             ┆ ---            ┆ ---      ┆ ---    ┆ ---     ┆ ---            ┆ ados           │\n",
      "│ str             ┆ str            ┆ str      ┆ str    ┆ str     ┆ str            ┆ ---            │\n",
      "│                 ┆                ┆          ┆        ┆         ┆                ┆ str            │\n",
      "╞═════════════════╪════════════════╪══════════╪════════╪═════════╪════════════════╪════════════════╡\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ tiene          │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆                │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆                │\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ que            │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆                │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆                │\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ valer          │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆                │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆                │\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ la             │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆                │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆                │\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ pena           │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆                │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆                │\n",
      "└─────────────────┴────────────────┴──────────┴────────┴─────────┴────────────────┴────────────────┘\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe con conteos:\n",
      " shape: (5, 2)\n",
      "┌───────┬────────┐\n",
      "│ token ┆ len    │\n",
      "│ ---   ┆ ---    │\n",
      "│ str   ┆ u32    │\n",
      "╞═══════╪════════╡\n",
      "│ de    ┆ 215391 │\n",
      "│ ,     ┆ 161864 │\n",
      "│ .     ┆ 156802 │\n",
      "│ que   ┆ 149950 │\n",
      "│ la    ┆ 135889 │\n",
      "└───────┴────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Total de tokens únicos en vocabulario: 205231. Mostrando 5 primeros:\n",
      " shape: (5, 3)\n",
      "┌──────────┬───────┬────────┐\n",
      "│ token_id ┆ token ┆ len    │\n",
      "│ ---      ┆ ---   ┆ ---    │\n",
      "│ u32      ┆ str   ┆ u32    │\n",
      "╞══════════╪═══════╪════════╡\n",
      "│ 0        ┆ de    ┆ 215391 │\n",
      "│ 1        ┆ ,     ┆ 161864 │\n",
      "│ 2        ┆ .     ┆ 156802 │\n",
      "│ 3        ┆ que   ┆ 149950 │\n",
      "│ 4        ┆ la    ┆ 135889 │\n",
      "└──────────┴───────┴────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== VOCABULARIO GLOBAL =====================\n",
    "\n",
    "Logger.info(\"CONSTRUCCIÓN DE VOCABULARIO GLOBAL\")\n",
    "\n",
    "# 1) Aplanamos la lista de tokens a filas\n",
    "lf_explotado = lf_tokens.explode(\"tweets_tokenizados\")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe explotado:\\n {lf_explotado.limit(5).collect()}\")\n",
    "\n",
    "# 2) Conteo de frecuencia global de cada token\n",
    "lf_conteos = (\n",
    "    lf_explotado\n",
    "    .group_by(pl.col(\"tweets_tokenizados\").alias(\"token\"))\n",
    "    .len()\n",
    "    .sort(\"len\", descending=True)  # o .sort(\"token\") si quieres orden alfabético\n",
    "\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe con conteos:\\n {lf_conteos.limit(5).collect()}\")\n",
    "\n",
    "# 3) Asignamos un ID de token estable (0..V-1) y persistimos\n",
    "df_vocab = (\n",
    "    lf_conteos\n",
    "    .with_row_index(name=\"token_id\", offset=0)  # reemplaza with_row_count\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "Logger.info(f\"Total de tokens únicos en vocabulario: {len(df_vocab)}. Mostrando 5 primeros:\\n {df_vocab.head()}\")\n",
    "df_vocab.write_parquet(\"data/author_profiling/vocabulario.parquet\")\n",
    "# schema: token_id: u32, token: Utf8, len: u32  (len = frecuencia global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad80ef7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
