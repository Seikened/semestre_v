{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da3245f",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae49b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from colorstreak import Logger\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b42045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
    "    porcentaje = (i + 1) / cantidad_registros * 100\n",
    "    # Con emojis\n",
    "    barra = int(50 * (i + 1) / cantidad_registros) * \"🟩\"\n",
    "    espacio = int(50 - len(barra)) * \"⬛️\"\n",
    "\n",
    "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a55da",
   "metadata": {},
   "source": [
    "# MEJORAR LA CARGA DESDE UN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4e63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test/truth_test.csv\"\u001b[0m\n",
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train/truth_train.csv\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generar_csv(path_lectura, path_guardado):\n",
    "    # Obtener la ruta absoluta del directorio actual\n",
    "    # Leerlo y convertirlo en un DataFrame .csv para manipularlo mejor\n",
    "    with open(path_lectura, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    data = [line.strip().split(\":::\") for line in lines]\n",
    "\n",
    "\n",
    "    df = pl.DataFrame(data, schema=[\"id\", \"genero\", \"pais\"], orient=\"row\")\n",
    "\n",
    "\n",
    "    # Guardar el DataFrame como un archivo .csv\n",
    "    df.write_csv(path_guardado)\n",
    "    return True\n",
    "\n",
    "# ==================== Configuración de rutas =====================\n",
    "\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "# ===================== Creación de CSV de datos de prueba =====================\n",
    "ruta_lectura_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth.txt\")\n",
    "ruta_guardado_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth_test.csv\")\n",
    "\n",
    "creado = generar_csv(ruta_lectura_prueba, ruta_guardado_prueba)\n",
    "\n",
    "if creado:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_prueba}\"')\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# ===================== Creación de CSV de datos de entrenamiento ==============\n",
    "ruta_lectura_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth.txt\")\n",
    "ruta_guardado_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth_train.csv\")\n",
    "\n",
    "creado_entrenamiento = generar_csv(ruta_lectura_entrenamiento, ruta_guardado_entrenamiento)\n",
    "if creado_entrenamiento:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_entrenamiento}\"')\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e521660",
   "metadata": {},
   "source": [
    "# CARGAR LOS INDICES DESDE TRUTH DESDE EL CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e4c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬────────┬──────────┐\n",
      "│ id                              ┆ genero ┆ pais     │\n",
      "│ ---                             ┆ ---    ┆ ---      │\n",
      "│ str                             ┆ str    ┆ str      │\n",
      "╞═════════════════════════════════╪════════╪══════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ female ┆ colombia │\n",
      "│ 4639c055f34ca1f944d0137a5aeb79… ┆ female ┆ colombia │\n",
      "│ 92ffa98bade702b86417b118e8aca3… ┆ female ┆ colombia │\n",
      "│ 4560c6567afcccef265f048ed117d0… ┆ female ┆ colombia │\n",
      "│ 393866dfaa80d414c9896cf8723932… ┆ female ┆ colombia │\n",
      "└─────────────────────────────────┴────────┴──────────┘\n",
      "CARGA XML: |⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️⬛️|   0.14%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARGA XML: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Carga de archivos XML completada.\n",
      "\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets crudos cargados: 4200 mostrando 1 registro:\n",
      " id:74bcc9b0882c8440716ff370494aea09\n",
      " pais:colombia\n",
      " genero:female\n",
      " xml_text:\n",
      "<author lang=\"es\">\n",
      "\t<documents>\n",
      "\t\t<document><![CDA...\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def cargar_xml(id_archivo, train=True) -> str:\n",
    "    ruta_base = os.path.join(base_path, \"data/author_profiling/es_test\" if not train else \"data/author_profiling/es_train\")\n",
    "    ruta_archivo = os.path.join(ruta_base, f\"{id_archivo}.xml\")\n",
    "    # Logger.info(f\"ruta archivo: {ruta_archivo}\")\n",
    "    \n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as file:\n",
    "        xml_text = file.read()\n",
    "\n",
    "    return xml_text\n",
    "\n",
    "# ===================== Cargar CSV de datos  ==================\n",
    " \n",
    "df_indices = pl.read_csv(ruta_guardado_entrenamiento)\n",
    "print(df_indices.head())\n",
    "\n",
    "cantidad_registros = len(df_indices)\n",
    "\n",
    "registros_crudos = [(\"id_user\",\"xml_doc\",\"pais\",\"genero\") for i in range(cantidad_registros)]\n",
    "\n",
    "\n",
    "for i, reg in enumerate(registros_crudos):\n",
    "    id_archivo = df_indices['id'][i]\n",
    "    id_user = id_archivo\n",
    "    pais = df_indices['pais'][i]\n",
    "    genero = df_indices['genero'][i]\n",
    "\n",
    "    xml_text: str = cargar_xml(id_archivo)\n",
    "    registros_crudos[i] = (id_user, xml_text, pais, genero)\n",
    "\n",
    "    print_bar(i, cantidad_registros, contexto=\"CARGA XML\")\n",
    "\n",
    "print()\n",
    "Logger.info(\"Carga de archivos XML completada.\\n\")\n",
    "id_user, xml_text, pais, genero = registros_crudos[0]\n",
    "\n",
    "\n",
    "Logger.debug(f\"Total de tweets crudos cargados: {len(registros_crudos)} mostrando 1 registro:\\n id:{id_user}\\n pais:{pais}\\n genero:{genero}\\n xml_text:\\n{xml_text[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4685f1",
   "metadata": {},
   "source": [
    "# PROCESAMOS LOS TEXTOS XML Y LOS DEJAMOS EN UN DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0287180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso registros limpiados: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Guardando archivo\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets procesados: 419998 mostrando 5 primeros registros:\n",
      " shape: (5, 5)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬──────────┬────────┬─────────┐\n",
      "│ id_user                         ┆ tweet_crudo                     ┆ pais     ┆ genero ┆ idioma  │\n",
      "│ ---                             ┆ ---                             ┆ ---      ┆ ---    ┆ ---     │\n",
      "│ str                             ┆ str                             ┆ str      ┆ str    ┆ str     │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪══════════╪════════╪═════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tiene que valer la pena que es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tintas chinas, si ven ésto, es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ \"Maestro no le abrió!\" -Ay qué… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Lo bueno de no estar enamorado… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Recordaré este día como el día… ┆ colombia ┆ female ┆ Español │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴──────────┴────────┴─────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "diccionario_idioma = {\n",
    "    'es': 'Español', \n",
    "    'en': 'Inglés', \n",
    "    'fr': 'Francés', \n",
    "    'de': 'Alemán', \n",
    "    'it': 'Italiano', \n",
    "    'nl': 'Neerlandés'\n",
    "}\n",
    "\n",
    "def limpiar_xml(texto_xml:str):\n",
    "    soup = BeautifulSoup(texto_xml, 'lxml-xml')   \n",
    "    lang = str(soup.author.get('lang'))\n",
    "    idioma = diccionario_idioma[lang] if lang in diccionario_idioma else lang\n",
    "    \n",
    "    documentos = soup.find_all('document') # Obtenemos todsa las etiquetas <document>\n",
    "    tweets = [doc.get_text(separator=\" \", strip=True) for doc in documentos] # Extraemos el texto de cada documento\n",
    "    \n",
    "    return tweets, idioma\n",
    "\n",
    "\n",
    "\n",
    "# ===================== Procesamiento de limpieza del XML =====================\n",
    "cantidad_registros = len(registros_crudos)\n",
    "\n",
    "registros_procesados = []\n",
    "\n",
    "for i, (id_user, doc_crudo, pais, genero) in enumerate(registros_crudos):\n",
    "    lista_tweets_por_usuario, idioma = limpiar_xml(doc_crudo)\n",
    "    \n",
    "    for tweet in lista_tweets_por_usuario:\n",
    "        registros_procesados.append({\n",
    "            \"id_user\": id_user,\n",
    "            \"tweet_crudo\": tweet,\n",
    "            \"pais\": pais,\n",
    "            \"genero\": genero,\n",
    "            \"idioma\": idioma\n",
    "        })\n",
    "    print_bar(i, cantidad_registros, contexto=\"Progreso registros limpiados\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ===================== Creamos dataset persistente =====================\n",
    "\n",
    "df_registros = pl.DataFrame(registros_procesados)\n",
    "df_registros.write_parquet(\"data/author_profiling/registros_procesados.parquet\")\n",
    "Logger.info(\"Guardando archivo\")\n",
    "\n",
    "lf_registros = df_registros.lazy()\n",
    "\n",
    "# ===================== Muestra =====================\n",
    "\n",
    "\n",
    "muestra = lf_registros.limit(5).collect()\n",
    "Logger.debug(f\"Total de tweets procesados: {len(registros_procesados)} mostrando 5 primeros registros:\\n {muestra}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45b49c",
   "metadata": {},
   "source": [
    "\n",
    "# Capa de dataclass de utilidd para normalización y limpieza de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15eed5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import unicodedata\n",
    "# ===================== Configuración de Limpieza de tweets =====================\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfigLimpieza:\n",
    "    normalizar_unicode: bool = True\n",
    "    a_minusculas: bool = True\n",
    "    quitar_urls: bool = True\n",
    "    quitar_menciones: bool = True\n",
    "    quitar_hashtags: bool = False \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    URL = re.compile(r'https?://\\S+', re.I)\n",
    "    MENCION = re.compile(r'@\\w+')\n",
    "    HASHTAG = re.compile(r'#\\w+')\n",
    "    ESPACIO = re.compile(r'\\s+')\n",
    "    TT = TweetTokenizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def limpiar(texto: str) -> str:\n",
    "        cfg = ConfigLimpieza()\n",
    "        if cfg.normalizar_unicode:\n",
    "            texto = unicodedata.normalize(\"NFKC\", texto)\n",
    "        if cfg.a_minusculas:\n",
    "            texto = texto.lower()\n",
    "        if cfg.quitar_urls:\n",
    "            texto = Tweet.URL.sub(\" \", texto)\n",
    "        if cfg.quitar_menciones:\n",
    "            texto = Tweet.MENCION.sub(\" \", texto)\n",
    "        if cfg.quitar_hashtags:\n",
    "            texto = Tweet.HASHTAG.sub(\" \", texto)\n",
    "\n",
    "        texto_limpio = Tweet.ESPACIO.sub(\" \", texto).strip()\n",
    "        return texto_limpio\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizar(texto: str) -> list[str]:\n",
    "        return [t for t in Tweet.TT.tokenize(texto)]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eca741",
   "metadata": {},
   "source": [
    "# PIPELINE DE LIMPIEZA Y/O TOKENIZACIÓN DE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3714c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] PIPELINE: LIMPIEZA/ TOKENIZACIÓN\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe limpio:\n",
      " shape: (2, 6)\n",
      "┌──────────────────────┬──────────────────────┬──────────┬────────┬─────────┬──────────────────────┐\n",
      "│ id_user              ┆ tweet_crudo          ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio         │\n",
      "│ ---                  ┆ ---                  ┆ ---      ┆ ---    ┆ ---     ┆ ---                  │\n",
      "│ str                  ┆ str                  ┆ str      ┆ str    ┆ str     ┆ str                  │\n",
      "╞══════════════════════╪══════════════════════╪══════════╪════════╪═════════╪══════════════════════╡\n",
      "│ 74bcc9b0882c8440716f ┆ Tiene que valer la   ┆ colombia ┆ female ┆ Español ┆ tiene que valer la   │\n",
      "│ f370494aea…          ┆ pena que es…         ┆          ┆        ┆         ┆ pena que es…         │\n",
      "│ 74bcc9b0882c8440716f ┆ Tintas chinas, si    ┆ colombia ┆ female ┆ Español ┆ tintas chinas, si    │\n",
      "│ f370494aea…          ┆ ven ésto, es…        ┆          ┆        ┆         ┆ ven ésto, es…        │\n",
      "└──────────────────────┴──────────────────────┴──────────┴────────┴─────────┴──────────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet limpio: tiene que valer la pena que esté despierta a esta hora #hoylosgrammycon40\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado:\n",
      " shape: (2, 7)\n",
      "┌─────────────────┬────────────────┬──────────┬────────┬─────────┬────────────────┬────────────────┐\n",
      "│ id_user         ┆ tweet_crudo    ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio   ┆ tweet_tokeniza │\n",
      "│ ---             ┆ ---            ┆ ---      ┆ ---    ┆ ---     ┆ ---            ┆ do             │\n",
      "│ str             ┆ str            ┆ str      ┆ str    ┆ str     ┆ str            ┆ ---            │\n",
      "│                 ┆                ┆          ┆        ┆         ┆                ┆ list[str]      │\n",
      "╞═════════════════╪════════════════╪══════════╪════════╪═════════╪════════════════╪════════════════╡\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ [\"tiene\",      │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆ \"que\", …       │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆ \"#hoylosgra…   │\n",
      "│ 74bcc9b0882c844 ┆ Tintas chinas, ┆ colombia ┆ female ┆ Español ┆ tintas chinas, ┆ [\"tintas\",     │\n",
      "│ 0716ff370494aea ┆ si ven ésto,   ┆          ┆        ┆         ┆ si ven ésto,   ┆ \"chinas\", …    │\n",
      "│ …               ┆ es…            ┆          ┆        ┆         ┆ es…            ┆ \"xoxo\"]        │\n",
      "└─────────────────┴────────────────┴──────────┴────────┴─────────┴────────────────┴────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet tokenizado: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tiene\"\n",
      "\t\"que\"\n",
      "\t\"valer\"\n",
      "\t\"la\"\n",
      "\t\"pena\"\n",
      "\t…\n",
      "\t\"despierta\"\n",
      "\t\"a\"\n",
      "\t\"esta\"\n",
      "\t\"hora\"\n",
      "\t\"#hoylosgrammycon40\"\n",
      "]\u001b[0m\n",
      "\u001b[94m[INFO] Token: tiene\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: valer\u001b[0m\n",
      "\u001b[94m[INFO] Token: la\u001b[0m\n",
      "\u001b[94m[INFO] Token: pena\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: esté\u001b[0m\n",
      "\u001b[94m[INFO] Token: despierta\u001b[0m\n",
      "\u001b[94m[INFO] Token: a\u001b[0m\n",
      "\u001b[94m[INFO] Token: esta\u001b[0m\n",
      "\u001b[94m[INFO] Token: hora\u001b[0m\n",
      "\u001b[94m[INFO] Token: #hoylosgrammycon40\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== TRANSFORMACIONES EN PIPELINE =====================\n",
    "\"\"\"\n",
    "Recordemos que los pipelines de polars si es un lazyframe no se ejecutan hasta que se les pida con .collect() (Docoumentación)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lf_registros_crudos = lf_registros\n",
    "\n",
    "\n",
    "Logger.info(\"PIPELINE: LIMPIEZA/ TOKENIZACIÓN\")\n",
    "lf_limpio = (\n",
    "    lf_registros_crudos.with_columns(\n",
    "        pl.col(\"tweet_crudo\")\n",
    "          .map_elements(Tweet.limpiar)\n",
    "          .alias(\"texto_limpio\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe limpio:\\n {lf_limpio.limit(2).collect()}\")\n",
    "\n",
    "primer_tweet_limpio = (\n",
    "    lf_limpio\n",
    "    .select('texto_limpio')\n",
    "    .limit(1)\n",
    "    .collect()\n",
    "    .to_series()\n",
    "    .to_list()[0]\n",
    ")\n",
    "\n",
    "Logger.info(f\"Primer tweet limpio: {primer_tweet_limpio}\")\n",
    "        \n",
    "lf_tokens = (\n",
    "    lf_limpio.with_columns(\n",
    "        pl.col(\"texto_limpio\")\n",
    "          .map_elements(\n",
    "              Tweet.tokenizar, \n",
    "              return_dtype=pl.List(pl.Utf8), \n",
    "              skip_nulls=True\n",
    "            )\n",
    "          .alias(\"tweet_tokenizado\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado:\\n {lf_tokens.limit(2).collect()}\")\n",
    "\n",
    "\n",
    "# ======================== Metodo para extraer el primer tweet tokenizado con polars =====================\n",
    "primer_tweet_tokenizado = (\n",
    "    lf_tokens\n",
    "    .select(\"tweet_tokenizado\") # Seleccionamos la columna que nos interesa\n",
    "    .limit(1) \n",
    "    .collect()\n",
    "    .item()  # Sirve para extraer el valor cuando el DataFrame es 1x1\n",
    ")\n",
    "# ========================================================================================================\n",
    "\n",
    "Logger.info(f\"Primer tweet tokenizado: {primer_tweet_tokenizado}\")\n",
    "\n",
    "for token in primer_tweet_tokenizado:\n",
    "    Logger.info(f\"Token: {token}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a6e4b",
   "metadata": {},
   "source": [
    "# VAMOS A GENERAR UN VOCABULARIO GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a514f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] CONSTRUCCIÓN DE VOCABULARIO GLOBAL\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado y aplanado:\n",
      " shape: (5_908_700, 2)\n",
      "┌─────────────────────────────────┬──────────────────┐\n",
      "│ id_user                         ┆ tokens_del_tweet │\n",
      "│ ---                             ┆ ---              │\n",
      "│ str                             ┆ str              │\n",
      "╞═════════════════════════════════╪══════════════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ tiene            │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ que              │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ valer            │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ la               │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ pena             │\n",
      "│ …                               ┆ …                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ recupere         │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ ,                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ saludos          │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ !                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ !                │\n",
      "└─────────────────────────────────┴──────────────────┘\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando vocabulario:\n",
      " shape: (205_231, 3)\n",
      "┌──────────────────┬───────────────────┬───────────────────┐\n",
      "│ tokens_del_tweet ┆ frecuencia_global ┆ documentos_unicos │\n",
      "│ ---              ┆ ---               ┆ ---               │\n",
      "│ str              ┆ u32               ┆ u32               │\n",
      "╞══════════════════╪═══════════════════╪═══════════════════╡\n",
      "│ prohíban         ┆ 11                ┆ 7                 │\n",
      "│ mariconazo       ┆ 1                 ┆ 1                 │\n",
      "│ #navidad40       ┆ 6                 ┆ 1                 │\n",
      "│ visitándote      ┆ 1                 ┆ 1                 │\n",
      "│ designaciones    ┆ 3                 ┆ 3                 │\n",
      "│ …                ┆ …                 ┆ …                 │\n",
      "│ matuk            ┆ 1                 ┆ 1                 │\n",
      "│ oas              ┆ 17                ┆ 15                │\n",
      "│ floppynoegomez   ┆ 1                 ┆ 1                 │\n",
      "│ petión           ┆ 1                 ┆ 1                 │\n",
      "│ monómeros        ┆ 1                 ┆ 1                 │\n",
      "└──────────────────┴───────────────────┴───────────────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== VOCABULARIO GLOBAL =====================\n",
    "\n",
    "Logger.info(\"CONSTRUCCIÓN DE VOCABULARIO GLOBAL\")\n",
    "\n",
    "\n",
    "lf_tokens_aplanados = (\n",
    "    lf_tokens\n",
    "    .select(\n",
    "        'id_user', \n",
    "        pl.col('tweet_tokenizado')\n",
    "    )\n",
    "    .explode('tweet_tokenizado')\n",
    "    .rename({'tweet_tokenizado':'tokens_del_tweet'})\n",
    ")\n",
    "\n",
    "\n",
    "lf_vocabulario = (\n",
    "    lf_tokens_aplanados\n",
    "    .group_by('tokens_del_tweet')\n",
    "    .agg([\n",
    "        pl.len().alias('frecuencia_global'),              # Conteo total de apariciones de cada token\n",
    "        pl.n_unique('id_user').alias('documentos_unicos') # En cuántos documentos aparece ese token\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado y aplanado:\\n {lf_tokens_aplanados.collect()}\")\n",
    "Logger.debug(f\"Mostrando vocabulario:\\n {lf_vocabulario.collect()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fda952",
   "metadata": {},
   "source": [
    "# GENERAMOS LAS ETIQUETAS PARA PODER AVENTARNOS LO DEL ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7325dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Total de documentos (usuarios): 4200\u001b[0m\n",
      "\u001b[92m[DEBUG] Muestra de etiquetas (y): shape: (4_200,)\n",
      "Series: 'pais' [str]\n",
      "[\n",
      "\t\"spain\"\n",
      "\t\"peru\"\n",
      "\t\"venezuela\"\n",
      "\t\"peru\"\n",
      "\t\"spain\"\n",
      "\t…\n",
      "\t\"colombia\"\n",
      "\t\"chile\"\n",
      "\t\"peru\"\n",
      "\t\"peru\"\n",
      "\t\"peru\"\n",
      "]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Parámetros de reducción de dimensión --------------------\n",
    "lf_documentos_unicos = (\n",
    "    lf_limpio\n",
    "    .select(['id_user', 'pais', 'genero'])\n",
    "    .unique(maintain_order=True)\n",
    "    .sort('id_user')\n",
    "    .with_row_index(name='indice_doc', offset=0)\n",
    ")\n",
    "\n",
    "\n",
    "Logger.info(f\"Total de documentos (usuarios): {lf_documentos_unicos.count().collect()[0,0]}\")\n",
    "\n",
    "\n",
    "# 2) Etiquetas alineadas (y_train)\n",
    "lf_etiquetas = (\n",
    "    lf_documentos_unicos\n",
    "    .select('pais', 'genero')\n",
    "    .collect()\n",
    "    .to_series()\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Muestra de etiquetas (y): {lf_etiquetas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312ff0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[DEBUG] Mostrando vocabulario reducido:\n",
      " shape: (38_575, 3)\n",
      "┌──────────────────┬───────────────────┬───────────────────┐\n",
      "│ tokens_del_tweet ┆ frecuencia_global ┆ documentos_unicos │\n",
      "│ ---              ┆ ---               ┆ ---               │\n",
      "│ str              ┆ u32               ┆ u32               │\n",
      "╞══════════════════╪═══════════════════╪═══════════════════╡\n",
      "│ de               ┆ 215391            ┆ 4190              │\n",
      "│ ,                ┆ 161864            ┆ 4125              │\n",
      "│ .                ┆ 156802            ┆ 4005              │\n",
      "│ que              ┆ 149950            ┆ 4155              │\n",
      "│ la               ┆ 135889            ┆ 4183              │\n",
      "│ …                ┆ …                 ┆ …                 │\n",
      "│ posicionado      ┆ 5                 ┆ 5                 │\n",
      "│ cerrarse         ┆ 5                 ┆ 5                 │\n",
      "│ posan            ┆ 5                 ┆ 5                 │\n",
      "│ valieron         ┆ 5                 ┆ 5                 │\n",
      "│ rayar            ┆ 5                 ┆ 5                 │\n",
      "└──────────────────┴───────────────────┴───────────────────┘\u001b[0m\n",
      "\u001b[92m[DEBUG] Vocabulario reducido numoy: [['de'], [','], ['.'], ['que'], ['la']]\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando diccionario de índices:\n",
      " shape: (38_575, 2)\n",
      "┌────────────┬──────────────────┐\n",
      "│ indice_col ┆ tokens_del_tweet │\n",
      "│ ---        ┆ ---              │\n",
      "│ u32        ┆ str              │\n",
      "╞════════════╪══════════════════╡\n",
      "│ 0          ┆ de               │\n",
      "│ 1          ┆ ,                │\n",
      "│ 2          ┆ .                │\n",
      "│ 3          ┆ que              │\n",
      "│ 4          ┆ la               │\n",
      "│ …          ┆ …                │\n",
      "│ 38570      ┆ microcefalia     │\n",
      "│ 38571      ┆ presenté         │\n",
      "│ 38572      ┆ #mascotas        │\n",
      "│ 38573      ┆ 9:15             │\n",
      "│ 38574      ┆ medido           │\n",
      "└────────────┴──────────────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 3) Vocabulario reducido → diccionario de índices (token → columna)\n",
    "\n",
    "MAX_TERMINOS = 50_000  \n",
    "MIN_DOCS = 5\n",
    "\n",
    "lf_vocabulario_reducido = (\n",
    "    lf_vocabulario\n",
    "    .filter(\n",
    "        pl.col('documentos_unicos') >= MIN_DOCS)      # filtra tokens raros\n",
    "    .sort('frecuencia_global', descending=True)\n",
    "    \n",
    "    \n",
    "    #.limit(MAX_TERMINOS)                                 \n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando vocabulario reducido:\\n {lf_vocabulario_reducido.collect()}\")  \n",
    "\n",
    "\n",
    "lf_vocabulario_reducido_numpy =(\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "\n",
    "Logger.debug(f\"Vocabulario reducido numoy: {lf_vocabulario_reducido_numpy[:5]}\")\n",
    "\n",
    "\n",
    "tokens_ordenados = (\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Usado por el profe\n",
    "# dicc_indices = {token: i for i, token in enumerate(tokens_ordenados)}\n",
    "\n",
    "# for token, i in list(dicc_indices.items())[:10]:\n",
    "#     Logger.info(f\"Token: {token} → Índice columna: {i}\")\n",
    "\n",
    "\n",
    "\n",
    "# Lazy_frame de un dict_indices\n",
    "\n",
    "lf_dict_indices = (\n",
    "    tokens_ordenados\n",
    "    .with_row_index(name='indice_col', offset=0)\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando diccionario de índices:\\n {lf_dict_indices.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33da903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "VERSION DEL PROFE\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "# def built_bow_tr_profe_version(tweets, Vocabulario, dict_indices):\n",
    "#     # Tweets: lista de tweets tokenizados | LazyFrame ya coleccionado\n",
    "#     # Vocabulario: lista de tuplas (token, frecuencia) | LazyFrame ya coleccionado\n",
    "#     # dict_indices: diccionario índice → token columna | lazyFrame ya coleccionado\n",
    "\n",
    "#     BOW_FRECUENCIA = np.zeros((len(tweets), len(Vocabulario)), dtype=int)\n",
    "#     BOW_BINARIO = np.zeros((len(tweets), len(Vocabulario)), dtype=int)\n",
    "\n",
    "#     contador = 0\n",
    "#     for tweet in tweets:\n",
    "#         for palabra in tweet:\n",
    "#             if palabra in dict_indices:\n",
    "#                 Logger.debug(f\"Palabra encontrada: {palabra} en tweet {contador}\")\n",
    "#                 BOW_FRECUENCIA[contador, dict_indices[palabra]] = tweet[palabra] # FRECUENCIA DE LA PALABRA EN VEZ DE HACERLO BINARIO\n",
    "#                 BOW_BINARIO[contador, dict_indices[palabra]] = 1 # HACIENDOLO BINARIO\n",
    "#         contador += 1\n",
    "\n",
    "#     return BOW_FRECUENCIA, BOW_BINARIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def built_bow_tr_profe_version(tweets_tokens, vocab_tokens, dict_indices):\n",
    "    # Tweets: lista de tweets tokenizados | LazyFrame ya coleccionado\n",
    "    # Vocabulario: lista de tuplas (token, frecuencia) | LazyFrame ya coleccionado\n",
    "    # dict_indices: diccionario índice → token columna | lazyFrame ya coleccionado\n",
    "    \n",
    "    V = len(vocab_tokens)\n",
    "    n = len(tweets_tokens)\n",
    "    \n",
    "    BOW_FRECUENCIA = np.zeros((n, V), dtype=int)\n",
    "    BOW_BINARIO    = np.zeros((n, V), dtype=int)\n",
    "    \n",
    "    for i, tokens in enumerate(tweets_tokens):\n",
    "        frec = Counter(tokens)              # {'de': 3, 'la': 1, ...}\n",
    "        for token, frecuencia in frec.items():\n",
    "            columna_vocab = dict_indices.get(token)       # columna del vocabulario\n",
    "            if columna_vocab is not None:                    # solo si el token está en el vocab reducido\n",
    "                BOW_FRECUENCIA[i, columna_vocab] = frecuencia\n",
    "                BOW_BINARIO[i, columna_vocab] = 1\n",
    "    return BOW_FRECUENCIA, BOW_BINARIO\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2796f",
   "metadata": {},
   "source": [
    "# CONSTRUCCIÓN DE BOW (BAG OF WORDS) - MATRIZ DE TÉRMINO-RECURSO (TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55238e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Vocabulario preparado: 38,575 términos\u001b[0m\n",
      "\u001b[94m[INFO] dict_indices listo: 38,575 entradas\u001b[0m\n",
      "\u001b[94m[INFO] Tweets preparados: 10,000 documentos\u001b[0m\n",
      "Construyendo BOW: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\u001b[94m[INFO] BOW construido.\u001b[0m\n",
      "\u001b[94m[INFO] BOW frecuencia: (10000, 38575) | nnz=115,921\u001b[0m\n",
      "\u001b[94m[INFO] BOW binario:    (10000, 38575)    | nnz=115,921\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# --- A) Preparar insumos en los tipos correctos ------------------------------\n",
    "\n",
    "# 1) vocabulario: aplanar si viene como [['de'], [','], ...]\n",
    "vocab_tokens = (\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    "    .collect()\n",
    "    .get_column('tokens_del_tweet')\n",
    "    .to_list()\n",
    ")\n",
    "# Si insistieras en usar tu *_numpy:\n",
    "# vocab_tokens = [x[0] for x in lf_vocabulario_reducido_numpy]\n",
    "\n",
    "Logger.info(f\"Vocabulario preparado: {len(vocab_tokens):,} términos\")\n",
    "\n",
    "# 2) dict_indices: convertir DataFrame -> dict {token: indice}\n",
    "df_dict = lf_dict_indices.collect()  # columnas: ['indice_col','tokens_del_tweet']\n",
    "dict_indices = dict(zip(\n",
    "    df_dict.get_column('tokens_del_tweet').to_list(),\n",
    "    map(int, df_dict.get_column('indice_col').to_list())\n",
    "))\n",
    "Logger.info(f\"dict_indices listo: {len(dict_indices):,} entradas\")\n",
    "\n",
    "# 3) tweets: lista de listas de tokens\n",
    "tweets = (\n",
    "    lf_tokens\n",
    "    .select(\"tweet_tokenizado\")\n",
    "    .limit(10000)  # solo si quieres probar con un subset\n",
    "    .collect()\n",
    "    .get_column(\"tweet_tokenizado\")\n",
    "    .to_list()\n",
    ")\n",
    "Logger.info(f\"Tweets preparados: {len(tweets):,} documentos\")\n",
    "\n",
    "\n",
    "# --- B) Función del profe adaptada a listas de tokens + dict_indices ---------\n",
    "\n",
    "def built_bow_tr_profe_version(tweets_tokens, vocab_tokens, dict_indices):\n",
    "    V = len(vocab_tokens)\n",
    "    n = len(tweets_tokens)\n",
    "    BOW_FRECUENCIA = np.zeros((n, V), dtype=int)\n",
    "    BOW_BINARIO    = np.zeros((n, V), dtype=int)\n",
    "\n",
    "    for i, toks in enumerate(tweets_tokens):\n",
    "        if not toks:\n",
    "            continue\n",
    "        frec = Counter(toks)  # {'de': 3, 'la': 1, ...}\n",
    "        for token, cnt in frec.items():\n",
    "            j = dict_indices.get(token)   # columna del vocabulario\n",
    "            if j is not None:\n",
    "                BOW_FRECUENCIA[i, j] = cnt\n",
    "                BOW_BINARIO[i, j] = 1\n",
    "        print_bar(i, n, contexto=\"Construyendo BOW\")\n",
    "    return BOW_FRECUENCIA, BOW_BINARIO\n",
    "\n",
    "\n",
    "# --- C) Construir BOW con los insumos ya limpios -----------------------------\n",
    "print()\n",
    "BOW_FRECUENCIA, BOW_BINARIO = built_bow_tr_profe_version(tweets, vocab_tokens, dict_indices)\n",
    "Logger.info(\"BOW construido.\")\n",
    "\n",
    "# ========== Chequeo rápido =========\n",
    "nnz_freq = int((BOW_FRECUENCIA > 0).sum())\n",
    "nnz_bin  = int(BOW_BINARIO.sum())\n",
    "Logger.info(f\"BOW frecuencia: {BOW_FRECUENCIA.shape} | nnz={nnz_freq:,}\")\n",
    "Logger.info(f\"BOW binario:    {BOW_BINARIO.shape}    | nnz={nnz_bin:,}\")\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ad782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] DOR frecuencia: (38575, 10000) | DOR binaria: (38575, 10000)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROFE VERSION\n",
    "\"\"\"\n",
    "def compute_dor_profe(TR_densa: np.ndarray) -> np.ndarray:\n",
    "    DTR = np.zeros((TR_densa.shape[1], TR_densa.shape[0]), dtype=float)\n",
    "    tam_v = TR_densa.shape[1]\n",
    "    for i, doc in enumerate(TR_densa):\n",
    "        pos_no_cero = np.nonzero(doc)[0]\n",
    "        tam_vocab_doc = len(pos_no_cero)\n",
    "        for termino in pos_no_cero:\n",
    "            DTR[termino, i] = doc[termino] * np.log(tam_v / max(1, tam_vocab_doc))\n",
    "    return DTR\n",
    "\n",
    "\n",
    "TR_frecuencia_densa = BOW_FRECUENCIA\n",
    "DTR_frecuencia = compute_dor_profe(TR_frecuencia_densa)\n",
    "\n",
    "TR_binaria_densa    = BOW_BINARIO\n",
    "DTR_binaria    = compute_dor_profe(TR_binaria_densa)\n",
    "\n",
    "\n",
    "\n",
    "Logger.info(f\"DOR frecuencia: {DTR_frecuencia.shape} | DOR binaria: {DTR_binaria.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0118c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] [CSR] TR_frecuencia: shape=(10000, 38575), nnz=115,921\u001b[0m\n",
      "\u001b[94m[INFO] [CSR] TR_binaria:    shape=(10000, 38575), nnz=115,921\u001b[0m\n",
      "\u001b[94m[INFO] [L2] TR normalizado (frecuencia y binario) listo.\u001b[0m\n",
      "\u001b[94m[INFO] [L2] DOR_frecuencia normalizado.\u001b[0m\n",
      "\u001b[94m[INFO] [L2] DOR_binaria normalizado.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10000, 4200]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 4) Split 80/20 con TR (elige qué representación usar)\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m Xf_tr, Xf_va, y_tr, y_va   = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTR_frecuencia_csr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m Xb_tr, Xb_va, _, _         = train_test_split(TR_binaria_csr,       y_train, test_size=\u001b[32m0.2\u001b[39m, stratify=y_train, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     35\u001b[39m Xf_trL2, Xf_vaL2, _, _     = train_test_split(TR_frecuencia_L2_csr, y_train, test_size=\u001b[32m0.2\u001b[39m, stratify=y_train, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2916\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_arrays == \u001b[32m0\u001b[39m:\n\u001b[32m   2914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one array required as input\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2916\u001b[39m arrays = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2919\u001b[39m n_train, n_test = _validate_shuffle_split(\n\u001b[32m   2920\u001b[39m     n_samples, test_size, train_size, default_test_size=\u001b[32m0.25\u001b[39m\n\u001b[32m   2921\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [10000, 4200]"
     ]
    }
   ],
   "source": [
    "# ================== NORMALIZACIÓN L2 (TR y DOR) ==================\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Pasar tus BOW densos a CSR (rápido y memoria-friendly)\n",
    "TR_frecuencia_csr = csr_matrix(BOW_FRECUENCIA, dtype=np.float32)\n",
    "TR_binaria_csr    = csr_matrix(BOW_BINARIO,    dtype=np.float32)\n",
    "y_train = lf_etiquetas.to_list()\n",
    "\n",
    "Logger.info(f\"[CSR] TR_frecuencia: shape={TR_frecuencia_csr.shape}, nnz={TR_frecuencia_csr.nnz:,}\")\n",
    "Logger.info(f\"[CSR] TR_binaria:    shape={TR_binaria_csr.shape}, nnz={TR_binaria_csr.nnz:,}\")\n",
    "\n",
    "# 2) Normalizar L2 por documento (filas) en TR\n",
    "TR_frecuencia_L2_csr = normalize(TR_frecuencia_csr, norm='l2', copy=True)\n",
    "TR_binaria_L2_csr    = normalize(TR_binaria_csr,    norm='l2', copy=True)\n",
    "\n",
    "Logger.info(\"[L2] TR normalizado (frecuencia y binario) listo.\")\n",
    "\n",
    "# 3) (OPCIONAL) Normalizar L2 también los DOR del profe\n",
    "#    DOR es término×documento → normalizamos por documento (columnas),\n",
    "#    así que transponemos, normalizamos filas y des-transponemos.\n",
    "#    Solo si ya creaste DTR_frecuencia / DTR_binaria (matrices densas).\n",
    "if 'DTR_frecuencia' in globals():\n",
    "    DOR_frecuencia_L2 = normalize(DTR_frecuencia.T, norm='l2', copy=True).T\n",
    "    Logger.info(\"[L2] DOR_frecuencia normalizado.\")\n",
    "if 'DTR_binaria' in globals():\n",
    "    DOR_binaria_L2 = normalize(DTR_binaria.T, norm='l2', copy=True).T\n",
    "    Logger.info(\"[L2] DOR_binaria normalizado.\")\n",
    "\n",
    "# 4) Split 80/20 con TR (elige qué representación usar)\n",
    "\n",
    "Xf_tr, Xf_va, y_tr, y_va   = train_test_split(TR_frecuencia_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_tr, Xb_va, _, _         = train_test_split(TR_binaria_csr,       y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xf_trL2, Xf_vaL2, _, _     = train_test_split(TR_frecuencia_L2_csr, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_trL2, Xb_vaL2, _, _     = train_test_split(TR_binaria_L2_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "Logger.info(f\"Shapes → Frecuencia tr={Xf_tr.shape} va={Xf_va.shape} | Binaria tr={Xb_tr.shape} va={Xb_va.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
