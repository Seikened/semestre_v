{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da3245f",
   "metadata": {},
   "source": [
    "# FERNANDO LEON FRANCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae49b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from colorstreak import Logger\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b42045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
    "    porcentaje = (i + 1) / cantidad_registros * 100\n",
    "    # Con emojis\n",
    "    barra = int(50 * (i + 1) / cantidad_registros) * \"🟩\"\n",
    "    espacio = int(50 - len(barra)) * \"⬛️\"\n",
    "\n",
    "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a55da",
   "metadata": {},
   "source": [
    "# MEJORAR LA CARGA DESDE UN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4e63c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test/truth_test.csv\"\u001b[0m\n",
      "\u001b[94m[INFO] Archivo CSV creado en: \"/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train/truth_train.csv\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generar_csv(path_lectura, path_guardado):\n",
    "    # Obtener la ruta absoluta del directorio actual\n",
    "    # Leerlo y convertirlo en un DataFrame .csv para manipularlo mejor\n",
    "    with open(path_lectura, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    data = [line.strip().split(\":::\") for line in lines]\n",
    "\n",
    "\n",
    "    df = pl.DataFrame(data, schema=[\"id\", \"genero\", \"pais\"], orient=\"row\")\n",
    "\n",
    "\n",
    "    # Guardar el DataFrame como un archivo .csv\n",
    "    df.write_csv(path_guardado)\n",
    "    return True\n",
    "\n",
    "# ==================== Configuración de rutas =====================\n",
    "\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "# ===================== Creación de CSV de datos de prueba =====================\n",
    "ruta_lectura_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth.txt\")\n",
    "ruta_guardado_prueba = os.path.join(base_path, \"data/author_profiling/es_test/truth_test.csv\")\n",
    "\n",
    "creado = generar_csv(ruta_lectura_prueba, ruta_guardado_prueba)\n",
    "\n",
    "if creado:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_prueba}\"')\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# ===================== Creación de CSV de datos de entrenamiento ==============\n",
    "ruta_lectura_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth.txt\")\n",
    "ruta_guardado_entrenamiento = os.path.join(base_path, \"data/author_profiling/es_train/truth_train.csv\")\n",
    "\n",
    "creado_entrenamiento = generar_csv(ruta_lectura_entrenamiento, ruta_guardado_entrenamiento)\n",
    "if creado_entrenamiento:\n",
    "    Logger.info(f'Archivo CSV creado en: \"{ruta_guardado_entrenamiento}\"')\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e521660",
   "metadata": {},
   "source": [
    "# CARGAR LOS INDICES DESDE TRUTH DESDE EL CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e4c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────────────────┬────────┬──────────┐\n",
      "│ id                              ┆ genero ┆ pais     │\n",
      "│ ---                             ┆ ---    ┆ ---      │\n",
      "│ str                             ┆ str    ┆ str      │\n",
      "╞═════════════════════════════════╪════════╪══════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ female ┆ colombia │\n",
      "│ 4639c055f34ca1f944d0137a5aeb79… ┆ female ┆ colombia │\n",
      "│ 92ffa98bade702b86417b118e8aca3… ┆ female ┆ colombia │\n",
      "│ 4560c6567afcccef265f048ed117d0… ┆ female ┆ colombia │\n",
      "│ 393866dfaa80d414c9896cf8723932… ┆ female ┆ colombia │\n",
      "└─────────────────────────────────┴────────┴──────────┘\n",
      "CARGA XML: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Carga de archivos XML completada.\n",
      "\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets crudos cargados: 4200 mostrando 1 registro:\n",
      " id:74bcc9b0882c8440716ff370494aea09\n",
      " pais:colombia\n",
      " genero:female\n",
      " xml_text:\n",
      "<author lang=\"es\">\n",
      "\t<documents>\n",
      "\t\t<document><![CDA...\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def cargar_xml(id_archivo, train=True) -> str:\n",
    "    ruta_base = os.path.join(base_path, \"data/author_profiling/es_test\" if not train else \"data/author_profiling/es_train\")\n",
    "    ruta_archivo = os.path.join(ruta_base, f\"{id_archivo}.xml\")\n",
    "    # Logger.info(f\"ruta archivo: {ruta_archivo}\")\n",
    "    \n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as file:\n",
    "        xml_text = file.read()\n",
    "\n",
    "    return xml_text\n",
    "\n",
    "# ===================== Cargar CSV de datos  ==================\n",
    " \n",
    "df_indices = pl.read_csv(ruta_guardado_entrenamiento)\n",
    "print(df_indices.head())\n",
    "\n",
    "cantidad_registros = len(df_indices)\n",
    "\n",
    "registros_crudos = [(\"id_user\",\"xml_doc\",\"pais\",\"genero\") for i in range(cantidad_registros)]\n",
    "\n",
    "\n",
    "for i, reg in enumerate(registros_crudos):\n",
    "    id_archivo = df_indices['id'][i]\n",
    "    id_user = id_archivo\n",
    "    pais = df_indices['pais'][i]\n",
    "    genero = df_indices['genero'][i]\n",
    "\n",
    "    xml_text: str = cargar_xml(id_archivo)\n",
    "    registros_crudos[i] = (id_user, xml_text, pais, genero)\n",
    "\n",
    "    print_bar(i, cantidad_registros, contexto=\"CARGA XML\")\n",
    "\n",
    "print()\n",
    "Logger.info(\"Carga de archivos XML completada.\\n\")\n",
    "id_user, xml_text, pais, genero = registros_crudos[0]\n",
    "\n",
    "\n",
    "Logger.debug(f\"Total de tweets crudos cargados: {len(registros_crudos)} mostrando 1 registro:\\n id:{id_user}\\n pais:{pais}\\n genero:{genero}\\n xml_text:\\n{xml_text[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4685f1",
   "metadata": {},
   "source": [
    "# PROCESAMOS LOS TEXTOS XML Y LOS DEJAMOS EN UN DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0287180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso registros limpiados: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] Guardando archivo\u001b[0m\n",
      "\u001b[92m[DEBUG] Total de tweets procesados: 419998 mostrando 5 primeros registros:\n",
      " shape: (5, 5)\n",
      "┌─────────────────────────────────┬─────────────────────────────────┬──────────┬────────┬─────────┐\n",
      "│ id_user                         ┆ tweet_crudo                     ┆ pais     ┆ genero ┆ idioma  │\n",
      "│ ---                             ┆ ---                             ┆ ---      ┆ ---    ┆ ---     │\n",
      "│ str                             ┆ str                             ┆ str      ┆ str    ┆ str     │\n",
      "╞═════════════════════════════════╪═════════════════════════════════╪══════════╪════════╪═════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tiene que valer la pena que es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Tintas chinas, si ven ésto, es… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ \"Maestro no le abrió!\" -Ay qué… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Lo bueno de no estar enamorado… ┆ colombia ┆ female ┆ Español │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ Recordaré este día como el día… ┆ colombia ┆ female ┆ Español │\n",
      "└─────────────────────────────────┴─────────────────────────────────┴──────────┴────────┴─────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "diccionario_idioma = {\n",
    "    'es': 'Español', \n",
    "    'en': 'Inglés', \n",
    "    'fr': 'Francés', \n",
    "    'de': 'Alemán', \n",
    "    'it': 'Italiano', \n",
    "    'nl': 'Neerlandés'\n",
    "}\n",
    "\n",
    "def limpiar_xml(texto_xml:str):\n",
    "    soup = BeautifulSoup(texto_xml, 'lxml-xml')   \n",
    "    lang = str(soup.author.get('lang'))\n",
    "    idioma = diccionario_idioma[lang] if lang in diccionario_idioma else lang\n",
    "    \n",
    "    documentos = soup.find_all('document') # Obtenemos todsa las etiquetas <document>\n",
    "    tweets = [doc.get_text(separator=\" \", strip=True) for doc in documentos] # Extraemos el texto de cada documento\n",
    "    \n",
    "    return tweets, idioma\n",
    "\n",
    "\n",
    "\n",
    "# ===================== Procesamiento de limpieza del XML =====================\n",
    "cantidad_registros = len(registros_crudos)\n",
    "\n",
    "registros_procesados = []\n",
    "\n",
    "for i, (id_user, doc_crudo, pais, genero) in enumerate(registros_crudos):\n",
    "    lista_tweets_por_usuario, idioma = limpiar_xml(doc_crudo)\n",
    "    \n",
    "    for tweet in lista_tweets_por_usuario:\n",
    "        registros_procesados.append({\n",
    "            \"id_user\": id_user,\n",
    "            \"tweet_crudo\": tweet,\n",
    "            \"pais\": pais,\n",
    "            \"genero\": genero,\n",
    "            \"idioma\": idioma\n",
    "        })\n",
    "    print_bar(i, cantidad_registros, contexto=\"Progreso registros limpiados\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ===================== Creamos dataset persistente =====================\n",
    "\n",
    "df_registros = pl.DataFrame(registros_procesados)\n",
    "df_registros.write_parquet(\"data/author_profiling/registros_procesados.parquet\")\n",
    "Logger.info(\"Guardando archivo\")\n",
    "\n",
    "lf_registros = df_registros.lazy()\n",
    "\n",
    "# ===================== Muestra =====================\n",
    "\n",
    "\n",
    "muestra = lf_registros.limit(5).collect()\n",
    "Logger.debug(f\"Total de tweets procesados: {len(registros_procesados)} mostrando 5 primeros registros:\\n {muestra}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45b49c",
   "metadata": {},
   "source": [
    "\n",
    "# Capa de dataclass de utilidd para normalización y limpieza de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15eed5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import unicodedata\n",
    "# ===================== Configuración de Limpieza de tweets =====================\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfigLimpieza:\n",
    "    normalizar_unicode: bool = True\n",
    "    a_minusculas: bool = True\n",
    "    quitar_urls: bool = True\n",
    "    quitar_menciones: bool = True\n",
    "    quitar_hashtags: bool = False \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    URL = re.compile(r'https?://\\S+', re.I)\n",
    "    MENCION = re.compile(r'@\\w+')\n",
    "    HASHTAG = re.compile(r'#\\w+')\n",
    "    ESPACIO = re.compile(r'\\s+')\n",
    "    TT = TweetTokenizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def limpiar(texto: str) -> str:\n",
    "        cfg = ConfigLimpieza()\n",
    "        if cfg.normalizar_unicode:\n",
    "            texto = unicodedata.normalize(\"NFKC\", texto)\n",
    "        if cfg.a_minusculas:\n",
    "            texto = texto.lower()\n",
    "        if cfg.quitar_urls:\n",
    "            texto = Tweet.URL.sub(\" \", texto)\n",
    "        if cfg.quitar_menciones:\n",
    "            texto = Tweet.MENCION.sub(\" \", texto)\n",
    "        if cfg.quitar_hashtags:\n",
    "            texto = Tweet.HASHTAG.sub(\" \", texto)\n",
    "\n",
    "        texto_limpio = Tweet.ESPACIO.sub(\" \", texto).strip()\n",
    "        return texto_limpio\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizar(texto: str) -> list[str]:\n",
    "        return [t for t in Tweet.TT.tokenize(texto)]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eca741",
   "metadata": {},
   "source": [
    "# PIPELINE DE LIMPIEZA Y/O TOKENIZACIÓN DE TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3714c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] PIPELINE: LIMPIEZA/ TOKENIZACIÓN\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe limpio:\n",
      " shape: (2, 6)\n",
      "┌──────────────────────┬──────────────────────┬──────────┬────────┬─────────┬──────────────────────┐\n",
      "│ id_user              ┆ tweet_crudo          ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio         │\n",
      "│ ---                  ┆ ---                  ┆ ---      ┆ ---    ┆ ---     ┆ ---                  │\n",
      "│ str                  ┆ str                  ┆ str      ┆ str    ┆ str     ┆ str                  │\n",
      "╞══════════════════════╪══════════════════════╪══════════╪════════╪═════════╪══════════════════════╡\n",
      "│ 74bcc9b0882c8440716f ┆ Tiene que valer la   ┆ colombia ┆ female ┆ Español ┆ tiene que valer la   │\n",
      "│ f370494aea…          ┆ pena que es…         ┆          ┆        ┆         ┆ pena que es…         │\n",
      "│ 74bcc9b0882c8440716f ┆ Tintas chinas, si    ┆ colombia ┆ female ┆ Español ┆ tintas chinas, si    │\n",
      "│ f370494aea…          ┆ ven ésto, es…        ┆          ┆        ┆         ┆ ven ésto, es…        │\n",
      "└──────────────────────┴──────────────────────┴──────────┴────────┴─────────┴──────────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet limpio: tiene que valer la pena que esté despierta a esta hora #hoylosgrammycon40\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado:\n",
      " shape: (2, 7)\n",
      "┌─────────────────┬────────────────┬──────────┬────────┬─────────┬────────────────┬────────────────┐\n",
      "│ id_user         ┆ tweet_crudo    ┆ pais     ┆ genero ┆ idioma  ┆ texto_limpio   ┆ tweet_tokeniza │\n",
      "│ ---             ┆ ---            ┆ ---      ┆ ---    ┆ ---     ┆ ---            ┆ do             │\n",
      "│ str             ┆ str            ┆ str      ┆ str    ┆ str     ┆ str            ┆ ---            │\n",
      "│                 ┆                ┆          ┆        ┆         ┆                ┆ list[str]      │\n",
      "╞═════════════════╪════════════════╪══════════╪════════╪═════════╪════════════════╪════════════════╡\n",
      "│ 74bcc9b0882c844 ┆ Tiene que      ┆ colombia ┆ female ┆ Español ┆ tiene que      ┆ [\"tiene\",      │\n",
      "│ 0716ff370494aea ┆ valer la pena  ┆          ┆        ┆         ┆ valer la pena  ┆ \"que\", …       │\n",
      "│ …               ┆ que es…        ┆          ┆        ┆         ┆ que es…        ┆ \"#hoylosgra…   │\n",
      "│ 74bcc9b0882c844 ┆ Tintas chinas, ┆ colombia ┆ female ┆ Español ┆ tintas chinas, ┆ [\"tintas\",     │\n",
      "│ 0716ff370494aea ┆ si ven ésto,   ┆          ┆        ┆         ┆ si ven ésto,   ┆ \"chinas\", …    │\n",
      "│ …               ┆ es…            ┆          ┆        ┆         ┆ es…            ┆ \"xoxo\"]        │\n",
      "└─────────────────┴────────────────┴──────────┴────────┴─────────┴────────────────┴────────────────┘\u001b[0m\n",
      "\u001b[94m[INFO] Primer tweet tokenizado: shape: (12,)\n",
      "Series: '' [str]\n",
      "[\n",
      "\t\"tiene\"\n",
      "\t\"que\"\n",
      "\t\"valer\"\n",
      "\t\"la\"\n",
      "\t\"pena\"\n",
      "\t…\n",
      "\t\"despierta\"\n",
      "\t\"a\"\n",
      "\t\"esta\"\n",
      "\t\"hora\"\n",
      "\t\"#hoylosgrammycon40\"\n",
      "]\u001b[0m\n",
      "\u001b[94m[INFO] Token: tiene\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: valer\u001b[0m\n",
      "\u001b[94m[INFO] Token: la\u001b[0m\n",
      "\u001b[94m[INFO] Token: pena\u001b[0m\n",
      "\u001b[94m[INFO] Token: que\u001b[0m\n",
      "\u001b[94m[INFO] Token: esté\u001b[0m\n",
      "\u001b[94m[INFO] Token: despierta\u001b[0m\n",
      "\u001b[94m[INFO] Token: a\u001b[0m\n",
      "\u001b[94m[INFO] Token: esta\u001b[0m\n",
      "\u001b[94m[INFO] Token: hora\u001b[0m\n",
      "\u001b[94m[INFO] Token: #hoylosgrammycon40\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== TRANSFORMACIONES EN PIPELINE =====================\n",
    "\"\"\"\n",
    "Recordemos que los pipelines de polars si es un lazyframe no se ejecutan hasta que se les pida con .collect() (Docoumentación)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lf_registros_crudos = lf_registros\n",
    "\n",
    "\n",
    "Logger.info(\"PIPELINE: LIMPIEZA/ TOKENIZACIÓN\")\n",
    "lf_limpio = (\n",
    "    lf_registros_crudos.with_columns(\n",
    "        pl.col(\"tweet_crudo\")\n",
    "          .map_elements(Tweet.limpiar)\n",
    "          .alias(\"texto_limpio\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe limpio:\\n {lf_limpio.limit(2).collect()}\")\n",
    "\n",
    "primer_tweet_limpio = (\n",
    "    lf_limpio\n",
    "    .select('texto_limpio')\n",
    "    .limit(1)\n",
    "    .collect()\n",
    "    .to_series()\n",
    "    .to_list()[0]\n",
    ")\n",
    "\n",
    "Logger.info(f\"Primer tweet limpio: {primer_tweet_limpio}\")\n",
    "        \n",
    "lf_tokens = (\n",
    "    lf_limpio.with_columns(\n",
    "        pl.col(\"texto_limpio\")\n",
    "          .map_elements(\n",
    "              Tweet.tokenizar, \n",
    "              return_dtype=pl.List(pl.Utf8), \n",
    "              skip_nulls=True\n",
    "            )\n",
    "          .alias(\"tweet_tokenizado\")\n",
    "    )\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado:\\n {lf_tokens.limit(2).collect()}\")\n",
    "\n",
    "\n",
    "# ======================== Metodo para extraer el primer tweet tokenizado con polars =====================\n",
    "primer_tweet_tokenizado = (\n",
    "    lf_tokens\n",
    "    .select(\"tweet_tokenizado\") # Seleccionamos la columna que nos interesa\n",
    "    .limit(1) \n",
    "    .collect()\n",
    "    .item()  # Sirve para extraer el valor cuando el DataFrame es 1x1\n",
    ")\n",
    "# ========================================================================================================\n",
    "\n",
    "Logger.info(f\"Primer tweet tokenizado: {primer_tweet_tokenizado}\")\n",
    "\n",
    "for token in primer_tweet_tokenizado:\n",
    "    Logger.info(f\"Token: {token}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a6e4b",
   "metadata": {},
   "source": [
    "# VAMOS A GENERAR UN VOCABULARIO GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a514f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] CONSTRUCCIÓN DE VOCABULARIO GLOBAL\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando registros del lazyframe tokenizado y aplanado:\n",
      " shape: (5_908_700, 2)\n",
      "┌─────────────────────────────────┬──────────────────┐\n",
      "│ id_user                         ┆ tokens_del_tweet │\n",
      "│ ---                             ┆ ---              │\n",
      "│ str                             ┆ str              │\n",
      "╞═════════════════════════════════╪══════════════════╡\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ tiene            │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ que              │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ valer            │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ la               │\n",
      "│ 74bcc9b0882c8440716ff370494aea… ┆ pena             │\n",
      "│ …                               ┆ …                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ recupere         │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ ,                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ saludos          │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ !                │\n",
      "│ 7ea28a182539c384d69cba3d10623f… ┆ !                │\n",
      "└─────────────────────────────────┴──────────────────┘\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando vocabulario:\n",
      " shape: (205_231, 3)\n",
      "┌──────────────────┬───────────────────┬───────────────────┐\n",
      "│ tokens_del_tweet ┆ frecuencia_global ┆ documentos_unicos │\n",
      "│ ---              ┆ ---               ┆ ---               │\n",
      "│ str              ┆ u32               ┆ u32               │\n",
      "╞══════════════════╪═══════════════════╪═══════════════════╡\n",
      "│ excelen          ┆ 2                 ┆ 2                 │\n",
      "│ 24nov            ┆ 1                 ┆ 1                 │\n",
      "│ admirarlos       ┆ 2                 ┆ 2                 │\n",
      "│ perras           ┆ 22                ┆ 22                │\n",
      "│ reacomodo        ┆ 2                 ┆ 2                 │\n",
      "│ …                ┆ …                 ┆ …                 │\n",
      "│ #gm              ┆ 1                 ┆ 1                 │\n",
      "│ puramente        ┆ 3                 ┆ 3                 │\n",
      "│ épocas           ┆ 55                ┆ 49                │\n",
      "│ shorando         ┆ 1                 ┆ 1                 │\n",
      "│ berlocq-mayer    ┆ 1                 ┆ 1                 │\n",
      "└──────────────────┴───────────────────┴───────────────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================== VOCABULARIO GLOBAL =====================\n",
    "\n",
    "Logger.info(\"CONSTRUCCIÓN DE VOCABULARIO GLOBAL\")\n",
    "\n",
    "\n",
    "lf_tokens_aplanados = (\n",
    "    lf_tokens\n",
    "    .select(\n",
    "        'id_user', \n",
    "        pl.col('tweet_tokenizado')\n",
    "    )\n",
    "    .explode('tweet_tokenizado')\n",
    "    .rename({'tweet_tokenizado':'tokens_del_tweet'})\n",
    ")\n",
    "\n",
    "\n",
    "lf_vocabulario = (\n",
    "    lf_tokens_aplanados\n",
    "    .group_by('tokens_del_tweet')\n",
    "    .agg([\n",
    "        pl.len().alias('frecuencia_global'),              # Conteo total de apariciones de cada token\n",
    "        pl.n_unique('id_user').alias('documentos_unicos') # En cuántos documentos aparece ese token\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "Logger.debug(f\"Mostrando registros del lazyframe tokenizado y aplanado:\\n {lf_tokens_aplanados.collect()}\")\n",
    "Logger.debug(f\"Mostrando vocabulario:\\n {lf_vocabulario.collect()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fda952",
   "metadata": {},
   "source": [
    "# GENERAMOS LAS ETIQUETAS PARA PODER AVENTARNOS LO DEL ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7325dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Total de documentos (usuarios): 4200\u001b[0m\n",
      "\u001b[92m[DEBUG] Muestra de etiquetas (y): shape: (4_200,)\n",
      "Series: 'pais' [str]\n",
      "[\n",
      "\t\"spain\"\n",
      "\t\"peru\"\n",
      "\t\"venezuela\"\n",
      "\t\"peru\"\n",
      "\t\"spain\"\n",
      "\t…\n",
      "\t\"colombia\"\n",
      "\t\"chile\"\n",
      "\t\"peru\"\n",
      "\t\"peru\"\n",
      "\t\"peru\"\n",
      "]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Parámetros de reducción de dimensión --------------------\n",
    "lf_documentos_unicos = (\n",
    "    lf_limpio\n",
    "    .select(['id_user', 'pais', 'genero'])\n",
    "    .unique(maintain_order=True)\n",
    "    .sort('id_user')\n",
    "    .with_row_index(name='indice_doc', offset=0)\n",
    ")\n",
    "\n",
    "\n",
    "Logger.info(f\"Total de documentos (usuarios): {lf_documentos_unicos.count().collect()[0,0]}\")\n",
    "\n",
    "\n",
    "# 2) Etiquetas alineadas (y_train)\n",
    "lf_etiquetas = (\n",
    "    lf_documentos_unicos\n",
    "    .select('pais', 'genero')\n",
    "    .collect()\n",
    "    .to_series()\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Muestra de etiquetas (y): {lf_etiquetas}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312ff0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[DEBUG] Mostrando vocabulario reducido:\n",
      " shape: (38_575, 3)\n",
      "┌──────────────────┬───────────────────┬───────────────────┐\n",
      "│ tokens_del_tweet ┆ frecuencia_global ┆ documentos_unicos │\n",
      "│ ---              ┆ ---               ┆ ---               │\n",
      "│ str              ┆ u32               ┆ u32               │\n",
      "╞══════════════════╪═══════════════════╪═══════════════════╡\n",
      "│ de               ┆ 215391            ┆ 4190              │\n",
      "│ ,                ┆ 161864            ┆ 4125              │\n",
      "│ .                ┆ 156802            ┆ 4005              │\n",
      "│ que              ┆ 149950            ┆ 4155              │\n",
      "│ la               ┆ 135889            ┆ 4183              │\n",
      "│ …                ┆ …                 ┆ …                 │\n",
      "│ pirobos          ┆ 5                 ┆ 5                 │\n",
      "│ #adn             ┆ 5                 ┆ 5                 │\n",
      "│ carmela          ┆ 5                 ┆ 5                 │\n",
      "│ signifique       ┆ 5                 ┆ 5                 │\n",
      "│ mazatlán         ┆ 5                 ┆ 5                 │\n",
      "└──────────────────┴───────────────────┴───────────────────┘\u001b[0m\n",
      "\u001b[92m[DEBUG] Vocabulario reducido numoy: [['de'], [','], ['.'], ['que'], ['la']]\u001b[0m\n",
      "\u001b[92m[DEBUG] Mostrando diccionario de índices:\n",
      " shape: (38_575, 2)\n",
      "┌────────────┬──────────────────┐\n",
      "│ indice_col ┆ tokens_del_tweet │\n",
      "│ ---        ┆ ---              │\n",
      "│ u32        ┆ str              │\n",
      "╞════════════╪══════════════════╡\n",
      "│ 0          ┆ de               │\n",
      "│ 1          ┆ ,                │\n",
      "│ 2          ┆ .                │\n",
      "│ 3          ┆ que              │\n",
      "│ 4          ┆ la               │\n",
      "│ …          ┆ …                │\n",
      "│ 38570      ┆ prota            │\n",
      "│ 38571      ┆ ghoul            │\n",
      "│ 38572      ┆ brand            │\n",
      "│ 38573      ┆ microcefalia     │\n",
      "│ 38574      ┆ clint            │\n",
      "└────────────┴──────────────────┘\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 3) Vocabulario reducido → diccionario de índices (token → columna)\n",
    "\n",
    "MAX_TERMINOS = 50_000  \n",
    "MIN_DOCS = 5\n",
    "\n",
    "lf_vocabulario_reducido = (\n",
    "    lf_vocabulario\n",
    "    .filter(\n",
    "        pl.col('documentos_unicos') >= MIN_DOCS)      # filtra tokens raros\n",
    "    .sort('frecuencia_global', descending=True)\n",
    "    \n",
    "    \n",
    "    #.limit(MAX_TERMINOS)                                 \n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando vocabulario reducido:\\n {lf_vocabulario_reducido.collect()}\")  \n",
    "\n",
    "\n",
    "lf_vocabulario_reducido_numpy =(\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "\n",
    "Logger.debug(f\"Vocabulario reducido numoy: {lf_vocabulario_reducido_numpy[:5]}\")\n",
    "\n",
    "\n",
    "tokens_ordenados = (\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Usado por el profe\n",
    "# dicc_indices = {token: i for i, token in enumerate(tokens_ordenados)}\n",
    "\n",
    "# for token, i in list(dicc_indices.items())[:10]:\n",
    "#     Logger.info(f\"Token: {token} → Índice columna: {i}\")\n",
    "\n",
    "\n",
    "\n",
    "# Lazy_frame de un dict_indices\n",
    "\n",
    "lf_dict_indices = (\n",
    "    tokens_ordenados\n",
    "    .with_row_index(name='indice_col', offset=0)\n",
    ")\n",
    "\n",
    "Logger.debug(f\"Mostrando diccionario de índices:\\n {lf_dict_indices.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33da903d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVERSION DEL PROFE\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "VERSION DEL PROFE\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "# def built_bow_tr_profe_version(tweets, Vocabulario, dict_indices):\n",
    "#     # Tweets: lista de tweets tokenizados | LazyFrame ya coleccionado\n",
    "#     # Vocabulario: lista de tuplas (token, frecuencia) | LazyFrame ya coleccionado\n",
    "#     # dict_indices: diccionario índice → token columna | lazyFrame ya coleccionado\n",
    "\n",
    "#     BOW_FRECUENCIA = np.zeros((len(tweets), len(Vocabulario)), dtype=int)\n",
    "#     BOW_BINARIO = np.zeros((len(tweets), len(Vocabulario)), dtype=int)\n",
    "\n",
    "#     contador = 0\n",
    "#     for tweet in tweets:\n",
    "#         for palabra in tweet:\n",
    "#             if palabra in dict_indices:\n",
    "#                 Logger.debug(f\"Palabra encontrada: {palabra} en tweet {contador}\")\n",
    "#                 BOW_FRECUENCIA[contador, dict_indices[palabra]] = tweet[palabra] # FRECUENCIA DE LA PALABRA EN VEZ DE HACERLO BINARIO\n",
    "#                 BOW_BINARIO[contador, dict_indices[palabra]] = 1 # HACIENDOLO BINARIO\n",
    "#         contador += 1\n",
    "\n",
    "#     return BOW_FRECUENCIA, BOW_BINARIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def built_bow_tr_profe_version(tweets_tokens, vocab_tokens, dict_indices):\n",
    "#     # Tweets: lista de tweets tokenizados | LazyFrame ya coleccionado\n",
    "#     # Vocabulario: lista de tuplas (token, frecuencia) | LazyFrame ya coleccionado\n",
    "#     # dict_indices: diccionario índice → token columna | lazyFrame ya coleccionado\n",
    "    \n",
    "#     V = len(vocab_tokens)\n",
    "#     n = len(tweets_tokens)\n",
    "    \n",
    "#     BOW_FRECUENCIA = np.zeros((n, V), dtype=int)\n",
    "#     BOW_BINARIO    = np.zeros((n, V), dtype=int)\n",
    "    \n",
    "#     for i, tokens in enumerate(tweets_tokens):\n",
    "#         frec = Counter(tokens)              # {'de': 3, 'la': 1, ...}\n",
    "#         for token, frecuencia in frec.items():\n",
    "#             columna_vocab = dict_indices.get(token)       # columna del vocabulario\n",
    "#             if columna_vocab is not None:                    # solo si el token está en el vocab reducido\n",
    "#                 BOW_FRECUENCIA[i, columna_vocab] = frecuencia\n",
    "#                 BOW_BINARIO[i, columna_vocab] = 1\n",
    "#     return BOW_FRECUENCIA, BOW_BINARIO\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2796f",
   "metadata": {},
   "source": [
    "# CONSTRUCCIÓN DE BOW (BAG OF WORDS) - MATRIZ DE TÉRMINO-RECURSO (TR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55238e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] Vocabulario preparado: 38,575 términos\u001b[0m\n",
      "\u001b[94m[INFO] dict_indices listo: 38,575 entradas\u001b[0m\n",
      "\u001b[94m[INFO] Tweets preparados: 10,000 documentos\u001b[0m\n",
      "\n",
      "Construyendo BOW: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\u001b[94m[INFO] BOW construido.\u001b[0m\n",
      "\u001b[94m[INFO] BOW frecuencia: (10000, 38575) | nnz=115,921\u001b[0m\n",
      "\u001b[94m[INFO] BOW binario:    (10000, 38575)    | nnz=115,921\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# --- A) Preparar insumos en los tipos correctos ------------------------------\n",
    "\n",
    "# 1) vocabulario: aplanar si viene como [['de'], [','], ...]\n",
    "vocab_tokens = (\n",
    "    lf_vocabulario_reducido\n",
    "    .select('tokens_del_tweet')\n",
    "    .collect()\n",
    "    .get_column('tokens_del_tweet')\n",
    "    .to_list()\n",
    ")\n",
    "# Si insistieras en usar tu *_numpy:\n",
    "# vocab_tokens = [x[0] for x in lf_vocabulario_reducido_numpy]\n",
    "\n",
    "Logger.info(f\"Vocabulario preparado: {len(vocab_tokens):,} términos\")\n",
    "\n",
    "# 2) dict_indices: convertir DataFrame -> dict {token: indice}\n",
    "df_dict = lf_dict_indices.collect()  # columnas: ['indice_col','tokens_del_tweet']\n",
    "dict_indices = dict(zip(\n",
    "    df_dict.get_column('tokens_del_tweet').to_list(),\n",
    "    map(int, df_dict.get_column('indice_col').to_list())\n",
    "))\n",
    "Logger.info(f\"dict_indices listo: {len(dict_indices):,} entradas\")\n",
    "\n",
    "# 3) tweets: lista de listas de tokens\n",
    "\n",
    "# LIMITE DE TEWEETS\n",
    "LIMITE_TWEETS = 10000\n",
    "\n",
    "tweets = (\n",
    "    lf_tokens\n",
    "    .select(\"tweet_tokenizado\")\n",
    "    .limit(LIMITE_TWEETS)  # solo si quieres probar con un subset\n",
    "    .collect()\n",
    "    .get_column(\"tweet_tokenizado\")\n",
    "    .to_list()\n",
    ")\n",
    "Logger.info(f\"Tweets preparados: {len(tweets):,} documentos\")\n",
    "\n",
    "\n",
    "lf_etiquetas = (\n",
    "    lf_documentos_unicos\n",
    "    .select('pais', 'genero')\n",
    "    .limit(LIMITE_TWEETS)  # Asegúrate de alinear con los tweets\n",
    "    .collect()\n",
    "    .to_series()\n",
    ")\n",
    "\n",
    "# --- B) Función del profe adaptada a listas de tokens + dict_indices ---------\n",
    "\n",
    "def built_bow_tr_profe_version(tweets_tokens, vocab_tokens, dict_indices):\n",
    "    V = len(vocab_tokens)\n",
    "    n = len(tweets_tokens)\n",
    "    BOW_FRECUENCIA = np.zeros((n, V), dtype=int)\n",
    "    BOW_BINARIO    = np.zeros((n, V), dtype=int)\n",
    "\n",
    "    for i, toks in enumerate(tweets_tokens):\n",
    "        if not toks:\n",
    "            continue\n",
    "        frec = Counter(toks)  # {'de': 3, 'la': 1, ...}\n",
    "        for token, cnt in frec.items():\n",
    "            j = dict_indices.get(token)   # columna del vocabulario\n",
    "            if j is not None:\n",
    "                BOW_FRECUENCIA[i, j] = cnt\n",
    "                BOW_BINARIO[i, j] = 1\n",
    "        print_bar(i, n, contexto=\"Construyendo BOW\")\n",
    "    return BOW_FRECUENCIA, BOW_BINARIO\n",
    "\n",
    "\n",
    "# --- C) Construir BOW con los insumos ya limpios -----------------------------\n",
    "print()\n",
    "BOW_FRECUENCIA, BOW_BINARIO = built_bow_tr_profe_version(tweets, vocab_tokens, dict_indices)\n",
    "Logger.info(\"BOW construido.\")\n",
    "\n",
    "# ========== Chequeo rápido =========\n",
    "nnz_freq = int((BOW_FRECUENCIA > 0).sum())\n",
    "nnz_bin  = int(BOW_BINARIO.sum())\n",
    "Logger.info(f\"BOW frecuencia: {BOW_FRECUENCIA.shape} | nnz={nnz_freq:,}\")\n",
    "Logger.info(f\"BOW binario:    {BOW_BINARIO.shape}    | nnz={nnz_bin:,}\")\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ad782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo DTR: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "Construyendo DTR: |🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩🟩| 100.00%\n",
      "\u001b[94m[INFO] DOR frecuencia: (38575, 10000) | DOR binaria: (38575, 10000)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROFE VERSION\n",
    "\"\"\"\n",
    "def compute_dor_profe(TR_densa: np.ndarray) -> np.ndarray:\n",
    "    DTR = np.zeros((TR_densa.shape[1], TR_densa.shape[0]), dtype=float)\n",
    "    tam_v = TR_densa.shape[1]\n",
    "    for i, doc in enumerate(TR_densa):\n",
    "        pos_no_cero = np.nonzero(doc)[0]\n",
    "        tam_vocab_doc = len(pos_no_cero)\n",
    "        for termino in pos_no_cero:\n",
    "            DTR[termino, i] = doc[termino] * np.log(tam_v / max(1, tam_vocab_doc))\n",
    "        print_bar(i, TR_densa.shape[0], contexto=\"Construyendo DTR\")\n",
    "    print()\n",
    "    return DTR\n",
    "\n",
    "\n",
    "TR_frecuencia_densa = BOW_FRECUENCIA\n",
    "DTR_frecuencia = compute_dor_profe(TR_frecuencia_densa)\n",
    "\n",
    "TR_binaria_densa    = BOW_BINARIO\n",
    "DTR_binaria    = compute_dor_profe(TR_binaria_densa)\n",
    "\n",
    "\n",
    "\n",
    "Logger.info(f\"DOR frecuencia: {DTR_frecuencia.shape} | DOR binaria: {DTR_binaria.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0118c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO] [CSR] TR_frecuencia: shape=(10000, 38575), nnz=115,921\u001b[0m\n",
      "\u001b[94m[INFO] [CSR] TR_binaria:    shape=(10000, 38575), nnz=115,921\u001b[0m\n",
      "\u001b[94m[INFO] [L2] TR normalizado (frecuencia y binario) listo.\u001b[0m\n",
      "\u001b[94m[INFO] [L2] DOR_frecuencia normalizado.\u001b[0m\n",
      "\u001b[94m[INFO] [L2] DOR_binaria normalizado.\u001b[0m\n",
      "\u001b[94m[INFO] Shapes → Frecuencia tr=(8000, 38575) va=(2000, 38575) | Binaria tr=(8000, 38575) va=(2000, 38575)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ================== NORMALIZACIÓN L2 (TR y DOR) ==================\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Pasar tus BOW densos a CSR (rápido y memoria-friendly)\n",
    "TR_frecuencia_csr = csr_matrix(BOW_FRECUENCIA, dtype=np.float32)\n",
    "TR_binaria_csr    = csr_matrix(BOW_BINARIO,    dtype=np.float32)\n",
    "\n",
    "\n",
    "# LazyFrame de tiquetas | genero | pais |\n",
    "y_train = (\n",
    "    lf_limpio\n",
    "    .select('pais')               # solo la nacionalidad\n",
    "    .limit(LIMITE_TWEETS)         # mismo límite que tweets\n",
    "    .collect()\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "assert TR_frecuencia_csr.shape[0] == len(y_train), \\\n",
    "    f\"X vs y desalineado: {TR_frecuencia_csr.shape[0]} vs {len(y_train)}\"\n",
    "\n",
    "\n",
    "Logger.info(f\"[CSR] TR_frecuencia: shape={TR_frecuencia_csr.shape}, nnz={TR_frecuencia_csr.nnz:,}\")\n",
    "Logger.info(f\"[CSR] TR_binaria:    shape={TR_binaria_csr.shape}, nnz={TR_binaria_csr.nnz:,}\")\n",
    "\n",
    "# 2) Normalizar L2 por documento (filas) en TR\n",
    "TR_frecuencia_L2_csr = normalize(TR_frecuencia_csr, norm='l2', copy=True)\n",
    "TR_binaria_L2_csr    = normalize(TR_binaria_csr,    norm='l2', copy=True)\n",
    "\n",
    "Logger.info(\"[L2] TR normalizado (frecuencia y binario) listo.\")\n",
    "\n",
    "# 3) (OPCIONAL) Normalizar L2 también los DOR del profe\n",
    "#    DOR es término×documento → normalizamos por documento (columnas),\n",
    "#    así que transponemos, normalizamos filas y des-transponemos.\n",
    "#    Solo si ya creaste DTR_frecuencia / DTR_binaria (matrices densas).\n",
    "if 'DTR_frecuencia' in globals():\n",
    "    DOR_frecuencia_L2 = normalize(DTR_frecuencia.T, norm='l2', copy=True).T\n",
    "    Logger.info(\"[L2] DOR_frecuencia normalizado.\")\n",
    "if 'DTR_binaria' in globals():\n",
    "    DOR_binaria_L2 = normalize(DTR_binaria.T, norm='l2', copy=True).T\n",
    "    Logger.info(\"[L2] DOR_binaria normalizado.\")\n",
    "\n",
    "# 4) Split 80/20 con TR (elige qué representación usar)\n",
    "\n",
    "Xf_tr, Xf_va, y_tr, y_va   = train_test_split(TR_frecuencia_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_tr, Xb_va, _, _         = train_test_split(TR_binaria_csr,       y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xf_trL2, Xf_vaL2, _, _     = train_test_split(TR_frecuencia_L2_csr, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "Xb_trL2, Xb_vaL2, _, _     = train_test_split(TR_binaria_L2_csr,    y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "Logger.info(f\"Shapes → Frecuencia tr={Xf_tr.shape} va={Xf_va.shape} | Binaria tr={Xb_tr.shape} va={Xb_va.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdcd2f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Shape TR_frecuencia_csr: (10000, 38575)\n",
      "[DEBUG] Primeras 10 etiquetas y_train: ['colombia', 'colombia', 'colombia', 'colombia', 'colombia', 'colombia', 'colombia', 'colombia', 'colombia', 'colombia']\n",
      "[DEBUG] Número total de etiquetas: 10000\n",
      "[DEBUG] Distribución de clases en y_train:\n",
      "  colombia: 10000\n",
      "[DEBUG] Xf_tr shape: (8000, 38575), Xf_va shape: (2000, 38575)\n",
      "[DEBUG] y_tr muestras: 8000, y_va muestras: 2000\n"
     ]
    }
   ],
   "source": [
    "# Debug antes del assert\n",
    "print(f\"[DEBUG] Shape TR_frecuencia_csr: {TR_frecuencia_csr.shape}\")\n",
    "print(f\"[DEBUG] Primeras 10 etiquetas y_train: {y_train[:10]}\")  # te aseguras de que tengan sentido\n",
    "print(f\"[DEBUG] Número total de etiquetas: {len(y_train)}\")\n",
    "\n",
    "assert TR_frecuencia_csr.shape[0] == len(y_train), \\\n",
    "    f\"X vs y desalineado: {TR_frecuencia_csr.shape[0]} vs {len(y_train)}\"\n",
    "\n",
    "# También podrías imprimir el histograma de clases\n",
    "import collections\n",
    "conteo = collections.Counter(y_train)\n",
    "print(\"[DEBUG] Distribución de clases en y_train:\")\n",
    "for clase, count in conteo.items():\n",
    "    print(f\"  {clase}: {count}\")\n",
    "\n",
    "# Y después del split, revisa tamaños\n",
    "print(f\"[DEBUG] Xf_tr shape: {Xf_tr.shape}, Xf_va shape: {Xf_va.shape}\")\n",
    "print(f\"[DEBUG] y_tr muestras: {len(y_tr)}, y_va muestras: {len(y_va)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071b04e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.str_('colombia')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcsv\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- 1) Entrenamiento de los 4 modelos ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m clf_bin    = \u001b[43mLinearSVC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m clf_fre    = LinearSVC(C=\u001b[32m1.0\u001b[39m).fit(Xf_tr,    y_tr)\n\u001b[32m     12\u001b[39m clf_bin_l2 = LinearSVC(C=\u001b[32m1.0\u001b[39m).fit(Xb_trL2,  y_tr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/svm/_classes.py:321\u001b[39m, in \u001b[36mLinearSVC.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n\u001b[32m    317\u001b[39m _dual = _validate_dual_parameter(\n\u001b[32m    318\u001b[39m     \u001b[38;5;28mself\u001b[39m.dual, \u001b[38;5;28mself\u001b[39m.loss, \u001b[38;5;28mself\u001b[39m.penalty, \u001b[38;5;28mself\u001b[39m.multi_class, X\n\u001b[32m    319\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[38;5;28mself\u001b[39m.coef_, \u001b[38;5;28mself\u001b[39m.intercept_, n_iter_ = \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_dual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = n_iter_.max().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/semestre_v/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1187\u001b[39m, in \u001b[36m_fit_liblinear\u001b[39m\u001b[34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[39m\n\u001b[32m   1185\u001b[39m     classes_ = enc.classes_\n\u001b[32m   1186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes_) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1188\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1189\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1190\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1191\u001b[39m         )\n\u001b[32m   1192\u001b[39m     class_weight_ = compute_class_weight(\n\u001b[32m   1193\u001b[39m         class_weight, classes=classes_, y=y, sample_weight=sample_weight\n\u001b[32m   1194\u001b[39m     )\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.str_('colombia')"
     ]
    }
   ],
   "source": [
    "# ====== ENTRENAR, EVALUAR Y GRAFICAR (TODO EN UNO) ======\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "# --- 1) Entrenamiento de los 4 modelos ---\n",
    "clf_bin    = LinearSVC(C=1.0).fit(Xb_tr,    y_tr)\n",
    "clf_fre    = LinearSVC(C=1.0).fit(Xf_tr,    y_tr)\n",
    "clf_bin_l2 = LinearSVC(C=1.0).fit(Xb_trL2,  y_tr)\n",
    "clf_fre_l2 = LinearSVC(C=1.0).fit(Xf_trL2,  y_tr)\n",
    "print(\"[OK] Modelos entrenados: clf_bin, clf_fre, clf_bin_l2, clf_fre_l2\")\n",
    "\n",
    "# --- 2) Evaluación unificada ---\n",
    "def eval_model(nombre, clf, Xva, y_va):\n",
    "    y_pred = clf.predict(Xva)\n",
    "    acc = accuracy_score(y_va, y_pred)\n",
    "    f1m = f1_score(y_va, y_pred, average='macro')\n",
    "    cm  = confusion_matrix(y_va, y_pred, labels=sorted(set(y_va)))\n",
    "    rep = classification_report(y_va, y_pred, digits=3, zero_division=0)\n",
    "    return {\n",
    "        \"name\": nombre,\n",
    "        \"acc\": acc,\n",
    "        \"f1m\": f1m,\n",
    "        \"cm\": cm,\n",
    "        \"labels\": sorted(set(y_va)),\n",
    "        \"report\": rep\n",
    "    }\n",
    "\n",
    "res_bin     = eval_model(\"Binario SIN L2\",    clf_bin,    Xb_va,   y_va)\n",
    "res_freq    = eval_model(\"Frecuencia SIN L2\", clf_fre,    Xf_va,   y_va)\n",
    "res_bin_l2  = eval_model(\"Binario CON L2\",    clf_bin_l2, Xb_vaL2, y_va)\n",
    "res_freq_l2 = eval_model(\"Frecuencia CON L2\", clf_fre_l2, Xf_vaL2, y_va)\n",
    "resultados = [res_bin, res_freq, res_bin_l2, res_freq_l2]\n",
    "\n",
    "# Guarda resumen en CSV (útil para la tabla del reporte)\n",
    "with open(\"resumen_validacion.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"Experimento\", \"Accuracy\", \"Macro-F1\"])\n",
    "    for r in resultados:\n",
    "        w.writerow([r[\"name\"], f\"{r['acc']:.4f}\", f\"{r['f1m']:.4f}\"])\n",
    "print(\"CSV guardado: resumen_validacion.csv\")\n",
    "\n",
    "# --- 3) Gráfico comparativo (Accuracy vs Macro-F1) a SVG ---\n",
    "names = [r[\"name\"] for r in resultados]\n",
    "accs  = [r[\"acc\"]  for r in resultados]\n",
    "f1ms  = [r[\"f1m\"]  for r in resultados]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "w = 0.35\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.bar(x - w/2, accs, width=w, label=\"Accuracy\")\n",
    "plt.bar(x + w/2, f1ms, width=w, label=\"Macro-F1\")\n",
    "plt.xticks(x, names, rotation=15, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparativa_svm.svg\", format=\"svg\")\n",
    "plt.close()\n",
    "print(\"SVG guardado: comparativa_svm.svg\")\n",
    "\n",
    "# --- 4) Matrices de confusión a SVG por experimento ---\n",
    "def plot_cm_svg(cm, labels, title, outfile):\n",
    "    plt.figure(figsize=(5.5,4.5))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "\n",
    "    # anotar conteos\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, format=\"svg\")\n",
    "    plt.close()\n",
    "    print(f\"SVG guardado: {outfile}\")\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    return s.lower().replace(\" \", \"_\")\n",
    "\n",
    "for r in resultados:\n",
    "    title   = f\"CM - {r['name']}\"\n",
    "    outfile = f\"cm_{safe_name(r['name'])}.svg\"\n",
    "    plot_cm_svg(r[\"cm\"], r[\"labels\"], title, outfile)\n",
    "\n",
    "# --- 5) (Opcional) Imprime el classification_report en consola para el PDF del notebook ---\n",
    "for r in resultados:\n",
    "    print(f\"\\n=== {r['name']} ===\")\n",
    "    print(f\"Accuracy: {r['acc']:.4f} | Macro-F1: {r['f1m']:.4f}\")\n",
    "    print(r[\"report\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semestre-v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
