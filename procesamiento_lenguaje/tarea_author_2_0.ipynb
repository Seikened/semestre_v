{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ac688fb",
      "metadata": {
        "id": "9ac688fb"
      },
      "source": [
        "# FERNANDO LEON FRANCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4b764265",
      "metadata": {
        "id": "4b764265"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "from wordcloud import WordCloud\n",
        "from bs4 import BeautifulSoup\n",
        "from colorstreak import Logger\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn import metrics, preprocessing, svm\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "18b4c829",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_bar(i, cantidad_registros, contexto=\"PROGRESO\"):\n",
        "    porcentaje = (i + 1) / cantidad_registros * 100\n",
        "    # Con emojis\n",
        "    barra = int(50 * (i + 1) / cantidad_registros) * \"üü©\"\n",
        "    espacio = int(50 - len(barra)) * \"‚¨õÔ∏è\"\n",
        "\n",
        "    print(f\"\\r{contexto}: |{barra}{espacio}| {porcentaje:6.2f}%\", end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "03efdbd8",
      "metadata": {
        "id": "03efdbd8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======================= Limpieza de texto =======================\n",
        "def limpiar_texto(texto):\n",
        "    \n",
        "    texto = BeautifulSoup(texto, \"html.parser\").get_text()\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)\n",
        "    texto = re.sub(r\"@\\w+\", \"\", texto)\n",
        "    texto = re.sub(r\"#+\", \"\", texto)\n",
        "    stop_words = set(stopwords.words(\"spanish\"))\n",
        "    texto_limpio = [word for word in texto.split() if word not in stop_words]\n",
        "    return \" \".join(texto_limpio)\n",
        "\n",
        "\n",
        "# ======================= Carga de datos =======================\n",
        "def get_texts_from_folder(path_folder):\n",
        "    tr_txt = []  # aqu√≠ van los documentos\n",
        "    tr_y = []  # aqu√≠ van las etiquetas\n",
        "\n",
        "    for file in os.listdir(path_folder):\n",
        "        if file.endswith(\".xml\"):\n",
        "            tree = ET.parse(os.path.join(path_folder, file))\n",
        "            root = tree.getroot()\n",
        "            docs = []\n",
        "            for doc in root.iter(\"document\"):\n",
        "                texto_limpio = limpiar_texto(doc.text)\n",
        "                docs.append(texto_limpio)\n",
        "    \n",
        "            tr_txt.append(\" \".join(docs))\n",
        "\n",
        "    truth_file = os.path.join(path_folder, \"truth.txt\")\n",
        "\n",
        "    file_to_label = {}\n",
        "    with open(truth_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\":::\")\n",
        "            # Pa√≠s\n",
        "            \n",
        "            # pais = parts[2]\n",
        "            # file_to_label[parts[0]] = pais\n",
        "            \n",
        "            # Genero\n",
        "            genero = parts[1]\n",
        "            file_to_label[parts[0]] = genero\n",
        "\n",
        "\n",
        "\n",
        "    for file in os.listdir(path_folder):\n",
        "        if file.endswith(\".xml\"):\n",
        "            file_id = file.split(\".\")[0]\n",
        "            if file_id in file_to_label:\n",
        "                tr_y.append(file_to_label[file_id])\n",
        "\n",
        "        print_bar(len(tr_y), len(file_to_label), contexto=\"CARGA DE ETIQUETAS\")\n",
        "\n",
        "    return tr_txt, tr_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4fe124d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fe124d4",
        "outputId": "50854b45-c417-4a7d-e769-bc82cf7aa02c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/j4/21ypxx3x0019wgj7rqnlm2340000gn/T/ipykernel_61533/549244330.py:4: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  texto = BeautifulSoup(texto, \"html.parser\").get_text()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CARGA DE ETIQUETAS: |üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©üü©| 100.02%\n",
            "Textos train: 4200, Etiquetas train: 4200\n"
          ]
        }
      ],
      "source": [
        "# ======================= Carga de datos =======================\n",
        "path_test = '/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_test'\n",
        "path_train = '/Users/ferleon/Github/semestre_v/procesamiento_lenguaje/data/author_profiling/es_train'\n",
        "tr_txt_train, tr_y_train = get_texts_from_folder(path_train)\n",
        "#tr_txt_test, tr_y_test = get_texts_from_folder(path_test)\n",
        "\n",
        "print(f\"\\nTextos train: {len(tr_txt_train)}, Etiquetas train: {len(tr_y_train)}\")\n",
        "#print(f\"Textos test: {len(tr_txt_test)}, Etiquetas test: {len(tr_y_test)}\")\n",
        "\n",
        "\n",
        "paises = sorted(list(set(tr_y_train)))\n",
        "paises_numericas = {pais: idx for idx, pais in enumerate(paises)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_train = [paises_numericas[pais] for pais in tr_y_train]\n",
        "#y_test = [paises_numericas[pais] for pais in tr_y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ca778d",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "# ======================= Par√°metros =======================\n",
        "TOP_PALABRAS = 1_000\n",
        "MAX_ITERACIONES = 1_000\n",
        "\n",
        "# ======================= Creaci√≥n del conteo de palabras por documento =======================\n",
        "\n",
        "tokens_por_documento = []\n",
        "for doc in tr_txt_train:                # baja a min√∫sculas, quita urls, etc.\n",
        "    toks = tokenizer.tokenize(doc.lower())              # tu TweetTokenizer\n",
        "    tokens_por_documento.append(toks)\n",
        "\n",
        "print(f\"Tama√±o del tokens por documento: {len(tokens_por_documento)}\")\n",
        "print(f\"Primeros 3 documentos: {tokens_por_documento[:10]}\")\n",
        "\n",
        "vocabulario_por_documento = {}\n",
        "for doc in tokens_por_documento:\n",
        "    palabras_unicas = set(doc)\n",
        "    for token in palabras_unicas:\n",
        "        vocabulario_por_documento[token] = vocabulario_por_documento.get(token, 0) + 1\n",
        "Logger.debug(f\"Total de palabras √∫nicas en vocabulario: {len(vocabulario_por_documento)}\")\n",
        "Logger.debug(f\"Vocabulario por documento: {list(vocabulario_por_documento.items())[:10]}\")\n",
        "\n",
        "# ======================= Creaci√≥n del corpus de palabras =======================\n",
        "\n",
        "corpus_de_palabras = []\n",
        "for doc in tr_txt_train:\n",
        "    corpus_de_palabras += tokenizer.tokenize(doc.lower())\n",
        "\n",
        "corpus_de_palabras = [token for token in corpus_de_palabras if re.match(r\"^[a-zA-Z√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë]+$\", token)]\n",
        "\n",
        "stop_words = stopwords.words('spanish')\n",
        "corpus_de_palabras_clean = [token for token in corpus_de_palabras if token not in stop_words]\n",
        "\n",
        "print(f\"Tama√±o del corpus de palabras: {len(corpus_de_palabras)}\")\n",
        "print(f\"Primeros 10 tokens del corpus: {corpus_de_palabras[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29115cf",
      "metadata": {
        "id": "a29115cf"
      },
      "outputs": [],
      "source": [
        "distribucion_frecuencias = nltk.FreqDist(corpus_de_palabras)\n",
        "distribucion_frecuencias_clean = nltk.FreqDist(corpus_de_palabras_clean)\n",
        "\n",
        "vocabulario = [word for word, _ in distribucion_frecuencias.most_common(TOP_PALABRAS)]\n",
        "vocabulario_clean = [word for word, _ in distribucion_frecuencias_clean.most_common(TOP_PALABRAS)]\n",
        "print(f\"Tama√±o del vocabulario: {len(vocabulario)} | Primeras 10 palabras: {vocabulario[:10]}\")\n",
        "print(f\"Tipo de vocabulario: {type(vocabulario)}\")\n",
        "\n",
        "dict_indices = {word: i for i, word in enumerate(vocabulario)}\n",
        "dict_indices_clean = {word: i for i, word in enumerate(vocabulario_clean)}\n",
        "print(f\"Tipo de dict_indices: {type(dict_indices)}\")\n",
        "print(f\"Diccionario de √≠ndices (primeras 10 entradas): {dict_indices}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50270c95",
      "metadata": {
        "id": "50270c95"
      },
      "outputs": [],
      "source": [
        "def built_bow_tr_binario(tr_txt, vocabulario, dict_indices):\n",
        "    bow = np.zeros((len(tr_txt), len(vocabulario)), dtype=np.int8)\n",
        "    for cont_doc, tr in enumerate(tr_txt):\n",
        "        if not tr or not isinstance(tr, str):\n",
        "            continue\n",
        "        tokens = tokenizer.tokenize(tr.lower())\n",
        "        for word in tokens:\n",
        "            if word in dict_indices:\n",
        "                bow[cont_doc, dict_indices[word]] = 1\n",
        "    return bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d069ee66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d069ee66",
        "outputId": "1df153aa-ba0c-4381-9d76-177f5366bd17"
      },
      "outputs": [],
      "source": [
        "bow_tr_binario = built_bow_tr_binario(tr_txt_train, vocabulario, dict_indices)\n",
        "bow_tr_binario"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1236befc",
      "metadata": {
        "id": "1236befc"
      },
      "source": [
        "# Ejercicio 1 | Clasificaci√≥n de autores usando Bag of Words Binario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50983460",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50983460",
        "outputId": "37c44894-c4e9-46a3-c3f0-3852f05e8011"
      },
      "outputs": [],
      "source": [
        "x_train_80, x_val_20, y_train_80, y_val_20 = train_test_split(bow_tr_binario, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "\n",
        "parametros = {'C': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]}\n",
        "svr = svm.LinearSVC(class_weight='balanced', max_iter=MAX_ITERACIONES)\n",
        "\n",
        "grid = GridSearchCV(estimator=svr, param_grid=parametros, n_jobs=4, scoring='f1_macro', cv=5)\n",
        "grid.fit(x_train_80, y_train_80)\n",
        "y_pred = grid.predict(x_val_20)\n",
        "\n",
        "prec_bow_binario, rec_bow_binario, f1_bow_binario, _ = precision_recall_fscore_support(y_val_20, y_pred, average='weighted')\n",
        "\n",
        "print(confusion_matrix(y_val_20, y_pred))\n",
        "print(metrics.classification_report(y_val_20, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11f123c",
      "metadata": {
        "id": "f11f123c"
      },
      "source": [
        "# Ejercicio 2 | Clasificaci√≥n de autores usando Bag of Words Frecuencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95bb76e5",
      "metadata": {
        "id": "95bb76e5"
      },
      "outputs": [],
      "source": [
        "def built_bow_tr_frecuencia(tr_txt, vocabulario, dict_indices):\n",
        "    bow = np.zeros((len(tr_txt), len(vocabulario)), dtype=int)\n",
        "    for cont_doc, tr in enumerate(tr_txt):\n",
        "        if not tr or not isinstance(tr, str):\n",
        "            continue\n",
        "        tokens = tokenizer.tokenize(tr.lower())\n",
        "        fdist_doc = nltk.FreqDist(tokens)\n",
        "        for word in fdist_doc:\n",
        "            if word in dict_indices:\n",
        "                bow[cont_doc, dict_indices[word]] = fdist_doc[word]\n",
        "    return bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc1bf6c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc1bf6c0",
        "outputId": "0dd5e7f6-444f-4cdc-c09f-4a853ba25c65"
      },
      "outputs": [],
      "source": [
        "# 1) TF: tu funci√≥n tal cual\n",
        "bow_train_frecuencia = built_bow_tr_frecuencia(tr_txt_train, vocabulario, dict_indices).astype(float)\n",
        "print(f\"BoW de frecuencia (shape): {bow_train_frecuencia.shape}\")\n",
        "\n",
        "# 2) DF: usando tokens_por_documento que ya generaste\n",
        "N = len(tokens_por_documento)  # n√∫mero de documentos\n",
        "df = np.zeros(len(dict_indices), dtype=int)\n",
        "\n",
        "for doc in tokens_por_documento:\n",
        "    presentes = set()\n",
        "    for tok in doc:\n",
        "        j = dict_indices.get(tok)\n",
        "        if j is not None:\n",
        "            presentes.add(j)\n",
        "    for j in presentes:\n",
        "        df[j] += 1\n",
        "\n",
        "# 3) IDF: f√≥rmula est√°ndar suavizada\n",
        "idf = np.log((1.0 + N) / (1.0 + df)) + 1.0\n",
        "\n",
        "# 4) TF-IDF: multiplicar cada columna por su IDF\n",
        "bow_tfidf = bow_train_frecuencia * idf[None, :]  # broadcasting en eje columnas\n",
        "\n",
        "# 5) Normalizaci√≥n L2 por documento (opcional pero buena para SVM)\n",
        "normas = np.linalg.norm(bow_tfidf, axis=1, keepdims=True) + 1e-12\n",
        "bow_tfidf = bow_tfidf / normas\n",
        "\n",
        "print(f\"TF-IDF listo (shape): {bow_tfidf.shape}\")\n",
        "print(f\"Primeras filas TF-IDF:\\n{bow_tfidf[:3, :10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2ed50f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b2ed50f",
        "outputId": "23ac5ffd-94ea-4616-9d5c-c31868116a72"
      },
      "outputs": [],
      "source": [
        "x_train_80, x_val_20, y_train_80, y_val_20 = train_test_split(bow_train_frecuencia, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "\n",
        "parametros = {'C': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]}\n",
        "svr = svm.LinearSVC(class_weight='balanced', max_iter=MAX_ITERACIONES)\n",
        "grid = GridSearchCV(estimator=svr, param_grid=parametros, n_jobs=4, scoring='f1_macro', cv=5)\n",
        "grid.fit(x_train_80, y_train_80)\n",
        "y_pred = grid.predict(x_val_20)\n",
        "\n",
        "prec_bow_frecuencia, rec_bow_frecuencia, f1_bow_frecuencia, _ = precision_recall_fscore_support(y_val_20, y_pred, average='macro')\n",
        "\n",
        "print(confusion_matrix(y_val_20, y_pred))\n",
        "print(metrics.classification_report(y_val_20, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9220b2fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_80, x_val_20, y_train_80, y_val_20 = train_test_split(bow_tfidf, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "\n",
        "parametros = {'C': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]}\n",
        "svr = svm.LinearSVC(class_weight='balanced', max_iter=MAX_ITERACIONES)\n",
        "grid = GridSearchCV(estimator=svr, param_grid=parametros, n_jobs=4, scoring='f1_macro', cv=5)\n",
        "grid.fit(x_train_80, y_train_80)\n",
        "y_pred = grid.predict(x_val_20)\n",
        "\n",
        "prec_bow_tfidf, rec_bow_tfidf, f1_bow_tfidf, _ = precision_recall_fscore_support(y_val_20, y_pred, average='macro')\n",
        "\n",
        "print(confusion_matrix(y_val_20, y_pred))\n",
        "print(metrics.classification_report(y_val_20, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97937ecc",
      "metadata": {
        "id": "97937ecc"
      },
      "source": [
        "# Ejercicio 3 | Clasificaci√≥n de autores usando Bag of Words Binario y normalizaci√≥n L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46cdf00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e46cdf00",
        "outputId": "fa09d669-f327-432e-aef2-fbbd644e1f5f"
      },
      "outputs": [],
      "source": [
        "\n",
        "bow_train_L2 = normalize(bow_tr_binario, norm='l2')\n",
        "\n",
        "x_train_80, x_val_20, y_train_80, y_val_20 = train_test_split(bow_train_L2, y_train, test_size=0.2, stratify=y_train)\n",
        "\n",
        "parametros = {'C': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]}\n",
        "svr = svm.LinearSVC(class_weight='balanced', max_iter=MAX_ITERACIONES)\n",
        "\n",
        "grid = GridSearchCV(estimator=svr, param_grid=parametros, n_jobs=4, scoring='f1_macro', cv=5)\n",
        "grid.fit(x_train_80, y_train_80)\n",
        "y_pred = grid.predict(x_val_20)\n",
        "\n",
        "prec_bow_binario_l2, rec_bow_binario_l2, f1_bow_binario_l2, _ = precision_recall_fscore_support(y_val_20, y_pred, average='macro')\n",
        "\n",
        "print(confusion_matrix(y_val_20, y_pred))\n",
        "print(metrics.classification_report(y_val_20, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8877ef",
      "metadata": {
        "id": "ce8877ef"
      },
      "source": [
        "# Ejercicio 4 | Clasificaci√≥n de autores usando Bag of Words Frecuencia y normalizaci√≥n L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11d050b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11d050b",
        "outputId": "2bc79cda-e963-49c7-c51d-31ac05882d31"
      },
      "outputs": [],
      "source": [
        "bow_train_frecuencia_L2 = normalize(bow_train_frecuencia, norm='l2')\n",
        "\n",
        "x_train_80, x_val_20, y_train_80, y_val_20 = train_test_split(bow_train_frecuencia_L2, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "\n",
        "parametros = {'C': [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]}\n",
        "svr = svm.LinearSVC(class_weight='balanced', max_iter=MAX_ITERACIONES)\n",
        "grid = GridSearchCV(estimator=svr, param_grid=parametros, n_jobs=4, scoring='f1_macro', cv=5)\n",
        "grid.fit(x_train_80, y_train_80)\n",
        "y_pred = grid.predict(x_val_20)\n",
        "\n",
        "prec_bow_frecuencia_L2, rec_bow_frecuencia_L2, f1_bow_frecuencia_L2, _ = precision_recall_fscore_support(y_val_20, y_pred, average='macro')\n",
        "\n",
        "print(confusion_matrix(y_val_20, y_pred))\n",
        "print(metrics.classification_report(y_val_20, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5G0T6gxcYtjP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G0T6gxcYtjP",
        "outputId": "c8b1e073-2070-4303-c62f-b868412a423b"
      },
      "outputs": [],
      "source": [
        "def ganador_metricas( etiquetas, precisiones, f1_scores):\n",
        "    max_prec = max(precisiones)\n",
        "    max_f1 = max(f1_scores)\n",
        "    \n",
        "    if max_prec >= max_f1:\n",
        "        indice_ganador = precisiones.index(max_prec)\n",
        "        return etiquetas[indice_ganador], max_prec\n",
        "    else:\n",
        "        indice_ganador = f1_scores.index(max_f1)\n",
        "        return etiquetas[indice_ganador], max_f1\n",
        "\n",
        "\n",
        "etiquetas = ['BOW Binario', 'BOW Frecuencia', 'BOW TF-IDF', 'BOW Binario + L2', 'BOW Frecuencia + L2' ]\n",
        "\n",
        "\n",
        "\n",
        "# ======================= Tabla comparativa =======================\n",
        "tabla_comparativa = {\n",
        "    'Experimento': etiquetas,\n",
        "    'Accuracy': [prec_bow_binario, prec_bow_frecuencia, prec_bow_tfidf, prec_bow_binario_l2, prec_bow_frecuencia_L2,],\n",
        "    'F1-Score (Macro)': [f1_bow_binario, f1_bow_frecuencia, f1_bow_tfidf, f1_bow_binario_l2, f1_bow_frecuencia_L2,],\n",
        "    'F1-Score (Weighted)': [f1_bow_binario, f1_bow_frecuencia, f1_bow_tfidf, f1_bow_binario_l2, f1_bow_frecuencia_L2,],\n",
        "    'Precisi√≥n (Macro)': [prec_bow_binario, prec_bow_frecuencia, prec_bow_tfidf, prec_bow_binario_l2, prec_bow_frecuencia_L2,]\n",
        "}\n",
        "\n",
        "df_comparativo = pl.DataFrame(tabla_comparativa)\n",
        "\n",
        "print(\"TABLA COMPARATIVA DE LOS 4 EJERCICIOS\")\n",
        "print(\"=\" * 50)\n",
        "print(df_comparativo)\n",
        "print(\"=\" * 50)\n",
        "\n",
        "ganador = ganador_metricas(etiquetas, tabla_comparativa['Precisi√≥n (Macro)'], tabla_comparativa['F1-Score (Macro)'])\n",
        "print(f\"Gano el modelo basado en: {ganador[0]} con un valor de: {ganador[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13065cc",
      "metadata": {},
      "source": [
        "# VECTORES DE PALABRAS M√ÅS IMPORTANTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4deb6339",
      "metadata": {},
      "outputs": [],
      "source": [
        "bow_train_frecuencia_clean = built_bow_tr_frecuencia(tr_txt_train,vocabulario_clean,dict_indices_clean)\n",
        "print(bow_train_frecuencia_clean)\n",
        "bow_train_frecuencia_clean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd1d027",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_dor_profe(TR):\n",
        "    DTR = np.zeros((TR.shape[1], TR.shape[0]), dtype=float)\n",
        "\n",
        "    tam_v = TR.shape[1] # Tama√±o del vocabulario TOTAL\n",
        "    \n",
        "    for i,doc in enumerate(TR):\n",
        "        non_zero_positions = np.nonzero(doc)[0] # Esto me dice las dimensiones que no son cero\n",
        "        tama√±o_vocabulario = len(non_zero_positions)\n",
        "        for termino in non_zero_positions:\n",
        "            DTR[termino, i] = doc[termino] * np.log(tam_v / tama√±o_vocabulario)\n",
        "    return DTR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7badaf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "dor = compute_dor_profe(bow_train_frecuencia_clean)\n",
        "\n",
        "dor_normalizado_clean = preprocessing.normalize(dor, norm='l2')\n",
        "\n",
        "print(dor_normalizado_clean.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f2b884",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "feature_selector = SelectKBest(chi2, k=1000)\n",
        "\n",
        "# Aprende a como hacer selecci√≥n de las palabras de manera muy interesante en automatico les da la relevancia\n",
        "feature_selector = SelectKBest(chi2, k=1000)\n",
        "feature_selector.fit(bow_train_frecuencia, y_train)\n",
        "best = feature_selector.get_support(indices=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684210de",
      "metadata": {},
      "outputs": [],
      "source": [
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0900666e",
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_indices_invertido_clean = {valor: key for key, valor in dict_indices_clean.items()}\n",
        "dict_indices_invertido_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c71e5deb",
      "metadata": {},
      "outputs": [],
      "source": [
        "t_words = [dict_indices_invertido_clean[index] for index in best]\n",
        "# dict_indices {\"palabra\": \"dimension_en_bow\",...}\n",
        "matris_objetivo_clean = np.array([dor_normalizado_clean[dict_indices_clean[word]] for word in t_words])\n",
        "matris_objetivo_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203509ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "matris_objetivo_clean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c56fe020",
      "metadata": {},
      "outputs": [],
      "source": [
        "dor_normalizado_clean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225f88a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "reduce_matrix = TSNE(n_components=2).fit_transform(matris_objetivo_clean)\n",
        "\n",
        "max_x , max_y = np.max(reduce_matrix, axis=0)\n",
        "min_x , min_y = np.min(reduce_matrix, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cf00b20",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "reduce_matrix = TSNE(n_components=2).fit_transform(matris_objetivo_clean)\n",
        "reduce_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46ff3d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import patheffects\n",
        "\n",
        "x, y = reduce_matrix[:, 0], reduce_matrix[:, 1]\n",
        "\n",
        "plt.figure(figsize=(50, 50), dpi=120)\n",
        "plt.xlim(min_x, max_x)\n",
        "plt.ylim(min_y, max_y)\n",
        "plt.scatter(x, y, s=30, color='black')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "stop_words = stopwords.words('spanish')\n",
        "\n",
        "\n",
        "for i, word in enumerate(t_words):\n",
        "    if word in stop_words:\n",
        "        plt.annotate(\n",
        "            word, \n",
        "            (x[i], y[i]), \n",
        "            fontsize=18, \n",
        "            color='red', \n",
        "            fontweight='bold',\n",
        "            path_effects=[patheffects.withStroke(linewidth=3, foreground=\"white\")]\n",
        "        )\n",
        "    elif word in ['politicos', 'corrupci√≥n', 'PRI', 'feliz', \n",
        "              'hermosa', 'chica', 'tu', 'hdp','madre','madres',\n",
        "              '@usuario' ,'hijos', 'pendeja', 'pendejo','mierda', \n",
        "              'loca', 'hijo', 'hija', 'mam√°', 't√≠a']:\n",
        "        plt.annotate(\n",
        "            word, \n",
        "            (x[i], y[i]), \n",
        "            fontsize=22, \n",
        "            color='blue', \n",
        "            fontweight='bold',\n",
        "            path_effects=[patheffects.withStroke(linewidth=3, foreground=\"white\")]\n",
        "        )\n",
        "    else:\n",
        "        plt.annotate(\n",
        "            word, \n",
        "            (x[i], y[i]), \n",
        "            fontsize=16, \n",
        "            color='black',\n",
        "            path_effects=[patheffects.withStroke(linewidth=3, foreground=\"white\")]\n",
        "        )\n",
        "\n",
        "plt.gca().set_facecolor('whitesmoke')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9391ebdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(t_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d287e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "subsetword = ['politicos', 'corrupci√≥n', 'PRI', 'feliz', \n",
        "              'hermosa', 'chica', 'tu', 'hdp','madre','madres',\n",
        "              '@usuario' ,'hijos', 'pendeja', 'pendejo','mierda', \n",
        "              'loca', 'hijo', 'hija', 'mam√°', 't√≠a','padre','pap√°']\n",
        "\n",
        "# t_words Las 1,000 mejores palabras seg√∫n chi2 ( Algoritmo de selecci√≥n de caracter√≠sticas)\n",
        "# reduce_matriz Aqu√≠ estan las 1,000 mejores palabras en dos dimensiones\n",
        "\n",
        "subreduce_matriz = []\n",
        "ploted_subsetwords = []\n",
        "\n",
        "\n",
        "for idx, word in enumerate(t_words):\n",
        "    if word in subsetword:\n",
        "        subreduce_matriz.append(reduce_matrix[idx])\n",
        "        ploted_subsetwords.append(word)\n",
        "    print_bar(idx, len(t_words), contexto=\"PROGRESO\")\n",
        "\n",
        "print()\n",
        "Logger.debug(f\"Palabras a graficar: {ploted_subsetwords}\")\n",
        "Logger.debug(f\"Coordenadas: {subreduce_matriz}\")\n",
        "\n",
        "# Convertir a numpy array\n",
        "subreduce_matriz = np.array(subreduce_matriz)\n",
        "\n",
        "\n",
        "# Hacemos la gr√°fica de flechas\n",
        "fig , ax = plt.subplots(figsize=(15, 15))\n",
        "\n",
        "for word in subreduce_matriz:\n",
        "    ax.arrow(0, 0, word[0], word[1], head_width=0.8, head_length=0.8, fc='red', ec='red', width=0.1e-2)\n",
        "    ax.annotate(\n",
        "        ploted_subsetwords[subreduce_matriz.tolist().index(word.tolist())], \n",
        "        (word[0], word[1]), \n",
        "        fontsize=12, \n",
        "        color='blue', \n",
        "        fontweight='bold',\n",
        "        path_effects=[patheffects.withStroke(linewidth=3, foreground=\"white\")]\n",
        "    )\n",
        "\n",
        "ax.scatter(subreduce_matriz[:,0], subreduce_matriz[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec6768c",
      "metadata": {},
      "source": [
        "# NUBE DE PALABRAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9aacb99",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Chi2\n",
        "feature_selector = SelectKBest(chi2, k=1000)\n",
        "feature_selector.fit(bow_train_frecuencia_clean, y_train) # EL y_train son los paises \n",
        "\n",
        "\n",
        "dict_indices_invertido_clean = {valor: key for key, valor in dict_indices_clean.items()}\n",
        "Logger.debug(f\" Diccionario invertido de √≠ndices : {dict_indices_invertido_clean}\")\n",
        "\n",
        "# Extraer las palabras seleccionadas\n",
        "palabras_mejores_indices = feature_selector.get_support(indices=True)\n",
        "palabras = [dict_indices_invertido_clean[indice] for indice in palabras_mejores_indices]\n",
        "\n",
        "\n",
        "# Extraer los scores\n",
        "chi2_scores = feature_selector.scores_\n",
        "best_scores = chi2_scores[palabras_mejores_indices] # type: ignore\n",
        "Logger.debug(f\" Puntajes Chi2 de las mejores palabras: {best_scores}\")\n",
        "\n",
        "\n",
        "# Distribuci√≥n de k mejores palabras\n",
        "distribucion_k_mejores_tweets = {palabra: score for palabra, score in zip(palabras, best_scores)} # type: ignore\n",
        "mejores_50 = dict(sorted(distribucion_k_mejores_tweets.items(), key=lambda item: item[1], reverse=True)[:500])\n",
        "Logger.debug(f\"Frecuencia de palabras en tweets: {mejores_50}\")\n",
        "\n",
        "\n",
        "# Generar y mostrar la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='black').generate_from_frequencies(mejores_50)\n",
        "\n",
        "# Mostrar la nube de palabras\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb42a07e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================= TCOR (Lavelli) con misma interfaz de uso =======================\n",
        "from collections import Counter\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize as sk_normalize\n",
        "\n",
        "def compute_tcor_profe(tokens_por_documento, dict_indices, min_pair_count=1):\n",
        "    \"\"\"\n",
        "    Construye TCOR seg√∫n Lavelli:\n",
        "      w_{k,j} = (1 + log #(k,j)) * log(|T| / T_k)\n",
        "    Co-ocurrencia por DOCUMENTO (pares √∫nicos por doc). Devuelve CSR (|V| x |V|).\n",
        "    \"\"\"\n",
        "    V = len(dict_indices)\n",
        "    pair_counts = Counter()\n",
        "    vecinos = [set() for _ in range(V)]\n",
        "\n",
        "    # 1) Contar co-ocurrencias por documento (sin duplicar dentro del doc)\n",
        "    for toks in tokens_por_documento:\n",
        "        idxs = [dict_indices[t] for t in toks if t in dict_indices]\n",
        "        if not idxs:\n",
        "            continue\n",
        "        uniq = sorted(set(idxs))\n",
        "        for a in range(len(uniq)):\n",
        "            i = uniq[a]\n",
        "            for b in range(a+1, len(uniq)):\n",
        "                j = uniq[b]\n",
        "                pair_counts[(i, j)] += 1\n",
        "                vecinos[i].add(j); vecinos[j].add(i)\n",
        "\n",
        "    if not pair_counts:\n",
        "        raise ValueError(\"No se encontraron co-ocurrencias con el vocabulario dado.\")\n",
        "\n",
        "    # 2) Matriz sim√©trica dispersa de conteos\n",
        "    rows, cols, data = [], [], []\n",
        "    for (i, j), c in pair_counts.items():\n",
        "        if c >= min_pair_count:\n",
        "            rows += [i, j]; cols += [j, i]; data += [c, c]\n",
        "    C = sparse.coo_matrix((data, (rows, cols)), shape=(V, V)).tocsr()\n",
        "\n",
        "    # 3) tff(i,j) = 1 + log(count)\n",
        "    C.data = 1.0 + np.log(C.data)\n",
        "\n",
        "    # 4) Escalado por fila con log(|T| / T_k), T_k = #vecinos √∫nicos de k\n",
        "    Tk = np.array([max(1, len(S)) for S in vecinos], dtype=float)\n",
        "    s_k = np.log((V) / Tk)                 # |T| = V\n",
        "    S = sparse.diags(s_k, format=\"csr\")\n",
        "    TCOR = S @ C\n",
        "\n",
        "    # 5) Limpieza\n",
        "    TCOR.setdiag(0)\n",
        "    TCOR.eliminate_zeros()\n",
        "    return TCOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5102db1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Antes ten√≠as:\n",
        "# dor = compute_dor_profe(bow_train_frecuencia_clean)\n",
        "# dor_normalizado_clean = preprocessing.normalize(dor, norm='l2')\n",
        "\n",
        "# Ahora:\n",
        "tcor = compute_tcor_profe(tokens_por_documento, dict_indices_clean, min_pair_count=1)\n",
        "tcor_normalizado_clean = preprocessing.normalize(tcor, norm='l2')  # t√©rmino√ót√©rmino, L2 por fila\n",
        "\n",
        "print(tcor_normalizado_clean.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d984b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_selector = SelectKBest(chi2, k=1000)\n",
        "feature_selector.fit(bow_tfidf, y_train)  # o bow_train_frecuencia, como prefieras\n",
        "best = feature_selector.get_support(indices=True)\n",
        "\n",
        "dict_indices_invertido_clean = {v:k for k,v in dict_indices_clean.items()}\n",
        "t_words = [dict_indices_invertido_clean[idx] for idx in best]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec5fa7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# √çndices de las palabras seleccionadas en el vocab limpio\n",
        "idx_words = [dict_indices_clean[w] for w in t_words]\n",
        "\n",
        "# Igual que con DOR: toma las filas de esas palabras\n",
        "# (opci√≥n A ‚Äî filas contra TODO el vocab; puede ser grande si |V| es grande)\n",
        "# matris_objetivo_clean = tcor_normalizado_clean[idx_words, :].toarray()\n",
        "\n",
        "# Mejor para estabilidad/tiempo: filas y columnas en las mismas 1000 palabras (1000x1000)\n",
        "tcor_1k = tcor_normalizado_clean[idx_words][:, idx_words].toarray()\n",
        "matris_objetivo_clean = tcor_1k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c937a366",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "reduce_matrix = TSNE(n_components=2, init=\"pca\", random_state=42, perplexity=30, learning_rate=\"auto\", max_iter=1000)\\\n",
        "                .fit_transform(matris_objetivo_clean)\n",
        "\n",
        "max_x, max_y = np.max(reduce_matrix, axis=0)\n",
        "min_x, min_y = np.min(reduce_matrix, axis=0)\n",
        "\n",
        "# Tu misma rutina de anotado\n",
        "from matplotlib import patheffects\n",
        "x, y = reduce_matrix[:,0], reduce_matrix[:,1]\n",
        "\n",
        "plt.figure(figsize=(50,50), dpi=120)\n",
        "plt.xlim(min_x, max_x); plt.ylim(min_y, max_y)\n",
        "plt.scatter(x, y, s=30, color='black')\n",
        "\n",
        "stop_words_es = stopwords.words('spanish')\n",
        "for i, word in enumerate(t_words):\n",
        "    if word in stop_words_es:\n",
        "        plt.annotate(word, (x[i],y[i]), fontsize=18, color='red', fontweight='bold',\n",
        "                     path_effects=[patheffects.withStroke(linewidth=3, foreground=\"white\")])\n",
        "    elif word in ['politicos','corrupci√≥n','PRI','feliz','hermosa','chica','tu','hdp','madre','madres',\n",
        "                  '@usuario','hijos','pendeja','pendejo','mierda','loca','hijo','hija','mam√°','t√≠a']:\n",
        "        plt.annotate(word, (x[i],y[i]), fontsize=22, color='blue', fontweight='bold',\n",
        "                     path_effects=[patheffects.withStroke(linewidth=3, foreground=\"white\")])\n",
        "    else:\n",
        "        plt.annotate(word, (x[i],y[i]), fontsize=16, color='black',\n",
        "                     path_effects=[patheffects.withStroke(linewidth=3, foreground=\"white\")])\n",
        "\n",
        "plt.gca().set_facecolor('whitesmoke')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "semestre-v",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
